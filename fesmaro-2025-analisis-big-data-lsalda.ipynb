{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fesmaro 2025 Analisis Big Data  \n\n\n## LDA & LSA","metadata":{}},{"cell_type":"code","source":"import re\nimport nltk\nimport pandas as pd\nfrom nltk.tokenize import word_tokenize\nfrom gensim.corpora import Dictionary\nfrom gensim.models import LdaModel, LsiModel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:25:35.164887Z","iopub.execute_input":"2025-03-26T09:25:35.165359Z","iopub.status.idle":"2025-03-26T09:25:35.171618Z","shell.execute_reply.started":"2025-03-26T09:25:35.165323Z","shell.execute_reply":"2025-03-26T09:25:35.170178Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"import gdown \nurl = \"https://drive.google.com/drive/u/4/folders/1La029vITSdOyDC-TaKZ9kxVk0twOneFv\"\ngdown.download_folder(url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:42:50.471221Z","iopub.execute_input":"2025-03-26T08:42:50.471796Z","iopub.status.idle":"2025-03-26T08:43:25.521433Z","shell.execute_reply.started":"2025-03-26T08:42:50.471768Z","shell.execute_reply":"2025-03-26T08:43:25.520397Z"}},"outputs":[{"name":"stderr","text":"Retrieving folder contents\n","output_type":"stream"},{"name":"stdout","text":"Processing file 14XpIdfj6FN9ckvBVrRp9lUa-OtuDIu5O df_processed.csv\nProcessing file 1ZwRcWT9RubqpZ_qFukgq9ZyO3Ob0I0Q9 df_test.csv\nProcessing file 1Btr1jmjO2fx8Uh6_4-It79Mf5pLdXQcy df_train.csv\nProcessing file 1giX5L2NXjxDXepK3wsaRNJjUcSzZf_9P df_val.csv\nProcessing file 1YioeFhGr0yUFVX3dFcIwzFsPFfd11LKj final_df.csv\n","output_type":"stream"},{"name":"stderr","text":"Retrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom (original): https://drive.google.com/uc?id=14XpIdfj6FN9ckvBVrRp9lUa-OtuDIu5O\nFrom (redirected): https://drive.google.com/uc?id=14XpIdfj6FN9ckvBVrRp9lUa-OtuDIu5O&confirm=t&uuid=a155c2dc-d2af-4c6f-92ff-bfb1faee881a\nTo: /kaggle/working/data/df_processed.csv\n100%|██████████| 128M/128M [00:01<00:00, 82.5MB/s] \nDownloading...\nFrom: https://drive.google.com/uc?id=1ZwRcWT9RubqpZ_qFukgq9ZyO3Ob0I0Q9\nTo: /kaggle/working/data/df_test.csv\n100%|██████████| 12.1M/12.1M [00:00<00:00, 33.2MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Btr1jmjO2fx8Uh6_4-It79Mf5pLdXQcy\nTo: /kaggle/working/data/df_train.csv\n100%|██████████| 40.8M/40.8M [00:00<00:00, 62.8MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1giX5L2NXjxDXepK3wsaRNJjUcSzZf_9P\nTo: /kaggle/working/data/df_val.csv\n100%|██████████| 8.06M/8.06M [00:00<00:00, 32.0MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1YioeFhGr0yUFVX3dFcIwzFsPFfd11LKj\nTo: /kaggle/working/data/final_df.csv\n100%|██████████| 29.3M/29.3M [00:00<00:00, 74.1MB/s]\nDownload completed\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/data/df_processed.csv',\n '/kaggle/working/data/df_test.csv',\n '/kaggle/working/data/df_train.csv',\n '/kaggle/working/data/df_val.csv',\n '/kaggle/working/data/final_df.csv']"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"!ls /kaggle/working/data","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T08:43:28.461791Z","iopub.execute_input":"2025-03-26T08:43:28.462433Z","iopub.status.idle":"2025-03-26T08:43:28.598903Z","shell.execute_reply.started":"2025-03-26T08:43:28.462400Z","shell.execute_reply":"2025-03-26T08:43:28.597510Z"}},"outputs":[{"name":"stdout","text":"df_processed.csv  df_test.csv  df_train.csv  df_val.csv  final_df.csv\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/working/data/final_df.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:10:52.929593Z","iopub.execute_input":"2025-03-26T09:10:52.930035Z","iopub.status.idle":"2025-03-26T09:10:53.521719Z","shell.execute_reply.started":"2025-03-26T09:10:52.930002Z","shell.execute_reply":"2025-03-26T09:10:53.520563Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:10:53.523425Z","iopub.execute_input":"2025-03-26T09:10:53.523775Z","iopub.status.idle":"2025-03-26T09:10:53.539841Z","shell.execute_reply.started":"2025-03-26T09:10:53.523745Z","shell.execute_reply":"2025-03-26T09:10:53.538431Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"   label                                    lemmatized_text  text_length  \\\n0      1  use book product herb easy access material giv...          357   \n1      1  cd great example heavy hardcore softer edge no...          552   \n2      0  not read book yet review sample kindle point s...          373   \n3      1  movie one favorite partly michigan favorite so...          558   \n4      1  fun ten barrel monkey movie escapist product s...          391   \n\n   word_count  avg_word_length  exclamation_count  question_count  \\\n0          63         4.682540                  0               0   \n1         100         4.530000                  0               0   \n2          74         4.054054                  1               0   \n3         105         4.323810                  1               0   \n4          63         5.222222                  2               0   \n\n   uppercase_ratio  positive_word_count  negative_word_count  \\\n0         0.000000                    1                    0   \n1         0.009901                    2                    0   \n2         0.000000                    0                    0   \n3         0.000000                    3                    0   \n4         0.000000                    1                    0   \n\n   sentiment_word_ratio  \n0                   2.0  \n1                   3.0  \n2                   1.0  \n3                   4.0  \n4                   2.0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>lemmatized_text</th>\n      <th>text_length</th>\n      <th>word_count</th>\n      <th>avg_word_length</th>\n      <th>exclamation_count</th>\n      <th>question_count</th>\n      <th>uppercase_ratio</th>\n      <th>positive_word_count</th>\n      <th>negative_word_count</th>\n      <th>sentiment_word_ratio</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>use book product herb easy access material giv...</td>\n      <td>357</td>\n      <td>63</td>\n      <td>4.682540</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>cd great example heavy hardcore softer edge no...</td>\n      <td>552</td>\n      <td>100</td>\n      <td>4.530000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.009901</td>\n      <td>2</td>\n      <td>0</td>\n      <td>3.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>not read book yet review sample kindle point s...</td>\n      <td>373</td>\n      <td>74</td>\n      <td>4.054054</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>movie one favorite partly michigan favorite so...</td>\n      <td>558</td>\n      <td>105</td>\n      <td>4.323810</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>3</td>\n      <td>0</td>\n      <td>4.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>fun ten barrel monkey movie escapist product s...</td>\n      <td>391</td>\n      <td>63</td>\n      <td>5.222222</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0.000000</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Tokenize the lemmatized text\n# This step assumes 'lemmatized_text' contains strings of space-separated words.\n# If it already contains lists of tokens, you can skip this.\ntokenized_texts = [word_tokenize(text.lower()) for text in df['lemmatized_text']]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:10:55.414183Z","iopub.execute_input":"2025-03-26T09:10:55.414650Z","iopub.status.idle":"2025-03-26T09:11:18.002822Z","shell.execute_reply.started":"2025-03-26T09:10:55.414616Z","shell.execute_reply":"2025-03-26T09:11:18.001579Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# --- 3. Create Gensim Dictionary and Corpus ---\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(tokenized_texts)\n\n# **Optional but Recommended:** Filter out extreme words\n# Filter out words that appear in fewer than (e.g.) 3 documents\n# or more than 50% of the documents. Adjust these values based on your dataset size.\nno_below = min(3, len(tokenized_texts) // 2) # Ensure no_below isn't too high for small samples\nprint(f\"Filtering dictionary: keep tokens appearing in at least {no_below} documents and less than 50% of documents.\")\ndictionary.filter_extremes(no_below=no_below, no_above=0.5)\n\n# Create a Corpus: Convert tokenized documents into bag-of-words vectors.\n# This is a list where each element corresponds to a document,\n# represented as a list of (word_id, word_frequency) tuples.\ncorpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n\n# Check if the corpus is empty after filtering (can happen with small/similar samples)\nif not corpus or all(not doc for doc in corpus):\n    print(\"\\nWarning: The corpus is empty or contains only empty documents after filtering.\")\n    print(\"This might be due to strict filtering or very short/similar input texts.\")\n    print(\"Consider adjusting filter_extremes parameters (no_below, no_above) or checking your data.\")\n    # Exit or handle this case appropriately\n    exit()\nelif not dictionary:\n     print(\"\\nWarning: The dictionary is empty after filtering.\")\n     print(\"This might be due to strict filtering or very short/similar input texts.\")\n     print(\"Consider adjusting filter_extremes parameters (no_below, no_above) or checking your data.\")\n     # Exit or handle this case appropriately\n     exit()\n\n\nprint(f\"\\nNumber of unique tokens after filtering: {len(dictionary)}\")\nprint(f\"Number of documents: {len(corpus)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:11:37.077024Z","iopub.execute_input":"2025-03-26T09:11:37.077409Z","iopub.status.idle":"2025-03-26T09:11:44.865028Z","shell.execute_reply.started":"2025-03-26T09:11:37.077382Z","shell.execute_reply":"2025-03-26T09:11:44.863944Z"}},"outputs":[{"name":"stdout","text":"Filtering dictionary: keep tokens appearing in at least 3 documents and less than 50% of documents.\n\nNumber of unique tokens after filtering: 29627\nNumber of documents: 100000\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# --- 4. Build LDA Model ---\nprint(\"\\n--- Building LDA Model ---\")\n# **Important Hyperparameter:** num_topics\n# You need to choose the number of topics you want to extract.\n# This often requires experimentation and evaluation (e.g., using coherence scores).\n# Let's start with 3 topics for this small example. For 100k documents, you'd likely use more (e.g., 20, 50, 100+).\nnum_topics_lda = 3\n\n# Train the LDA model\n# `passes`: Number of passes through the corpus during training. More passes can lead to better topics but take longer.\n# `random_state`: For reproducibility.\ntry:\n    lda_model = LdaModel(corpus=corpus,\n                         id2word=dictionary,\n                         num_topics=num_topics_lda,\n                         random_state=42,\n                         passes=10, # Increase passes for potentially better results on larger data\n                         # alpha='auto', # Let Gensim learn asymmetric alpha from data (optional)\n                         # eta='auto'    # Let Gensim learn asymmetric eta from data (optional)\n                        )\n\n    # Print the topics found by the LDA model\n    print(f\"\\nLDA Topics (Top words per topic):\")\n    # `num_words`: How many keywords to display for each topic\n    topics_lda = lda_model.print_topics(num_words=5)\n    for topic_num, topic_words in topics_lda:\n        print(f\"Topic {topic_num}: {topic_words}\")\n\nexcept ValueError as e:\n    print(f\"\\nError building LDA model: {e}\")\n    print(\"This might happen if the corpus is effectively empty after filtering.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:11:59.305464Z","iopub.execute_input":"2025-03-26T09:11:59.305814Z","iopub.status.idle":"2025-03-26T09:18:02.084700Z","shell.execute_reply.started":"2025-03-26T09:11:59.305787Z","shell.execute_reply":"2025-03-26T09:18:02.083344Z"}},"outputs":[{"name":"stdout","text":"\n--- Building LDA Model ---\n\nLDA Topics (Top words per topic):\nTopic 0: 0.039*\"number\" + 0.011*\"one\" + 0.010*\"would\" + 0.009*\"great\" + 0.009*\"work\"\nTopic 1: 0.049*\"book\" + 0.015*\"read\" + 0.010*\"one\" + 0.010*\"movie\" + 0.009*\"story\"\nTopic 2: 0.027*\"number\" + 0.016*\"cd\" + 0.014*\"like\" + 0.013*\"album\" + 0.013*\"one\"\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"# --- 5. Build LSA (LSI) Model ---\nprint(\"\\n--- Building LSA (LSI) Model ---\")\n# **Important Hyperparameter:** num_topics\n# Similar to LDA, you choose the number of desired dimensions/topics.\n# LSI often benefits from starting with a larger number of topics initially (e.g., 100-500)\n# and then potentially using only the top ones, but let's stick with a small number for illustration.\nnum_topics_lsa = 3\n\n# Train the LSI model\n# Note: LSI often works well with TF-IDF input, but we'll use the BoW corpus here for direct comparison with LDA setup.\n# For larger datasets, consider creating a TF-IDF corpus first:\n# from gensim.models import TfidfModel\n# tfidf = TfidfModel(corpus)\n# corpus_tfidf = tfidf[corpus]\n# lsi_model = LsiModel(corpus=corpus_tfidf, id2word=dictionary, num_topics=num_topics_lsa)\ntry:\n    lsi_model = LsiModel(corpus=corpus,\n                         id2word=dictionary,\n                         num_topics=num_topics_lsa\n                         # decay=1.0, # Default value, controls diminishing influence of later documents in online training\n                         # onepass=True # Set to False for multi-pass SVD, potentially more accurate but slower/memory intensive\n                        )\n\n    # Print the topics found by the LSI model\n    print(f\"\\nLSA/LSI Topics (Top words per topic):\")\n    # `num_words`: How many keywords to display for each topic\n    topics_lsa = lsi_model.print_topics(num_words=5)\n    for topic_num, topic_words in topics_lsa:\n        # Clean up the output format which often looks like '0.700*\"word1\" + 0.500*\"word2\" ...'\n        cleaned_topic = re.sub(r'-?\\d+\\.\\d+\\*', '', topic_words).replace('\"', '').replace(' + ', ', ')\n        print(f\"Topic {topic_num}: {cleaned_topic.strip()}\")\n\nexcept Exception as e: # LSI can raise different errors, e.g., related to SVD\n    print(f\"\\nError building LSI model: {e}\")\n    print(\"This might happen with very small or sparse corpora.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-26T09:25:38.582432Z","iopub.execute_input":"2025-03-26T09:25:38.582846Z","iopub.status.idle":"2025-03-26T09:25:44.454916Z","shell.execute_reply.started":"2025-03-26T09:25:38.582816Z","shell.execute_reply":"2025-03-26T09:25:44.453763Z"}},"outputs":[{"name":"stdout","text":"\n--- Building LSA (LSI) Model ---\n\nLSA/LSI Topics (Top words per topic):\nTopic 0: number, book, one, great, would\nTopic 1: book, number, read, one, good\nTopic 2: book, one, number, movie, like\n","output_type":"stream"}],"execution_count":25}]}