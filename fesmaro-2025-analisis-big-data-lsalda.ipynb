{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fesmaro 2025 Analisis Big Data\n",
    "\n",
    "## Proyek Analisis Sentimen: Topic Modeling dengan LDA dan LSA\n",
    "\n",
    "**Tujuan:** Notebook ini bertujuan untuk menerapkan dan mengevaluasi metode Latent Dirichlet Allocation (LDA) dan Latent Semantic Analysis (LSA) pada dataset teks ulasan sebagai bagian dari proyek analisis sentimen. Tujuannya adalah untuk mengidentifikasi topik-topik laten dalam ulasan dan mengeksplorasi bagaimana fitur topik ini dapat digunakan lebih lanjut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup selesai. Library dan data NLTK siap digunakan.\n"
     ]
    }
   ],
   "source": [
    "# Import library dasar\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import os\n",
    "import time # Untuk mengukur waktu eksekusi\n",
    "\n",
    "# Import library NLTK untuk pemrosesan teks\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Import library Gensim untuk LSA dan LDA\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel, LsiModel, TfidfModel, CoherenceModel\n",
    "\n",
    "# Import library untuk visualisasi LDA\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "\n",
    "# Import gdown untuk download data dari Google Drive\n",
    "import gdown\n",
    "\n",
    "# Konfigurasi tampilan\n",
    "# %matplotlib inline\n",
    "# pd.set_option('display.max_colwidth', 200) # Tampilkan lebih banyak teks di kolom pandas\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning) # Abaikan warning Deprecation\n",
    "# pyLDAvis.enable_notebook() # Aktifkan visualisasi pyLDAvis di notebook\n",
    "\n",
    "# Download NLTK data (jika belum ada)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Mengunduh NLTK 'punkt'...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    print(\"Mengunduh NLTK 'stopwords'...\")\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"Setup selesai. Library dan data NLTK siap digunakan.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA LOADING & INITIAL EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "1. MEMUAT DAN MENGEKSPLORASI DATA\n",
      "============================================================\n",
      "Mengunduh data dari Google Drive folder: https://drive.google.com/drive/u/4/folders/1La029vITSdOyDC-TaKZ9kxVk0twOneFv\n",
      "File yang berhasil diunduh: ['/kaggle/working/data\\\\df_processed.csv', '/kaggle/working/data\\\\df_test.csv', '/kaggle/working/data\\\\df_train.csv', '/kaggle/working/data\\\\df_val.csv', '/kaggle/working/data\\\\final_df.csv']\n",
      "\n",
      "Memuat data dari: /kaggle/working/data\\final_df.csv\n",
      "Data berhasil dimuat. Jumlah baris: 100000, Jumlah kolom: 11\n",
      "\n",
      "--- Informasi Data ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 11 columns):\n",
      " #   Column                Non-Null Count   Dtype  \n",
      "---  ------                --------------   -----  \n",
      " 0   label                 100000 non-null  int64  \n",
      " 1   lemmatized_text       100000 non-null  object \n",
      " 2   text_length           100000 non-null  int64  \n",
      " 3   word_count            100000 non-null  int64  \n",
      " 4   avg_word_length       100000 non-null  float64\n",
      " 5   exclamation_count     100000 non-null  int64  \n",
      " 6   question_count        100000 non-null  int64  \n",
      " 7   uppercase_ratio       100000 non-null  float64\n",
      " 8   positive_word_count   100000 non-null  int64  \n",
      " 9   negative_word_count   100000 non-null  int64  \n",
      " 10  sentiment_word_ratio  100000 non-null  float64\n",
      "dtypes: float64(3), int64(7), object(1)\n",
      "memory usage: 8.4+ MB\n",
      "\n",
      "--- 5 Baris Pertama Data ---\n",
      "   label                                    lemmatized_text  text_length  \\\n",
      "0      1  use book product herb easy access material giv...          357   \n",
      "1      1  cd great example heavy hardcore softer edge no...          552   \n",
      "2      0  not read book yet review sample kindle point s...          373   \n",
      "3      1  movie one favorite partly michigan favorite so...          558   \n",
      "4      1  fun ten barrel monkey movie escapist product s...          391   \n",
      "\n",
      "   word_count  avg_word_length  exclamation_count  question_count  \\\n",
      "0          63         4.682540                  0               0   \n",
      "1         100         4.530000                  0               0   \n",
      "2          74         4.054054                  1               0   \n",
      "3         105         4.323810                  1               0   \n",
      "4          63         5.222222                  2               0   \n",
      "\n",
      "   uppercase_ratio  positive_word_count  negative_word_count  \\\n",
      "0         0.000000                    1                    0   \n",
      "1         0.009901                    2                    0   \n",
      "2         0.000000                    0                    0   \n",
      "3         0.000000                    3                    0   \n",
      "4         0.000000                    1                    0   \n",
      "\n",
      "   sentiment_word_ratio  \n",
      "0                   2.0  \n",
      "1                   3.0  \n",
      "2                   1.0  \n",
      "3                   4.0  \n",
      "4                   2.0  \n",
      "\n",
      "--- Statistik Deskriptif (Fitur Numerik) ---\n",
      "               label   text_length    word_count  avg_word_length  \\\n",
      "count  100000.000000  100000.00000  100000.00000    100000.000000   \n",
      "mean        0.500000     394.28408      72.45959         4.473468   \n",
      "std         0.500003     102.18181      18.98513         0.411110   \n",
      "min         0.000000     230.00000      42.00000         2.486842   \n",
      "25%         0.000000     306.00000      56.00000         4.190476   \n",
      "50%         0.500000     385.00000      71.00000         4.434211   \n",
      "75%         1.000000     478.00000      87.00000         4.712500   \n",
      "max         1.000000     594.00000     127.00000         8.596154   \n",
      "\n",
      "       exclamation_count  question_count  uppercase_ratio  \\\n",
      "count      100000.000000   100000.000000    100000.000000   \n",
      "mean            0.920030        0.159090         0.020918   \n",
      "std             2.648752        0.676051         0.087185   \n",
      "min             0.000000        0.000000         0.000000   \n",
      "25%             0.000000        0.000000         0.000000   \n",
      "50%             0.000000        0.000000         0.000000   \n",
      "75%             1.000000        0.000000         0.017241   \n",
      "max           190.000000       43.000000         0.988764   \n",
      "\n",
      "       positive_word_count  negative_word_count  sentiment_word_ratio  \n",
      "count        100000.000000         100000.00000         100000.000000  \n",
      "mean              0.907190              0.29315              1.710844  \n",
      "std               1.105047              0.62275              1.153240  \n",
      "min               0.000000              0.00000              0.125000  \n",
      "25%               0.000000              0.00000              1.000000  \n",
      "50%               1.000000              0.00000              1.000000  \n",
      "75%               1.000000              0.00000              2.000000  \n",
      "max              12.000000              8.00000             13.000000  \n",
      "\n",
      "--- Cek Missing Values ---\n",
      "label                   0\n",
      "lemmatized_text         0\n",
      "text_length             0\n",
      "word_count              0\n",
      "avg_word_length         0\n",
      "exclamation_count       0\n",
      "question_count          0\n",
      "uppercase_ratio         0\n",
      "positive_word_count     0\n",
      "negative_word_count     0\n",
      "sentiment_word_ratio    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"1. MEMUAT DAN MENGEKSPLORASI DATA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- Unduh Data dari Google Drive ---\n",
    "# Pastikan folder 'data' ada atau buat jika belum\n",
    "output_folder = '/kaggle/working/data'\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "    print(f\"Folder '{output_folder}' dibuat.\")\n",
    "\n",
    "# URL folder Google Drive (pastikan link ini benar dan dapat diakses)\n",
    "drive_folder_url = \"https://drive.google.com/drive/u/4/folders/1La029vITSdOyDC-TaKZ9kxVk0twOneFv\"\n",
    "print(f\"Mengunduh data dari Google Drive folder: {drive_folder_url}\")\n",
    "try:\n",
    "    downloaded_files = gdown.download_folder(drive_folder_url, output=output_folder, quiet=True, use_cookies=False)\n",
    "    print(f\"File yang berhasil diunduh: {downloaded_files}\")\n",
    "    if not downloaded_files:\n",
    "        raise ValueError(\"Tidak ada file yang diunduh. Periksa URL dan hak akses Google Drive.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saat mengunduh data: {e}\")\n",
    "    print(\"Pastikan URL folder Google Drive benar dan folder/file dapat diakses (sharing settings).\")\n",
    "    # Anda mungkin perlu menangani error ini lebih lanjut, misalnya dengan menghentikan eksekusi\n",
    "    # exit()\n",
    "\n",
    "# --- Muat Data dari CSV ---\n",
    "data_path = os.path.join(output_folder, \"final_df.csv\")\n",
    "print(f\"\\nMemuat data dari: {data_path}\")\n",
    "try:\n",
    "    df = pd.read_csv(data_path)\n",
    "    print(f\"Data berhasil dimuat. Jumlah baris: {len(df)}, Jumlah kolom: {len(df.columns)}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File tidak ditemukan di '{data_path}'. Pastikan file CSV ada di folder 'data'.\")\n",
    "    # exit() # Hentikan eksekusi jika file tidak ditemukan\n",
    "\n",
    "# --- Eksplorasi Awal ---\n",
    "print(\"\\n--- Informasi Data ---\")\n",
    "df.info()\n",
    "\n",
    "print(\"\\n--- 5 Baris Pertama Data ---\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\n--- Statistik Deskriptif (Fitur Numerik) ---\")\n",
    "# Hanya tampilkan statistik untuk kolom numerik yang relevan untuk analisis awal\n",
    "numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "print(df[numeric_cols].describe())\n",
    "\n",
    "print(\"\\n--- Cek Missing Values ---\")\n",
    "print(df.isnull().sum())\n",
    "# Jika ada missing values di 'lemmatized_text', perlu penanganan (misal, diisi string kosong atau dihapus)\n",
    "if df['lemmatized_text'].isnull().any():\n",
    "    print(\"\\nWarning: Terdapat missing values di kolom 'lemmatized_text'. Mengisinya dengan string kosong.\")\n",
    "    df['lemmatized_text'].fillna('', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. PREPROCESSING TEKS\n",
      "============================================================\n",
      "Jumlah stopwords: 204\n",
      "Contoh stopwords: ['wouldn', 'too', 'll', \"that'll\", 'ain', \"doesn't\", 'we', \"he'll\", \"we've\", \"he's\"]\n",
      "\n",
      "Memulai preprocessing teks (tokenisasi & stopword removal)...\n",
      "Preprocessing selesai dalam 1828.84 detik.\n",
      "\n",
      "--- Contoh Hasil Preprocessing (Kolom 'tokens') ---\n",
      "                                     lemmatized_text  \\\n",
      "0  use book product herb easy access material giv...   \n",
      "1  cd great example heavy hardcore softer edge no...   \n",
      "2  not read book yet review sample kindle point s...   \n",
      "3  movie one favorite partly michigan favorite so...   \n",
      "4  fun ten barrel monkey movie escapist product s...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [book, product, herb, easy, access, material, ...  \n",
      "1  [great, example, heavy, hardcore, softer, edge...  \n",
      "2  [read, book, yet, review, sample, kindle, poin...  \n",
      "3  [movie, favorite, partly, michigan, favorite, ...  \n",
      "4  [fun, ten, barrel, monkey, movie, escapist, pr...  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. PREPROCESSING TEKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- Definisikan Stopwords ---\n",
    "# Menggunakan stopwords bahasa Inggris dari NLTK\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Tambahkan custom stopwords jika perlu (misalnya, kata yang sangat umum di domain Anda tapi tidak informatif)\n",
    "custom_stopwords = {'number', 'also', 'get', 'use', 'make', 'one'} # Contoh\n",
    "stop_words.update(custom_stopwords)\n",
    "print(f\"Jumlah stopwords: {len(stop_words)}\")\n",
    "print(f\"Contoh stopwords: {list(stop_words)[:10]}\")\n",
    "\n",
    "# --- Fungsi Preprocessing ---\n",
    "# Meskipun input sudah 'lemmatized_text', kita tetap lakukan tokenisasi dan stopword removal\n",
    "# Ini memastikan format token yang konsisten dan menghilangkan noise.\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Membersihkan, melakukan tokenisasi, dan menghapus stopwords dari teks.\n",
    "     diasumsikan input sudah lowercase dan lemmatized.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return [] # Kembalikan list kosong jika input bukan string\n",
    "\n",
    "    # 1. Tokenisasi (sudah ada di notebook asli, kita pastikan lagi)\n",
    "    tokens = word_tokenize(text.lower()) # Pastikan lowercase\n",
    "\n",
    "    # 2. Hapus Stopwords dan token non-alphanumeric atau terlalu pendek\n",
    "    processed_tokens = [word for word in tokens if word.isalpha() and word not in stop_words and len(word) > 2]\n",
    "\n",
    "    return processed_tokens\n",
    "\n",
    "# --- Terapkan Preprocessing ---\n",
    "print(\"\\nMemulai preprocessing teks (tokenisasi & stopword removal)...\")\n",
    "start_time = time.time()\n",
    "# Terapkan fungsi ke kolom 'lemmatized_text'\n",
    "# Kolom baru 'tokens' akan berisi list of strings (token)\n",
    "df['tokens'] = df['lemmatized_text'].apply(preprocess_text)\n",
    "end_time = time.time()\n",
    "print(f\"Preprocessing selesai dalam {end_time - start_time:.2f} detik.\")\n",
    "\n",
    "# --- Tampilkan Hasil Preprocessing ---\n",
    "print(\"\\n--- Contoh Hasil Preprocessing (Kolom 'tokens') ---\")\n",
    "print(df[['lemmatized_text', 'tokens']].head())\n",
    "\n",
    "# Filter dokumen yang mungkin menjadi kosong setelah preprocessing\n",
    "original_count = len(df)\n",
    "df = df[df['tokens'].apply(len) > 0]\n",
    "new_count = len(df)\n",
    "if new_count < original_count:\n",
    "    print(f\"\\nWarning: {original_count - new_count} dokumen dihapus karena kosong setelah preprocessing.\")\n",
    "\n",
    "# Pastikan kita masih punya data\n",
    "if df.empty:\n",
    "    print(\"\\nError: Tidak ada dokumen tersisa setelah preprocessing. Periksa data input dan langkah preprocessing.\")\n",
    "    # exit()\n",
    "\n",
    "tokenized_texts = df['tokens'].tolist() # Ambil list token untuk Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "3. FEATURE ENGINEERING (GENSIM)\n",
      "============================================================\n",
      "Membuat Gensim Dictionary...\n",
      "Ukuran dictionary awal: 124918 token unik.\n",
      "Filtering dictionary: Hapus token yang muncul di < 5 dokumen atau > 50.0% dokumen.\n",
      "Ukuran dictionary setelah filtering: 20105 token unik.\n",
      "\n",
      "Membuat Corpus Bag-of-Words (BoW)...\n",
      "Membuat Corpus TF-IDF...\n",
      "\n",
      "Corpus BoW dan TF-IDF berhasil dibuat untuk 100000 dokumen.\n",
      "Contoh representasi BoW dokumen pertama: [(0, 1), (1, 3), (2, 3), (3, 2), (4, 2), (5, 1), (6, 2), (7, 1), (8, 1), (9, 1)]\n",
      "Contoh representasi TF-IDF dokumen pertama: [(0, 0.21998350420090385), (1, 0.14926716905598086), (2, 0.3409594639479748), (3, 0.2378154149200219), (4, 0.3410049630684199), (5, 0.1062925172673465), (6, 0.5413356034202567), (7, 0.3127771262612593), (8, 0.17827303598323577), (9, 0.11192820963143983)]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. FEATURE ENGINEERING (GENSIM)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- Buat Dictionary ---\n",
    "# Dictionary memetakan setiap kata unik ke sebuah ID.\n",
    "print(\"Membuat Gensim Dictionary...\")\n",
    "dictionary = Dictionary(tokenized_texts)\n",
    "print(f\"Ukuran dictionary awal: {len(dictionary)} token unik.\")\n",
    "\n",
    "# --- Filter Ekstrem (Sangat Penting!) ---\n",
    "# Menghapus kata yang terlalu jarang atau terlalu sering muncul.\n",
    "# no_below: minimal jumlah dokumen tempat kata harus muncul.\n",
    "# no_above: maksimal proporsi dokumen tempat kata boleh muncul.\n",
    "# keep_n: simpan hanya N kata paling frekuen (opsional, bisa None).\n",
    "min_doc_freq = 5 # Kata harus muncul di minimal 5 dokumen\n",
    "max_doc_freq_ratio = 0.5 # Kata tidak boleh muncul di lebih dari 50% dokumen\n",
    "print(f\"Filtering dictionary: Hapus token yang muncul di < {min_doc_freq} dokumen atau > {max_doc_freq_ratio*100}% dokumen.\")\n",
    "dictionary.filter_extremes(no_below=min_doc_freq, no_above=max_doc_freq_ratio, keep_n=None) # keep_n=100000 jika ingin membatasi vocab size\n",
    "print(f\"Ukuran dictionary setelah filtering: {len(dictionary)} token unik.\")\n",
    "\n",
    "# --- Buat Corpus Bag-of-Words (BoW) ---\n",
    "# Corpus adalah representasi vektor dari setiap dokumen.\n",
    "# Untuk BoW, ini adalah list of (token_id, frequency).\n",
    "print(\"\\nMembuat Corpus Bag-of-Words (BoW)...\")\n",
    "corpus_bow = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "\n",
    "# --- Buat Corpus TF-IDF ---\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency) memberi bobot lebih pada kata yang penting.\n",
    "# Seringkali lebih baik untuk LSA.\n",
    "print(\"Membuat Corpus TF-IDF...\")\n",
    "tfidf_model = TfidfModel(corpus_bow) # Latih model TF-IDF pada corpus BoW\n",
    "corpus_tfidf = tfidf_model[corpus_bow] # Terapkan transformasi TF-IDF\n",
    "\n",
    "# --- Validasi Corpus ---\n",
    "if not corpus_bow or all(not doc for doc in corpus_bow):\n",
    "    print(\"\\nError: Corpus BoW kosong setelah filtering/pembuatan. Periksa parameter filter_extremes atau data.\")\n",
    "    # exit()\n",
    "elif not dictionary:\n",
    "     print(\"\\nError: Dictionary kosong setelah filtering. Periksa parameter filter_extremes atau data.\")\n",
    "     # exit()\n",
    "else:\n",
    "    print(f\"\\nCorpus BoW dan TF-IDF berhasil dibuat untuk {len(corpus_bow)} dokumen.\")\n",
    "    # Tampilkan contoh representasi dokumen pertama\n",
    "    print(\"Contoh representasi BoW dokumen pertama:\", corpus_bow[0][:10]) # Tampilkan 10 elemen pertama\n",
    "    print(\"Contoh representasi TF-IDF dokumen pertama:\", corpus_tfidf[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LATENT DIRICHLET ALLOCATION (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "4. LATENT DIRICHLET ALLOCATION (LDA)\n",
      "============================================================\n",
      "\n",
      "Perhitungan Coherence Score dilewati. Menggunakan jumlah topik yang ditentukan manual.\n",
      "Jumlah topik LDA diatur ke: 5\n",
      "\n",
      "Melatih model LDA final dengan 5 topik...\n",
      "Pelatihan LDA selesai dalam 297.99 detik.\n",
      "\n",
      "--- Topik LDA (Top 10 kata per topik) ---\n",
      "Topik 0: 0.015*\"would\" + 0.012*\"great\" + 0.010*\"work\" + 0.010*\"time\" + 0.009*\"bought\" + 0.009*\"good\" + 0.008*\"like\" + 0.007*\"year\" + 0.007*\"buy\" + 0.006*\"money\"\n",
      "Topik 1: 0.023*\"movie\" + 0.018*\"like\" + 0.014*\"album\" + 0.013*\"song\" + 0.012*\"great\" + 0.012*\"good\" + 0.011*\"music\" + 0.010*\"love\" + 0.010*\"best\" + 0.008*\"really\"\n",
      "Topik 2: 0.071*\"book\" + 0.023*\"read\" + 0.013*\"story\" + 0.008*\"good\" + 0.008*\"would\" + 0.007*\"reading\" + 0.007*\"life\" + 0.007*\"great\" + 0.006*\"many\" + 0.006*\"time\"\n",
      "Topik 3: 0.034*\"game\" + 0.017*\"dvd\" + 0.012*\"work\" + 0.010*\"version\" + 0.010*\"play\" + 0.009*\"phone\" + 0.009*\"player\" + 0.009*\"camera\" + 0.008*\"system\" + 0.008*\"computer\"\n",
      "Topik 4: 0.017*\"dog\" + 0.013*\"water\" + 0.012*\"coffee\" + 0.011*\"food\" + 0.011*\"love\" + 0.011*\"cat\" + 0.009*\"taste\" + 0.009*\"smell\" + 0.008*\"bottle\" + 0.008*\"recipe\"\n",
      "\n",
      "Menghitung Coherence Score (C_v) untuk model LDA final...\n",
      "Coherence Score (C_v) Model Final LDA: 0.3983\n",
      "\n",
      "Menyiapkan visualisasi pyLDAvis...\n",
      "Visualisasi siap ditampilkan.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"4. LATENT DIRICHLET ALLOCATION (LDA)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- 4.1. Menentukan Jumlah Topik Optimal (Opsional tapi Direkomendasikan) ---\n",
    "# Menggunakan Coherence Score (C_v) untuk membantu memilih jumlah topik.\n",
    "# Ini bisa memakan waktu cukup lama pada dataset besar.\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Menghitung skor koherensi C_v untuk berbagai jumlah topik LDA.\n",
    "\n",
    "    Args:\n",
    "        dictionary (Dictionary): Gensim dictionary.\n",
    "        corpus (list): Gensim corpus (BoW).\n",
    "        texts (list): List of tokenized texts.\n",
    "        limit (int): Jumlah topik maksimal untuk diuji.\n",
    "        start (int): Jumlah topik awal.\n",
    "        step (int): Kenaikan jumlah topik.\n",
    "\n",
    "    Returns:\n",
    "        list: List skor koherensi.\n",
    "        list: List jumlah topik yang diuji.\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    num_topics_range = range(start, limit, step)\n",
    "    total_models = len(num_topics_range)\n",
    "    print(f\"Menghitung Coherence Score (C_v) untuk {total_models} model LDA (jumlah topik dari {start} sampai {limit-1})...\")\n",
    "\n",
    "    for i, num_topics in enumerate(num_topics_range):\n",
    "        model_start_time = time.time()\n",
    "        # Gunakan LdaMulticore untuk mempercepat jika > 1 core CPU tersedia\n",
    "        # workers = max(1, os.cpu_count() - 1) # Gunakan semua core kecuali 1\n",
    "        # model = LdaMulticore(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "        #                      passes=10, workers=workers, random_state=42)\n",
    "        model = LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary,\n",
    "                         passes=10, random_state=42, alpha='auto', eta='auto') # passes bisa disesuaikan\n",
    "\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_score = coherencemodel.get_coherence()\n",
    "        coherence_values.append(coherence_score)\n",
    "        model_end_time = time.time()\n",
    "        print(f\"Model {i+1}/{total_models}: Num Topics = {num_topics}, Coherence = {coherence_score:.4f}, Time = {model_end_time - model_start_time:.2f}s\")\n",
    "\n",
    "    return coherence_values, list(num_topics_range)\n",
    "\n",
    "# --- Jalankan Perhitungan Koherensi (Sesuaikan range jika perlu) ---\n",
    "# PERINGATAN: Ini bisa sangat lama (beberapa menit hingga jam tergantung data & hardware)\n",
    "# Jika terlalu lama, Anda bisa skip bagian ini dan langsung set num_topics_lda secara manual.\n",
    "COMPUTE_COHERENCE = False # Set ke True untuk menjalankan, False untuk skip\n",
    "NUM_TOPICS_START = 2\n",
    "NUM_TOPICS_LIMIT = 15 # Coba sampai 14 topik (misalnya)\n",
    "NUM_TOPICS_STEP = 2\n",
    "coherence_values_lda = []\n",
    "num_topics_range_lda = []\n",
    "\n",
    "if COMPUTE_COHERENCE:\n",
    "    coherence_values_lda, num_topics_range_lda = compute_coherence_values(\n",
    "        dictionary=dictionary,\n",
    "        corpus=corpus_bow, # LDA biasanya pakai BoW\n",
    "        texts=tokenized_texts,\n",
    "        start=NUM_TOPICS_START,\n",
    "        limit=NUM_TOPICS_LIMIT,\n",
    "        step=NUM_TOPICS_STEP\n",
    "    )\n",
    "\n",
    "    # --- Plot Coherence Scores ---\n",
    "    if coherence_values_lda:\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(num_topics_range_lda, coherence_values_lda, marker='o')\n",
    "        plt.title('LDA Coherence Score (C_v) vs. Number of Topics')\n",
    "        plt.xlabel(\"Number of Topics\")\n",
    "        plt.ylabel(\"Coherence Score (C_v)\")\n",
    "        plt.xticks(num_topics_range_lda)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "        # Pilih jumlah topik dengan skor koherensi tertinggi\n",
    "        optimal_lda_topics_index = np.argmax(coherence_values_lda)\n",
    "        optimal_num_topics_lda = num_topics_range_lda[optimal_lda_topics_index]\n",
    "        print(f\"\\nJumlah topik optimal berdasarkan C_v: {optimal_num_topics_lda} (Coherence: {coherence_values_lda[optimal_lda_topics_index]:.4f})\")\n",
    "    else:\n",
    "        print(\"\\nTidak ada skor koherensi yang dihitung.\")\n",
    "        optimal_num_topics_lda = 5 # Default jika tidak dihitung\n",
    "        print(f\"Menggunakan jumlah topik default: {optimal_num_topics_lda}\")\n",
    "else:\n",
    "    print(\"\\nPerhitungan Coherence Score dilewati. Menggunakan jumlah topik yang ditentukan manual.\")\n",
    "    # Tentukan jumlah topik secara manual jika tidak menghitung koherensi\n",
    "    # Nilai ini HARUS disesuaikan berdasarkan pemahaman domain atau hasil eksperimen sebelumnya.\n",
    "    optimal_num_topics_lda = 5 # Contoh: pilih 5 topik\n",
    "    print(f\"Jumlah topik LDA diatur ke: {optimal_num_topics_lda}\")\n",
    "\n",
    "# --- 4.2. Melatih Model LDA Final ---\n",
    "print(f\"\\nMelatih model LDA final dengan {optimal_num_topics_lda} topik...\")\n",
    "start_time = time.time()\n",
    "# Gunakan parameter yang sama seperti saat tuning (jika dilakukan)\n",
    "# passes=15 atau 20 mungkin lebih baik untuk hasil akhir\n",
    "lda_model = LdaModel(corpus=corpus_bow,\n",
    "                     id2word=dictionary,\n",
    "                     num_topics=optimal_num_topics_lda,\n",
    "                     random_state=42,\n",
    "                     passes=15, # Lebih banyak pass untuk model final\n",
    "                     alpha='auto',\n",
    "                     eta='auto',\n",
    "                     # chunksize=2000, # Default gensim, sesuaikan jika perlu\n",
    "                     # update_every=1, # Default: update model setelah setiap chunk (online)\n",
    "                     # eval_every=None # Default: jangan evaluasi perplexity selama training\n",
    "                    )\n",
    "end_time = time.time()\n",
    "print(f\"Pelatihan LDA selesai dalam {end_time - start_time:.2f} detik.\")\n",
    "\n",
    "# --- 4.3. Menampilkan Topik LDA ---\n",
    "print(f\"\\n--- Topik LDA (Top {10} kata per topik) ---\")\n",
    "# `num_words`: Jumlah kata kunci yang ditampilkan per topik.\n",
    "topics_lda = lda_model.print_topics(num_topics=optimal_num_topics_lda, num_words=10)\n",
    "for topic_num, topic_words in topics_lda:\n",
    "    print(f\"Topik {topic_num}: {topic_words}\")\n",
    "\n",
    "# --- 4.4. Evaluasi Kualitas Topik (Coherence Score) ---\n",
    "print(\"\\nMenghitung Coherence Score (C_v) untuk model LDA final...\")\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_texts, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(f'Coherence Score (C_v) Model Final LDA: {coherence_lda:.4f}')\n",
    "# Skor C_v yang baik biasanya > 0.5, tetapi sangat tergantung pada dataset.\n",
    "\n",
    "# --- 4.5. Visualisasi Topik LDA dengan pyLDAvis ---\n",
    "print(\"\\nMenyiapkan visualisasi pyLDAvis...\")\n",
    "# PERINGATAN: Ini juga bisa memakan waktu dan memori, terutama untuk data besar.\n",
    "# Jika error atau terlalu lambat, coba kurangi jumlah data sampel untuk visualisasi.\n",
    "try:\n",
    "    vis_data = gensimvis.prepare(lda_model, corpus_bow, dictionary, mds='tsne') # 'tsne' seringkali lebih baik dari 'pca'\n",
    "    print(\"Visualisasi siap ditampilkan.\")\n",
    "    # Tampilkan visualisasi di notebook (jika pyLDAvis.enable_notebook() dipanggil di awal)\n",
    "    # Atau simpan ke file HTML:\n",
    "    # pyLDAvis.save_html(vis_data, 'lda_visualization.html')\n",
    "    # print(\"Visualisasi disimpan sebagai 'lda_visualization.html'\")\n",
    "    pyLDAvis.display(vis_data) # Baris ini akan menampilkan visualisasi di output sel notebook\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saat membuat visualisasi pyLDAvis: {e}\")\n",
    "    print(\"Coba jalankan ulang atau periksa instalasi pyLDAvis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LATENT SEMANTIC ANALYSIS (LSA / LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "5. LATENT SEMANTIC ANALYSIS (LSA / LSI)\n",
      "============================================================\n",
      "Jumlah topik LSA diatur ke: 5\n",
      "\n",
      "Melatih model LSA (LSI) menggunakan corpus TF-IDF...\n",
      "Pelatihan LSA (LSI) selesai dalam 8.69 detik.\n",
      "\n",
      "--- Topik LSA (Top 10 kata per topik) ---\n",
      "Topik 0: book\", movie\", read\", great\", good\", like\", would\", time\", really\", love\"\n",
      "Topik 1: book\", read\", movie\", album\", song\", game\", reading\", music\", dvd\", author\"\n",
      "Topik 2: movie\", album\", film\", game\", song\", watch\", story\", work\", seen\", dvd\"\n",
      "Topik 3: game\", album\", song\", music\", band\", track\", listen\", play\", best\", sound\"\n",
      "Topik 4: game\", album\", song\", work\", battery\", play\", fun\", book\", item\", quality\"\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"5. LATENT SEMANTIC ANALYSIS (LSA / LSI)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# --- 5.1. Menentukan Jumlah Topik LSA ---\n",
    "# Tidak ada metrik sepopuler Coherence Score untuk LSA.\n",
    "# Jumlah topik sering dipilih berdasarkan tujuan (misalnya, reduksi dimensi)\n",
    "# atau dievaluasi secara ekstrinsik (misalnya, performa klasifikasi).\n",
    "# Kita akan gunakan jumlah topik yang sama dengan LDA untuk perbandingan, atau pilih nilai lain.\n",
    "num_topics_lsa = optimal_num_topics_lda # Menggunakan jumlah topik yang sama dengan LDA\n",
    "# Atau pilih jumlah yang berbeda, misal:\n",
    "# num_topics_lsa = 100 # LSA kadang bekerja baik dengan dimensi lebih tinggi awal\n",
    "print(f\"Jumlah topik LSA diatur ke: {num_topics_lsa}\")\n",
    "\n",
    "# --- 5.2. Melatih Model LSA (LSI) ---\n",
    "# LSA/LSI seringkali lebih baik menggunakan input TF-IDF.\n",
    "print(\"\\nMelatih model LSA (LSI) menggunakan corpus TF-IDF...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    lsi_model = LsiModel(corpus=corpus_tfidf, # Gunakan corpus TF-IDF\n",
    "                         id2word=dictionary,\n",
    "                         num_topics=num_topics_lsa\n",
    "                         # decay=1.0, # Default\n",
    "                         # onepass=False, # Multi-pass SVD bisa lebih akurat tapi lebih lambat/memori intensif\n",
    "                         # power_iters=2, # Iterasi tambahan untuk SVD acak\n",
    "                         # extra_samples=100 # Sampel tambahan untuk SVD acak\n",
    "                        )\n",
    "    end_time = time.time()\n",
    "    print(f\"Pelatihan LSA (LSI) selesai dalam {end_time - start_time:.2f} detik.\")\n",
    "\n",
    "    # --- 5.3. Menampilkan Topik LSA ---\n",
    "    print(f\"\\n--- Topik LSA (Top {10} kata per topik) ---\")\n",
    "    topics_lsa = lsi_model.print_topics(num_topics=num_topics_lsa, num_words=10)\n",
    "    for topic_num, topic_words in topics_lsa:\n",
    "        # Bersihkan format output LSI\n",
    "        cleaned_topic = re.sub(r'-?\\d+\\.\\d+\\*?\"', '', topic_words).replace(' + ', ', ').strip()\n",
    "        print(f\"Topik {topic_num}: {cleaned_topic}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError saat melatih model LSI: {e}\")\n",
    "    print(\"Ini mungkin terjadi pada corpus yang sangat kecil atau sparse. Coba sesuaikan parameter atau periksa data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MENGGUNAKAN FITUR TOPIK UNTUK ANALISIS SENTIMEN (KONSEP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "6. PENGGUNAAN FITUR TOPIK UNTUK KLASIFIKASI SENTIMEN\n",
      "============================================================\n",
      "\n",
      "Topic modeling (LDA/LSA) sendiri tidak secara langsung melakukan klasifikasi sentimen.\n",
      "Namun, hasil dari topic modeling dapat digunakan sebagai fitur tambahan atau pengganti\n",
      "untuk model klasifikasi sentimen (seperti Logistic Regression, SVM, dll.).\n",
      "\n",
      "Caranya adalah dengan merepresentasikan setiap dokumen (ulasan) sebagai distribusi\n",
      "probabilitas atas topik-topik yang telah ditemukan. Vektor distribusi topik ini\n",
      "kemudian menjadi fitur input untuk classifier.\n",
      "\n",
      "Mendapatkan distribusi topik LDA untuk setiap dokumen...\n",
      "Contoh fitur LDA (5 baris pertama):\n",
      "   LDA_Topic_0  LDA_Topic_1  LDA_Topic_2  LDA_Topic_3  LDA_Topic_4\n",
      "0     0.066440     0.022693     0.647598     0.110501     0.152769\n",
      "1     0.193998     0.748886     0.049634     0.004449     0.003032\n",
      "2     0.127234     0.104365     0.756809     0.006939     0.004653\n",
      "3     0.064210     0.761371     0.137502     0.004928     0.031989\n",
      "4     0.034521     0.840549     0.114455     0.006189     0.004286\n",
      "\n",
      "Mendapatkan representasi vektor LSA untuk setiap dokumen...\n",
      "Contoh fitur LSA (5 baris pertama):\n",
      "   LSA_Topic_0  LSA_Topic_1  LSA_Topic_2  LSA_Topic_3  LSA_Topic_4\n",
      "0     0.136011    -0.137581     0.018471     0.002674    -0.001942\n",
      "1     0.146094     0.110679     0.078782    -0.133570    -0.061816\n",
      "2     0.151312    -0.150183    -0.008094    -0.015683    -0.019236\n",
      "3     0.137854     0.103944    -0.211908     0.012600    -0.035960\n",
      "4     0.134042     0.048519    -0.101636    -0.010036    -0.024865\n",
      "\n",
      "Langkah Selanjutnya:\n",
      "1.  **Gabungkan Fitur:** Gabungkan fitur topik (LDA atau LSA) dengan DataFrame asli yang berisi label sentimen ('label').\n",
      "    ```python\n",
      "    df_final_features = pd.concat([df[['label']], lda_features_df], axis=1) # Atau lsa_features_df\n",
      "    ```\n",
      "2.  **Split Data:** Bagi data menjadi set pelatihan, validasi, dan pengujian (jika belum dilakukan atau menggunakan file terpisah df_train, df_val, df_test). Pastikan menggunakan index yang sama jika menggabungkan fitur ke split yang sudah ada.\n",
      "    ```python\n",
      "    # Contoh jika menggunakan df_final_features\n",
      "    from sklearn.model_selection import train_test_split\n",
      "    X = df_final_features.drop('label', axis=1)\n",
      "    y = df_final_features['label']\n",
      "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
      "    # Lakukan split lagi untuk validation set jika perlu\n",
      "    ```\n",
      "3.  **Latih Classifier:** Latih model klasifikasi (misalnya, Logistic Regression, SVM, Naive Bayes, RandomForest, XGBoost) menggunakan fitur topik sebagai input (X_train) dan label sentimen sebagai target (y_train).\n",
      "    ```python\n",
      "    from sklearn.linear_model import LogisticRegression\n",
      "    classifier = LogisticRegression(random_state=42)\n",
      "    classifier.fit(X_train, y_train)\n",
      "    ```\n",
      "4.  **Evaluasi Classifier:** Evaluasi performa model pada set validasi/pengujian menggunakan metrik seperti akurasi, presisi, recall, F1-score, dan AUC.\n",
      "    ```python\n",
      "    from sklearn.metrics import classification_report, accuracy_score\n",
      "    y_pred = classifier.predict(X_test)\n",
      "    print(accuracy_score(y_test, y_pred))\n",
      "    print(classification_report(y_test, y_pred))\n",
      "    ```\n",
      "5.  **Eksperimen:** Anda juga bisa mencoba menggabungkan fitur topik dengan fitur lain (misalnya, TF-IDF dari teks asli, fitur numerik yang sudah ada di df) untuk melihat apakah performa meningkat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"6. PENGGUNAAN FITUR TOPIK UNTUK KLASIFIKASI SENTIMEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "Topic modeling (LDA/LSA) sendiri tidak secara langsung melakukan klasifikasi sentimen.\n",
    "Namun, hasil dari topic modeling dapat digunakan sebagai fitur tambahan atau pengganti\n",
    "untuk model klasifikasi sentimen (seperti Logistic Regression, SVM, dll.).\n",
    "\n",
    "Caranya adalah dengan merepresentasikan setiap dokumen (ulasan) sebagai distribusi\n",
    "probabilitas atas topik-topik yang telah ditemukan. Vektor distribusi topik ini\n",
    "kemudian menjadi fitur input untuk classifier.\n",
    "\"\"\")\n",
    "\n",
    "# --- Mendapatkan Distribusi Topik per Dokumen ---\n",
    "\n",
    "# Untuk LDA:\n",
    "print(\"Mendapatkan distribusi topik LDA untuk setiap dokumen...\")\n",
    "lda_topic_distributions = []\n",
    "for bow_doc in corpus_bow:\n",
    "    # get_document_topics mengembalikan list tuple (topic_id, probability)\n",
    "    doc_topics = lda_model.get_document_topics(bow_doc, minimum_probability=0.0)\n",
    "    # Ubah menjadi vektor padat (dense vector) sepanjang num_topics\n",
    "    topic_vector = [0.0] * optimal_num_topics_lda\n",
    "    for topic_id, prob in doc_topics:\n",
    "        if topic_id < optimal_num_topics_lda: # Pastikan ID topik valid\n",
    "             topic_vector[topic_id] = prob\n",
    "    lda_topic_distributions.append(topic_vector)\n",
    "\n",
    "# Buat DataFrame dari distribusi topik LDA\n",
    "lda_features_df = pd.DataFrame(lda_topic_distributions, index=df.index, columns=[f'LDA_Topic_{i}' for i in range(optimal_num_topics_lda)])\n",
    "print(\"Contoh fitur LDA (5 baris pertama):\")\n",
    "print(lda_features_df.head())\n",
    "\n",
    "# Untuk LSA:\n",
    "print(\"\\nMendapatkan representasi vektor LSA untuk setiap dokumen...\")\n",
    "lsa_vectors = []\n",
    "# Terapkan model LSI ke corpus TF-IDF\n",
    "for tfidf_doc in corpus_tfidf:\n",
    "    # Hasil LSI adalah list tuple (topic_id, weight)\n",
    "    lsa_vector_sparse = lsi_model[tfidf_doc]\n",
    "    # Ubah menjadi vektor padat\n",
    "    topic_vector = [0.0] * num_topics_lsa\n",
    "    for topic_id, weight in lsa_vector_sparse:\n",
    "         if topic_id < num_topics_lsa:\n",
    "             topic_vector[topic_id] = weight\n",
    "    lsa_vectors.append(topic_vector)\n",
    "\n",
    "# Buat DataFrame dari vektor LSA\n",
    "lsa_features_df = pd.DataFrame(lsa_vectors, index=df.index, columns=[f'LSA_Topic_{i}' for i in range(num_topics_lsa)])\n",
    "print(\"Contoh fitur LSA (5 baris pertama):\")\n",
    "print(lsa_features_df.head())\n",
    "\n",
    "# --- Menggabungkan Fitur Topik dengan Data Asli ---\n",
    "# Fitur ini sekarang dapat digabungkan kembali dengan DataFrame asli\n",
    "# df_with_lda_features = pd.concat([df, lda_features_df], axis=1)\n",
    "# df_with_lsa_features = pd.concat([df, lsa_features_df], axis=1)\n",
    "# print(\"\\nDataFrame dengan fitur LDA:\")\n",
    "# print(df_with_lda_features.head())\n",
    "\n",
    "# --- Langkah Selanjutnya untuk Klasifikasi Sentimen ---\n",
    "print(\"\"\"\n",
    "Langkah Selanjutnya:\n",
    "1.  **Gabungkan Fitur:** Gabungkan fitur topik (LDA atau LSA) dengan DataFrame asli yang berisi label sentimen ('label').\n",
    "    ```python\n",
    "    df_final_features = pd.concat([df[['label']], lda_features_df], axis=1) # Atau lsa_features_df\n",
    "    ```\n",
    "2.  **Split Data:** Bagi data menjadi set pelatihan, validasi, dan pengujian (jika belum dilakukan atau menggunakan file terpisah df_train, df_val, df_test). Pastikan menggunakan index yang sama jika menggabungkan fitur ke split yang sudah ada.\n",
    "    ```python\n",
    "    # Contoh jika menggunakan df_final_features\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X = df_final_features.drop('label', axis=1)\n",
    "    y = df_final_features['label']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    # Lakukan split lagi untuk validation set jika perlu\n",
    "    ```\n",
    "3.  **Latih Classifier:** Latih model klasifikasi (misalnya, Logistic Regression, SVM, Naive Bayes, RandomForest, XGBoost) menggunakan fitur topik sebagai input (X_train) dan label sentimen sebagai target (y_train).\n",
    "    ```python\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    classifier = LogisticRegression(random_state=42)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    ```\n",
    "4.  **Evaluasi Classifier:** Evaluasi performa model pada set validasi/pengujian menggunakan metrik seperti akurasi, presisi, recall, F1-score, dan AUC.\n",
    "    ```python\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    ```\n",
    "5.  **Eksperimen:** Anda juga bisa mencoba menggabungkan fitur topik dengan fitur lain (misalnya, TF-IDF dari teks asli, fitur numerik yang sudah ada di df) untuk melihat apakah performa meningkat.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KESIMPULAN & LANGKAH SELANJUTNYA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "7. KESIMPULAN & LANGKAH SELANJUTNYA\n",
      "============================================================\n",
      "\n",
      "Notebook ini telah mendemonstrasikan alur kerja untuk menerapkan LDA dan LSA/LSI pada data teks ulasan:\n",
      "1.  Data dimuat dan dieksplorasi.\n",
      "2.  Teks diproses dengan tokenisasi dan penghapusan stopwords.\n",
      "3.  Dictionary, Corpus BoW, dan Corpus TF-IDF dibuat menggunakan Gensim.\n",
      "4.  Model LDA dilatih, dievaluasi menggunakan Coherence Score (C_v=0.3983), dan divisualisasikan dengan pyLDAvis. Jumlah topik optimal (berdasarkan C_v atau manual) adalah 5.\n",
      "5.  Model LSA (LSI) dilatih menggunakan input TF-IDF dengan 5 topik.\n",
      "6.  Konsep penggunaan distribusi topik LDA/LSA sebagai fitur untuk klasifikasi sentimen dijelaskan, beserta contoh cara mengekstrak fitur tersebut.\n",
      "\n",
      "Langkah Selanjutnya yang Mungkin:\n",
      "-   **Implementasi Klasifikasi:** Melanjutkan langkah ke-6 untuk melatih dan mengevaluasi model klasifikasi sentimen menggunakan fitur topik.\n",
      "-   **Fine-tuning Parameter:** Melakukan tuning lebih lanjut pada parameter LDA (alpha, eta, passes) dan LSA.\n",
      "-   **Preprocessing Lanjutan:** Mencoba teknik preprocessing lain seperti n-grams (bigrams, trigrams) atau stemming alih-alih lemmatization.\n",
      "-   **Perbandingan Fitur:** Membandingkan performa classifier yang hanya menggunakan fitur topik vs. fitur TF-IDF asli vs. kombinasi keduanya.\n",
      "-   **Analisis Topik Lebih Dalam:** Menganalisis kata-kata dalam setiap topik secara lebih mendalam untuk memberikan interpretasi bisnis atau domain yang relevan.\n",
      "-   **Model Topic Modeling Lain:** Mengeksplorasi model lain seperti NMF (Non-negative Matrix Factorization) atau model yang lebih canggih berbasis deep learning (misalnya, Top2Vec, BERTopic).\n",
      "\n",
      "\n",
      "Analisis selesai.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"7. KESIMPULAN & LANGKAH SELANJUTNYA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\"\"\n",
    "Notebook ini telah mendemonstrasikan alur kerja untuk menerapkan LDA dan LSA/LSI pada data teks ulasan:\n",
    "1.  Data dimuat dan dieksplorasi.\n",
    "2.  Teks diproses dengan tokenisasi dan penghapusan stopwords.\n",
    "3.  Dictionary, Corpus BoW, dan Corpus TF-IDF dibuat menggunakan Gensim.\n",
    "4.  Model LDA dilatih, dievaluasi menggunakan Coherence Score (C_v={coherence_lda:.4f}), dan divisualisasikan dengan pyLDAvis. Jumlah topik optimal (berdasarkan C_v atau manual) adalah {optimal_num_topics_lda}.\n",
    "5.  Model LSA (LSI) dilatih menggunakan input TF-IDF dengan {num_topics_lsa} topik.\n",
    "6.  Konsep penggunaan distribusi topik LDA/LSA sebagai fitur untuk klasifikasi sentimen dijelaskan, beserta contoh cara mengekstrak fitur tersebut.\n",
    "\n",
    "Langkah Selanjutnya yang Mungkin:\n",
    "-   **Implementasi Klasifikasi:** Melanjutkan langkah ke-6 untuk melatih dan mengevaluasi model klasifikasi sentimen menggunakan fitur topik.\n",
    "-   **Fine-tuning Parameter:** Melakukan tuning lebih lanjut pada parameter LDA (alpha, eta, passes) dan LSA.\n",
    "-   **Preprocessing Lanjutan:** Mencoba teknik preprocessing lain seperti n-grams (bigrams, trigrams) atau stemming alih-alih lemmatization.\n",
    "-   **Perbandingan Fitur:** Membandingkan performa classifier yang hanya menggunakan fitur topik vs. fitur TF-IDF asli vs. kombinasi keduanya.\n",
    "-   **Analisis Topik Lebih Dalam:** Menganalisis kata-kata dalam setiap topik secara lebih mendalam untuk memberikan interpretasi bisnis atau domain yang relevan.\n",
    "-   **Model Topic Modeling Lain:** Mengeksplorasi model lain seperti NMF (Non-negative Matrix Factorization) atau model yang lebih canggih berbasis deep learning (misalnya, Top2Vec, BERTopic).\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nAnalisis selesai.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
