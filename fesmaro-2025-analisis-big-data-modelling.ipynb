{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proyek Analisis Big Data - Fesmaro 2025**\n",
    "\n",
    "# **Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Make model\n",
    "- Visualisasi model (torchviz) opsional kalo ada yang lain \n",
    "- Training (CrossEntropy, Accuracy, Validation Accuracy)\n",
    "- Visualisasi Training\n",
    "- Evaluation (pakai classification report, ROC AUC)\n",
    "- Confussion Matrix\n",
    "- Save Model\n",
    "- Inference Model (dalam dashboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT-LSTM-GCN (Ensamble Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "final_df=pd.read_csv('./data/final_df.csv')\n",
    "df_val=pd.read_csv('./data/df_val.csv')\n",
    "df_test = pd.read_csv('./data/df_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.rename(columns={'lemmatized_text': 'text'}, inplace=True)\n",
    "\n",
    "final_df = final_df[['label', 'text']]\n",
    "df_val = df_val[['label', 'text']]\n",
    "df_test = df_test[['label', 'text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harry\\anaconda3\\envs\\torch-nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import spacy\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "# Kelas untuk Graph Convolutional Network (GCN)\n",
    "class GraphConvolution(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer konvolusi sederhana untuk GCN\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        torch.nn.init.xavier_uniform_(self.weight)\n",
    "        if self.bias is not None:\n",
    "            torch.nn.init.zeros_(self.bias)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        # input: [batch_size, seq_len, in_features]\n",
    "        # adj: [batch_size, seq_len, seq_len]\n",
    "        \n",
    "        support = torch.bmm(input, self.weight.expand(input.size(0), -1, -1))\n",
    "        output = torch.bmm(adj, support)\n",
    "        \n",
    "        if self.bias is not None:\n",
    "            output = output + self.bias\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Model Fusion BERT+BiLSTM+GCN\n",
    "class BERTBiLSTMGCNModel(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', hidden_dim=128, gcn_layers=2, \n",
    "                 lstm_layers=1, dropout=0.2, num_classes=2, linguistic_feat_dim=9):\n",
    "        super(BERTBiLSTMGCNModel, self).__init__()\n",
    "        \n",
    "        # Inisialisasi BERT\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.bert_dim = self.bert.config.hidden_size\n",
    "        \n",
    "        # Freeze BERT layers (opsional, untuk efisiensi training)\n",
    "        # for param in self.bert.parameters():\n",
    "        #     param.requires_grad = False\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.bert_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=lstm_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0\n",
    "        )\n",
    "        \n",
    "        # GCN layers\n",
    "        self.gcn_layers = nn.ModuleList()\n",
    "        self.gcn_layers.append(GraphConvolution(self.bert_dim, hidden_dim))\n",
    "        for i in range(1, gcn_layers):\n",
    "            self.gcn_layers.append(GraphConvolution(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Attention layer untuk weighted pooling\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)  # BiLSTM output dim = hidden_dim * 2\n",
    "        \n",
    "        # Classifier untuk BiLSTM\n",
    "        self.bilstm_classifier = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        \n",
    "        # Classifier untuk GCN\n",
    "        self.gcn_classifier = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Fitur linguistik processing\n",
    "        self.linguistic_proj = nn.Linear(linguistic_feat_dim, hidden_dim)\n",
    "        \n",
    "        # Fusion layer dan classifier akhir\n",
    "        self.fusion = nn.Linear(hidden_dim * 3, hidden_dim)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, adj_matrix, linguistic_features):\n",
    "        # BERT forward pass\n",
    "        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = bert_outputs.last_hidden_state  # [batch_size, seq_len, bert_dim]\n",
    "        \n",
    "        # BiLSTM forward pass\n",
    "        lstm_output, _ = self.lstm(sequence_output)  # [batch_size, seq_len, hidden_dim*2]\n",
    "        \n",
    "        # Attention mechanism untuk BiLSTM\n",
    "        attention_weights = F.softmax(self.attention(lstm_output), dim=1)\n",
    "        lstm_pooled = torch.sum(attention_weights * lstm_output, dim=1)  # [batch_size, hidden_dim*2]\n",
    "        \n",
    "        # GCN forward pass\n",
    "        gcn_output = sequence_output\n",
    "        for gcn_layer in self.gcn_layers:\n",
    "            gcn_output = F.leaky_relu(gcn_layer(gcn_output, adj_matrix))\n",
    "        \n",
    "        # Global max pooling untuk GCN\n",
    "        gcn_pooled = torch.max(gcn_output, dim=1)[0]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Process linguistic features\n",
    "        ling_features = F.leaky_relu(self.linguistic_proj(linguistic_features))  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Projecting each component to same dimension\n",
    "        bilstm_features = F.leaky_relu(self.bilstm_classifier(lstm_pooled))  # [batch_size, hidden_dim]\n",
    "        gcn_features = F.leaky_relu(self.gcn_classifier(gcn_pooled))  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Fusion semua fitur\n",
    "        combined = torch.cat([bilstm_features, gcn_features, ling_features], dim=1)  # [batch_size, hidden_dim*3]\n",
    "        fused = F.leaky_relu(self.fusion(combined))  # [batch_size, hidden_dim]\n",
    "        fused = self.dropout(fused)\n",
    "        \n",
    "        # Final classification\n",
    "        logits = self.classifier(fused)  # [batch_size, num_classes]\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Ekstraksi Fitur\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', max_length=128, cache_dir=None):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        self.max_length = max_length\n",
    "        self.cache_dir = cache_dir\n",
    "        \n",
    "        if cache_dir and not os.path.exists(cache_dir):\n",
    "            os.makedirs(cache_dir)\n",
    "    \n",
    "    def prepare_bert_inputs(self, texts, batch=True):\n",
    "        \"\"\"Tokenisasi texts untuk input ke BERT\"\"\"\n",
    "        encodings = self.tokenizer(\n",
    "            texts,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encodings['input_ids'],\n",
    "            'attention_mask': encodings['attention_mask'],\n",
    "        }\n",
    "    \n",
    "    def build_dependency_graph(self, text, use_cache=True):\n",
    "        \"\"\"Membuat graph berdasarkan dependency parsing\"\"\"\n",
    "        if self.cache_dir and use_cache:\n",
    "            # Create hash for caching\n",
    "            import hashlib\n",
    "            text_hash = hashlib.md5(text.encode()).hexdigest()\n",
    "            cache_file = os.path.join(self.cache_dir, f\"dep_graph_{text_hash}.pkl\")\n",
    "            \n",
    "            if os.path.exists(cache_file):\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    return pickle.load(f)\n",
    "        \n",
    "        # Process with SpaCy (limit text length for efficiency)\n",
    "        doc = self.nlp(text[:1000])\n",
    "        \n",
    "        # Get tokens (limited to max_length)\n",
    "        tokens = [token.text.lower() for token in doc][:self.max_length]\n",
    "        n = len(tokens)\n",
    "        \n",
    "        # Initialize adjacency matrix\n",
    "        adjacency_matrix = np.zeros((self.max_length, self.max_length))\n",
    "        \n",
    "        # Fill adjacency matrix based on dependencies\n",
    "        for token in doc:\n",
    "            if token.i < n and token.head.i < n:\n",
    "                # Add edge between token and its head\n",
    "                adjacency_matrix[token.i, token.head.i] = 1\n",
    "                adjacency_matrix[token.head.i, token.i] = 1\n",
    "        \n",
    "        # Add self-loops\n",
    "        adjacency_matrix = adjacency_matrix + np.eye(self.max_length)\n",
    "        \n",
    "        # Normalize adjacency matrix (important for GCN)\n",
    "        rowsum = np.array(adjacency_matrix.sum(1))\n",
    "        d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "        d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "        d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n",
    "        normalized_adj = d_mat_inv_sqrt.dot(adjacency_matrix).dot(d_mat_inv_sqrt)\n",
    "        \n",
    "        result = torch.FloatTensor(normalized_adj)\n",
    "        \n",
    "        # Cache result if cache directory is provided\n",
    "        if self.cache_dir and use_cache:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump(result, f)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def extract_linguistic_features(self, text):\n",
    "        \"\"\"Ekstrak fitur linguistik untuk analisis sentimen\"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Text statistics\n",
    "        features['text_length'] = min(len(text), 1000) / 1000  # Normalized length\n",
    "        words = str(text).split()\n",
    "        features['word_count'] = min(len(words), 200) / 200  # Normalized count\n",
    "        features['avg_word_length'] = min(np.mean([len(word) for word in words]) if words else 0, 20) / 20\n",
    "        \n",
    "        # Sentiment indicators\n",
    "        features['exclamation_count'] = min(text.count('!'), 10) / 10\n",
    "        features['question_count'] = min(text.count('?'), 10) / 10\n",
    "        features['uppercase_word_count'] = min(sum(1 for word in words if word.isupper() and len(word) > 1), 20) / 20\n",
    "        features['uppercase_ratio'] = min(features['uppercase_word_count'] * 200 / (len(words) + 1), 1.0)\n",
    "        \n",
    "        # Sentiment lexicon features\n",
    "        positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'best', 'love', \n",
    "                         'perfect', 'recommend', 'happy', 'awesome']\n",
    "        negative_words = ['bad', 'poor', 'terrible', 'horrible', 'worst', 'waste', 'disappointed', \n",
    "                         'disappointing', 'difficult', 'hate', 'problem', 'issue', 'fail']\n",
    "        \n",
    "        features['positive_word_count'] = min(sum(1 for word in text.lower().split() if word in positive_words), 20) / 20\n",
    "        features['negative_word_count'] = min(sum(1 for word in text.lower().split() if word in negative_words), 20) / 20\n",
    "        features['sentiment_word_ratio'] = min((features['positive_word_count'] * 20 + 1) / \n",
    "                                            (features['negative_word_count'] * 20 + 1), 10) / 10\n",
    "        \n",
    "        return torch.FloatTensor([\n",
    "            features['text_length'], \n",
    "            features['word_count'],\n",
    "            features['avg_word_length'],\n",
    "            features['exclamation_count'],\n",
    "            features['question_count'],\n",
    "            features['uppercase_ratio'],\n",
    "            features['positive_word_count'],\n",
    "            features['negative_word_count'],\n",
    "            features['sentiment_word_ratio']\n",
    "        ])\n",
    "    \n",
    "    def batch_extract_features(self, texts):\n",
    "        \"\"\"Ekstrak fitur untuk batch texts\"\"\"\n",
    "        # Prepare BERT inputs\n",
    "        bert_inputs = self.prepare_bert_inputs(texts)\n",
    "        \n",
    "        # Build dependency graphs\n",
    "        adj_matrices = []\n",
    "        for text in texts:\n",
    "            adj_matrices.append(self.build_dependency_graph(text))\n",
    "        adj_matrices = torch.stack(adj_matrices)\n",
    "        \n",
    "        # Extract linguistic features\n",
    "        linguistic_features = []\n",
    "        for text in texts:\n",
    "            linguistic_features.append(self.extract_linguistic_features(text))\n",
    "        linguistic_features = torch.stack(linguistic_features)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': bert_inputs['input_ids'],\n",
    "            'attention_mask': bert_inputs['attention_mask'],\n",
    "            'adj_matrices': adj_matrices,\n",
    "            'linguistic_features': linguistic_features\n",
    "        }\n",
    "\n",
    "# Dataset yang menggunakan FeatureExtractor\n",
    "class AmazonReviewDataset(Dataset):\n",
    "    def __init__(self, reviews, labels, feature_extractor, precompute=False):\n",
    "        self.reviews = reviews\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.precomputed_features = None\n",
    "        \n",
    "        if precompute:\n",
    "            print(\"Precomputing features...\")\n",
    "            self.precomputed_features = self.feature_extractor.batch_extract_features(reviews)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.reviews)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.precomputed_features:\n",
    "            features = {\n",
    "                'input_ids': self.precomputed_features['input_ids'][idx],\n",
    "                'attention_mask': self.precomputed_features['attention_mask'][idx],\n",
    "                'adj_matrix': self.precomputed_features['adj_matrices'][idx],\n",
    "                'linguistic_features': self.precomputed_features['linguistic_features'][idx],\n",
    "                'label': torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            }\n",
    "        else:\n",
    "            text = self.reviews[idx]\n",
    "            label = self.labels[idx]\n",
    "            \n",
    "            # Extract features for single text\n",
    "            bert_inputs = self.feature_extractor.prepare_bert_inputs([text], batch=False)\n",
    "            adj_matrix = self.feature_extractor.build_dependency_graph(text)\n",
    "            linguistic_features = self.feature_extractor.extract_linguistic_features(text)\n",
    "            \n",
    "            features = {\n",
    "                'input_ids': bert_inputs['input_ids'].squeeze(0),\n",
    "                'attention_mask': bert_inputs['attention_mask'].squeeze(0),\n",
    "                'adj_matrix': adj_matrix,\n",
    "                'linguistic_features': linguistic_features,\n",
    "                'label': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        \n",
    "        return features\n",
    "\n",
    "# Fungsi training dan evaluasi\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        adj_matrix = batch['adj_matrix'].to(device)\n",
    "        linguistic_features = batch['linguistic_features'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask, adj_matrix, linguistic_features)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass dan update\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return epoch_loss / len(dataloader), accuracy\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            adj_matrix = batch['adj_matrix'].to(device)\n",
    "            linguistic_features = batch['linguistic_features'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask, adj_matrix, linguistic_features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Metrics\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    return epoch_loss / len(dataloader), accuracy\n",
    "\n",
    "# Fungsi main untuk training\n",
    "def train_model(train_reviews, train_labels, val_reviews, val_labels, \n",
    "                epochs=5, batch_size=16, learning_rate=2e-5, cache_dir='./feature_cache'):\n",
    "    \n",
    "    # Inisialisasi feature extractor\n",
    "    feature_extractor = FeatureExtractor(cache_dir=cache_dir)\n",
    "    \n",
    "    # Buat dataset\n",
    "    train_dataset = AmazonReviewDataset(train_reviews, train_labels, feature_extractor)\n",
    "    val_dataset = AmazonReviewDataset(val_reviews, val_labels, feature_extractor)\n",
    "    \n",
    "    # Buat dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Inisialisasi model\n",
    "    # device = torch_directml.device()\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = BERTBiLSTMGCNModel().to(device)\n",
    "    \n",
    "    # Optimizer dan loss\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        train_loss, train_acc = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_loss, val_acc = evaluate(model, val_dataloader, criterion, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(\"Saved best model!\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Fungsi untuk batch processing pada dataset besar\n",
    "def process_large_dataset(reviews, labels, batch_size=10000, cache_dir='./feature_cache'):\n",
    "    \"\"\"\n",
    "    Process dataset besar dengan batch untuk menghindari bottleneck memori\n",
    "    \"\"\"\n",
    "    feature_extractor = FeatureExtractor(cache_dir=cache_dir)\n",
    "    \n",
    "    for i in range(0, len(reviews), batch_size):\n",
    "        end_idx = min(i + batch_size, len(reviews))\n",
    "        batch_reviews = reviews[i:end_idx]\n",
    "        batch_labels = labels[i:end_idx]\n",
    "        \n",
    "        print(f\"Processing batch {i//batch_size + 1}/{len(reviews)//batch_size + 1}\")\n",
    "        \n",
    "        # Extract and cache features\n",
    "        batch_features = feature_extractor.batch_extract_features(batch_reviews)\n",
    "        \n",
    "        # Save features\n",
    "        torch.save({\n",
    "            'input_ids': batch_features['input_ids'],\n",
    "            'attention_mask': batch_features['attention_mask'],\n",
    "            'adj_matrices': batch_features['adj_matrices'],\n",
    "            'linguistic_features': batch_features['linguistic_features'],\n",
    "            'labels': torch.tensor(batch_labels, dtype=torch.long)\n",
    "        }, f\"{cache_dir}/batch_{i//batch_size}.pt\")\n",
    "        \n",
    "    print(\"Preprocessing complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\harry\\anaconda3\\envs\\torch-nlp\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m y_val \u001b[38;5;241m=\u001b[39m df_val[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Untuk dataset besar\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# process_large_dataset(all_reviews, all_labels)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 399\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(train_reviews, train_labels, val_reviews, val_labels, epochs, batch_size, learning_rate, cache_dir)\u001b[0m\n\u001b[0;32m    395\u001b[0m best_val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[0;32m    402\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_dataloader, criterion, device)\n",
      "Cell \u001b[1;32mIn[3], line 332\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# Backward pass dan update\u001b[39;00m\n\u001b[0;32m    331\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 332\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    334\u001b[0m \u001b[38;5;66;03m# Metrics\u001b[39;00m\n\u001b[0;32m    335\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\harry\\anaconda3\\envs\\torch-nlp\\lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\harry\\anaconda3\\envs\\torch-nlp\\lib\\site-packages\\torch\\optim\\optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\harry\\anaconda3\\envs\\torch-nlp\\lib\\site-packages\\torch\\optim\\adamw.py:188\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    175\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    177\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    178\u001b[0m         group,\n\u001b[0;32m    179\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    185\u001b[0m         state_steps,\n\u001b[0;32m    186\u001b[0m     )\n\u001b[1;32m--> 188\u001b[0m     \u001b[43madamw\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    189\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    190\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    209\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\harry\\anaconda3\\envs\\torch-nlp\\lib\\site-packages\\torch\\optim\\adamw.py:340\u001b[0m, in \u001b[0;36madamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adamw\n\u001b[1;32m--> 340\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\harry\\anaconda3\\envs\\torch-nlp\\lib\\site-packages\\torch\\optim\\adamw.py:416\u001b[0m, in \u001b[0;36m_single_tensor_adamw\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, has_complex)\u001b[0m\n\u001b[0;32m    413\u001b[0m step_t \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    415\u001b[0m \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[1;32m--> 416\u001b[0m \u001b[43mparam\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    418\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    419\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train= final_df['text']\n",
    "y_train = final_df['label']  # 1 = positive, 0 = negative\n",
    "\n",
    "x_val = df_val['text']\n",
    "y_val = df_val['label']\n",
    "\n",
    "# Train model\n",
    "model = train_model(x_train, y_train, x_val, y_val, epochs=5)\n",
    "\n",
    "# Untuk dataset besar\n",
    "# process_large_dataset(all_reviews, all_labels)\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT-LSTM-CCN (Ensamble Method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 1340369,
     "sourceId": 2233682,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "torch-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
