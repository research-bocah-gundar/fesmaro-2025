{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fesmaro 2025**\n# **Analisis Big Data: Tahap Modelling**","metadata":{}},{"cell_type":"code","source":"print(\"this is just a place holder\\nCreate by Research Bocah Gundar\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:36:29.726129Z","iopub.execute_input":"2025-03-28T10:36:29.726366Z","iopub.status.idle":"2025-03-28T10:36:29.730682Z","shell.execute_reply.started":"2025-03-28T10:36:29.726346Z","shell.execute_reply":"2025-03-28T10:36:29.729954Z"}},"outputs":[{"name":"stdout","text":"this is just a place holder\nCreate by Research Bocah Gundar\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Model and Function Declaration","metadata":{}},{"cell_type":"markdown","source":"## Install Library","metadata":{}},{"cell_type":"code","source":"# Install libraries\n!pip install safetensors spacy torchviz wandb --quiet\n\n# Download the required SpaCy model\n!python -m spacy download en_core_web_sm","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:36:00.153215Z","iopub.execute_input":"2025-03-28T10:36:00.153526Z","iopub.status.idle":"2025-03-28T10:36:11.051588Z","shell.execute_reply.started":"2025-03-28T10:36:00.153500Z","shell.execute_reply":"2025-03-28T10:36:11.050768Z"}},"outputs":[{"name":"stdout","text":"Collecting en-core-web-sm==3.7.1\n  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\nRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.5)\nRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\nRequirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\nRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.11)\nRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\nRequirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.5)\nRequirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.3)\nRequirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.5.0)\nRequirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\nRequirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.4.1)\nRequirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.15.1)\nRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.67.1)\nRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.32.3)\nRequirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.0a2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (75.1.0)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.2)\nRequirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.5.0)\nRequirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.4)\nRequirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.1)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.29.0)\nRequirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.12.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2025.1.31)\nRequirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\nRequirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.5)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (13.9.4)\nRequirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.20.0)\nRequirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (7.0.5)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.2)\nRequirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.19.1)\nRequirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.19.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.2)\n\u001b[38;5;2m✔ Download and installation successful\u001b[0m\nYou can now load the package via spacy.load('en_core_web_sm')\n\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\nIf you are in a Jupyter or Colab notebook, you may need to restart Python in\norder to load all the package's dependencies. You can do this by selecting the\n'Restart kernel' or 'Restart runtime' option.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import gdown \nurl = \"https://drive.google.com/drive/u/4/folders/1La029vITSdOyDC-TaKZ9kxVk0twOneFv\"\ngdown.download_folder(url)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:36:38.413445Z","iopub.execute_input":"2025-03-28T10:36:38.413713Z","iopub.status.idle":"2025-03-28T10:37:07.332835Z","shell.execute_reply.started":"2025-03-28T10:36:38.413692Z","shell.execute_reply":"2025-03-28T10:37:07.332012Z"}},"outputs":[{"name":"stderr","text":"Retrieving folder contents\n","output_type":"stream"},{"name":"stdout","text":"Processing file 14XpIdfj6FN9ckvBVrRp9lUa-OtuDIu5O df_processed.csv\nProcessing file 1ZwRcWT9RubqpZ_qFukgq9ZyO3Ob0I0Q9 df_test.csv\nProcessing file 1Btr1jmjO2fx8Uh6_4-It79Mf5pLdXQcy df_train.csv\nProcessing file 1giX5L2NXjxDXepK3wsaRNJjUcSzZf_9P df_val.csv\nProcessing file 1YioeFhGr0yUFVX3dFcIwzFsPFfd11LKj final_df.csv\n","output_type":"stream"},{"name":"stderr","text":"Retrieving folder contents completed\nBuilding directory structure\nBuilding directory structure completed\nDownloading...\nFrom (original): https://drive.google.com/uc?id=14XpIdfj6FN9ckvBVrRp9lUa-OtuDIu5O\nFrom (redirected): https://drive.google.com/uc?id=14XpIdfj6FN9ckvBVrRp9lUa-OtuDIu5O&confirm=t&uuid=ec8cb96a-2798-4404-b6a1-85760b87241e\nTo: /kaggle/working/data/df_processed.csv\n100%|██████████| 128M/128M [00:00<00:00, 227MB/s] \nDownloading...\nFrom: https://drive.google.com/uc?id=1ZwRcWT9RubqpZ_qFukgq9ZyO3Ob0I0Q9\nTo: /kaggle/working/data/df_test.csv\n100%|██████████| 12.1M/12.1M [00:00<00:00, 94.0MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1Btr1jmjO2fx8Uh6_4-It79Mf5pLdXQcy\nTo: /kaggle/working/data/df_train.csv\n100%|██████████| 40.8M/40.8M [00:00<00:00, 67.9MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1giX5L2NXjxDXepK3wsaRNJjUcSzZf_9P\nTo: /kaggle/working/data/df_val.csv\n100%|██████████| 8.06M/8.06M [00:00<00:00, 42.5MB/s]\nDownloading...\nFrom: https://drive.google.com/uc?id=1YioeFhGr0yUFVX3dFcIwzFsPFfd11LKj\nTo: /kaggle/working/data/final_df.csv\n100%|██████████| 29.3M/29.3M [00:00<00:00, 52.5MB/s]\nDownload completed\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"['/kaggle/working/data/df_processed.csv',\n '/kaggle/working/data/df_test.csv',\n '/kaggle/working/data/df_train.csv',\n '/kaggle/working/data/df_val.csv',\n '/kaggle/working/data/final_df.csv']"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Import Library","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertModel, BertTokenizer, AutoConfig\nfrom sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport spacy\nimport os\nimport pickle\nimport hashlib\nimport json\nfrom tqdm.notebook import tqdm  # Use notebook-friendly tqdm\nfrom safetensors.torch import save_file, load_file\nimport wandb\nimport re\nfrom collections import Counter\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Untuk visualisasi model (opsional, install jika diperlukan)\ntry:\n    import torchviz\nexcept ImportError:\n    print(\"torchviz not found. Install it via 'pip install torchviz' for model visualization.\")\n    torchviz = None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:37:12.600968Z","iopub.execute_input":"2025-03-28T10:37:12.601417Z","iopub.status.idle":"2025-03-28T10:37:37.700459Z","shell.execute_reply.started":"2025-03-28T10:37:12.601392Z","shell.execute_reply":"2025-03-28T10:37:37.699579Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Konfigurasi Eksperimen","metadata":{}},{"cell_type":"markdown","source":"**PLEASE READ ME!**\n\n**SILAHKAN PILIH SALAH SATU CONFIGURASI MODEL**","metadata":{}},{"cell_type":"code","source":"# --- 1. Konfigurasi Dual GPU ---\nnum_gpus = torch.cuda.device_count()\nprint(f\"Found {num_gpus} GPUs.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:37:55.081248Z","iopub.execute_input":"2025-03-28T10:37:55.081551Z","iopub.status.idle":"2025-03-28T10:37:55.086392Z","shell.execute_reply.started":"2025-03-28T10:37:55.081527Z","shell.execute_reply":"2025-03-28T10:37:55.085306Z"}},"outputs":[{"name":"stdout","text":"Found 2 GPUs.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n\ndef simple_tokenizer(text):\n    return tokenizer.tokenize(text)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:37:55.333103Z","iopub.execute_input":"2025-03-28T10:37:55.333370Z","iopub.status.idle":"2025-03-28T10:37:56.296679Z","shell.execute_reply.started":"2025-03-28T10:37:55.333343Z","shell.execute_reply":"2025-03-28T10:37:56.296055Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e9c9bcce0d844fa966bb21e0be11383"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bb0a975c24443dc95d795484cef570c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b25bcfa9f9d942228a879a0b0a8c1330"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"573ecbffef384a6396a09cce7e0217ac"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"### CNN","metadata":{}},{"cell_type":"code","source":"config = {\n    # Pilihan Model: 'bert', 'lstm', 'cnn', 'gcn', 'bert_lstm_gcn', 'bert_lstm_cnn'\n    \"model_type\": \"cnn\",\n    \"bert_model_name\": \"bert-base-uncased\",\n    \"max_length\": 128,\n    \"embedding_dim\": 300, # Untuk model non-BERT (LSTM, CNN, GCN)\n    \"vocab_size\": 10000,  # Perkiraan ukuran vocab untuk model non-BERT\n    \"hidden_dim\": 128,\n    \"lstm_layers\": 1,\n    \"gcn_layers\": 2,\n    \"cnn_filters\": 256,\n    \"cnn_kernel_sizes\": [3, 4, 5],\n    \"dropout\": 0.3,\n    \"num_classes\": 2, # Akan diupdate setelah LabelEncoder\n    \"epochs\": 3, # Kurangi untuk testing cepat\n    # ====> IMPORTANT: Increase batch size for multi-GPU training <====\n    # Original: 16. Try doubling it for 2 GPUs, adjust based on memoxry.\n    \"batch_size\": 128, # Total batch size across all GPUs\n    \"learning_rate\": 2e-5, # LR umum untuk fine-tuning BERT\n    \"non_bert_lr\": 1e-3, # LR umum untuk model non-BERT\n    \"cache_dir\": \"./feature_cache\",\n    \"log_file\": \"training_log.json\",\n    \"model_save_path\": \"best_model.safetensors\",\n    # Use 'cuda' which defaults to cuda:0 (primary GPU for DataParallel)\n    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n    \"use_linguistic_features\": True, # Set False jika tidak ingin menggunakan fitur linguistik\n    \"linguistic_feat_dim\": 9,\n    \"freeze_bert\": False # Set True untuk freeze bobot BERT (lebih cepat, mungkin akurasi turun)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:04:04.892776Z","iopub.execute_input":"2025-03-27T11:04:04.893039Z","iopub.status.idle":"2025-03-27T11:04:04.897599Z","shell.execute_reply.started":"2025-03-27T11:04:04.893018Z","shell.execute_reply":"2025-03-27T11:04:04.896787Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### LSTM","metadata":{}},{"cell_type":"code","source":"config = {\n    # Pilihan Model: 'bert', 'lstm', 'cnn', 'gcn', 'bert_lstm_gcn', 'bert_lstm_cnn'\n    \"model_type\": \"lstm\",\n    \"bert_model_name\": \"bert-base-uncased\",\n    \"max_length\": 128,\n    \"embedding_dim\": 300, # Untuk model non-BERT (LSTM, CNN, GCN)\n    \"vocab_size\": 10000,  # Perkiraan ukuran vocab untuk model non-BERT\n    \"hidden_dim\": 128,\n    \"lstm_layers\": 1,\n    \"gcn_layers\": 2,\n    \"cnn_filters\": 256,\n    \"cnn_kernel_sizes\": [3, 4, 5],\n    \"dropout\": 0.3,\n    \"num_classes\": 2, # Akan diupdate setelah LabelEncoder\n    \"epochs\": 3, # Kurangi untuk testing cepat\n    # ====> IMPORTANT: Increase batch size for multi-GPU training <====\n    # Original: 16. Try doubling it for 2 GPUs, adjust based on memoxry.\n    \"batch_size\": 128, # Total batch size across all GPUs\n    \"learning_rate\": 2e-5, # LR umum untuk fine-tuning BERT\n    \"non_bert_lr\": 1e-3, # LR umum untuk model non-BERT\n    \"cache_dir\": \"./feature_cache\",\n    \"log_file\": \"training_log.json\",\n    \"model_save_path\": \"best_model.safetensors\",\n    # Use 'cuda' which defaults to cuda:0 (primary GPU for DataParallel)\n    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n    \"use_linguistic_features\": True, # Set False jika tidak ingin menggunakan fitur linguistik\n    \"linguistic_feat_dim\": 9,\n    \"freeze_bert\": False # Set True untuk freeze bobot BERT (lebih cepat, mungkin akurasi turun)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T11:04:04.898409Z","iopub.execute_input":"2025-03-27T11:04:04.898639Z","iopub.status.idle":"2025-03-27T11:04:04.912411Z","shell.execute_reply.started":"2025-03-27T11:04:04.898619Z","shell.execute_reply":"2025-03-27T11:04:04.911779Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"### GCN","metadata":{}},{"cell_type":"code","source":"config = {\n    # Pilihan Model: 'bert', 'lstm', 'cnn', 'gcn', 'bert_lstm_gcn', 'bert_lstm_cnn'\n    \"model_type\": \"gcn\",\n    \"bert_model_name\": \"bert-base-uncased\",\n    \"max_length\": 128,\n    \"embedding_dim\": 300, # Untuk model non-BERT (LSTM, CNN, GCN)\n    \"vocab_size\": 10000,  # Perkiraan ukuran vocab untuk model non-BERT\n    \"hidden_dim\": 128,\n    \"lstm_layers\": 1,\n    \"gcn_layers\": 2,\n    \"cnn_filters\": 256,\n    \"cnn_kernel_sizes\": [3, 4, 5],\n    \"dropout\": 0.3,\n    \"num_classes\": 2, # Akan diupdate setelah LabelEncoder\n    \"epochs\": 3, # Kurangi untuk testing cepat\n    # ====> IMPORTANT: Increase batch size for multi-GPU training <====\n    # Original: 16. Try doubling it for 2 GPUs, adjust based on memoxry.\n    \"batch_size\": 128, # Total batch size across all GPUs\n    \"learning_rate\": 2e-5, # LR umum untuk fine-tuning BERT\n    \"non_bert_lr\": 1e-3, # LR umum untuk model non-BERT\n    \"cache_dir\": \"./feature_cache\",\n    \"log_file\": \"training_log.json\",\n    \"model_save_path\": \"best_model.safetensors\",\n    # Use 'cuda' which defaults to cuda:0 (primary GPU for DataParallel)\n    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n    \"use_linguistic_features\": True, # Set False jika tidak ingin menggunakan fitur linguistik\n    \"linguistic_feat_dim\": 9,\n    \"freeze_bert\": False # Set True untuk freeze bobot BERT (lebih cepat, mungkin akurasi turun)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T00:54:08.270975Z","iopub.execute_input":"2025-03-28T00:54:08.271242Z","iopub.status.idle":"2025-03-28T00:54:08.276081Z","shell.execute_reply.started":"2025-03-28T00:54:08.271221Z","shell.execute_reply":"2025-03-28T00:54:08.275190Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### BERT","metadata":{}},{"cell_type":"code","source":"config = {\n    # Pilihan Model: 'bert', 'lstm', 'cnn', 'gcn', 'bert_lstm_gcn', 'bert_lstm_cnn'\n    \"model_type\": \"bert\",\n    \"bert_model_name\": \"bert-base-uncased\",\n    \"max_length\": 128,\n    \"embedding_dim\": 300, # Untuk model non-BERT (LSTM, CNN, GCN)\n    \"vocab_size\": 10000,  # Perkiraan ukuran vocab untuk model non-BERT\n    \"hidden_dim\": 128,\n    \"lstm_layers\": 1,\n    \"gcn_layers\": 2,\n    \"cnn_filters\": 256,\n    \"cnn_kernel_sizes\": [3, 4, 5],\n    \"dropout\": 0.3,\n    \"num_classes\": 2, # Akan diupdate setelah LabelEncoder\n    \"epochs\": 3, # Kurangi untuk testing cepat\n    # ====> IMPORTANT: Increase batch size for multi-GPU training <====\n    # Original: 16. Try doubling it for 2 GPUs, adjust based on memoxry.\n    \"batch_size\": 128, # Total batch size across all GPUs\n    \"learning_rate\": 2e-5, # LR umum untuk fine-tuning BERT\n    \"non_bert_lr\": 1e-3, # LR umum untuk model non-BERT\n    \"cache_dir\": \"./feature_cache\",\n    \"log_file\": \"training_log.json\",\n    \"model_save_path\": \"best_model.safetensors\",\n    # Use 'cuda' which defaults to cuda:0 (primary GPU for DataParallel)\n    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n    \"use_linguistic_features\": True, # Set False jika tidak ingin menggunakan fitur linguistik\n    \"linguistic_feat_dim\": 9,\n    \"freeze_bert\": False # Set True untuk freeze bobot BERT (lebih cepat, mungkin akurasi turun)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T09:30:09.833785Z","iopub.execute_input":"2025-03-27T09:30:09.834055Z","iopub.status.idle":"2025-03-27T09:30:09.838436Z","shell.execute_reply.started":"2025-03-27T09:30:09.834032Z","shell.execute_reply":"2025-03-27T09:30:09.837699Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"### BERT-LSTM-CNN","metadata":{}},{"cell_type":"code","source":"config = {\n    # Pilihan Model: 'bert', 'lstm', 'cnn', 'gcn', 'bert_lstm_gcn', 'bert_lstm_cnn'\n    \"model_type\": \"bert_lstm_cnn\",\n    \"bert_model_name\": \"bert-base-uncased\",\n    \"max_length\": 128,\n    \"embedding_dim\": 300, # Untuk model non-BERT (LSTM, CNN, GCN)\n    \"vocab_size\": 10000,  # Perkiraan ukuran vocab untuk model non-BERT\n    \"hidden_dim\": 128,\n    \"lstm_layers\": 1,\n    \"gcn_layers\": 2,\n    \"cnn_filters\": 256,\n    \"cnn_kernel_sizes\": [3, 4, 5],\n    \"dropout\": 0.3,\n    \"num_classes\": 2, # Akan diupdate setelah LabelEncoder\n    \"epochs\": 3, # Kurangi untuk testing cepat\n    # ====> IMPORTANT: Increase batch size for multi-GPU training <====\n    # Original: 16. Try doubling it for 2 GPUs, adjust based on memoxry.\n    \"batch_size\": 128, # Total batch size across all GPUs\n    \"learning_rate\": 2e-5, # LR umum untuk fine-tuning BERT\n    \"non_bert_lr\": 1e-3, # LR umum untuk model non-BERT\n    \"cache_dir\": \"./feature_cache\",\n    \"log_file\": \"training_log.json\",\n    \"model_save_path\": \"best_model.safetensors\",\n    # Use 'cuda' which defaults to cuda:0 (primary GPU for DataParallel)\n    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n    \"use_linguistic_features\": True, # Set False jika tidak ingin menggunakan fitur linguistik\n    \"linguistic_feat_dim\": 9,\n    \"freeze_bert\": False # Set True untuk freeze bobot BERT (lebih cepat, mungkin akurasi turun)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:38:01.441990Z","iopub.execute_input":"2025-03-28T10:38:01.442308Z","iopub.status.idle":"2025-03-28T10:38:01.447450Z","shell.execute_reply.started":"2025-03-28T10:38:01.442284Z","shell.execute_reply":"2025-03-28T10:38:01.446602Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### BERT-LSTM-GCN","metadata":{}},{"cell_type":"code","source":"config = {\n    # Pilihan Model: 'bert', 'lstm', 'cnn', 'gcn', 'bert_lstm_gcn', 'bert_lstm_cnn'\n    \"model_type\": \"bert_lstm_gcn\",\n    \"bert_model_name\": \"bert-base-uncased\",\n    \"max_length\": 128,\n    \"embedding_dim\": 300, # Untuk model non-BERT (LSTM, CNN, GCN)\n    \"vocab_size\": 10000,  # Perkiraan ukuran vocab untuk model non-BERT\n    \"hidden_dim\": 128,\n    \"lstm_layers\": 1,\n    \"gcn_layers\": 2,\n    \"cnn_filters\": 256,\n    \"cnn_kernel_sizes\": [3, 4, 5],\n    \"dropout\": 0.3,\n    \"num_classes\": 2, # Akan diupdate setelah LabelEncoder\n    \"epochs\": 3, # Kurangi untuk testing cepat\n    # ====> IMPORTANT: Increase batch size for multi-GPU training <====\n    # Original: 16. Try doubling it for 2 GPUs, adjust based on memoxry.\n    \"batch_size\": 128, # Total batch size across all GPUs\n    \"learning_rate\": 2e-5, # LR umum untuk fine-tuning BERT\n    \"non_bert_lr\": 1e-3, # LR umum untuk model non-BERT\n    \"cache_dir\": \"./feature_cache\",\n    \"log_file\": \"training_log.json\",\n    \"model_save_path\": \"best_model.safetensors\",\n    # Use 'cuda' which defaults to cuda:0 (primary GPU for DataParallel)\n    \"device\": 'cuda' if torch.cuda.is_available() else 'cpu',\n    \"use_linguistic_features\": True, # Set False jika tidak ingin menggunakan fitur linguistik\n    \"linguistic_feat_dim\": 9,\n    \"freeze_bert\": False # Set True untuk freeze bobot BERT (lebih cepat, mungkin akurasi turun)\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T04:27:04.377577Z","iopub.execute_input":"2025-03-27T04:27:04.377898Z","iopub.status.idle":"2025-03-27T04:27:04.383414Z","shell.execute_reply.started":"2025-03-27T04:27:04.377875Z","shell.execute_reply":"2025-03-27T04:27:04.382412Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Data Loading","metadata":{}},{"cell_type":"code","source":"# Check device and cache\nprint(f\"Using primary device: {config['device']}\")\nif config['device'] == 'cuda' and num_gpus > 1:\n    print(f\"Activating DataParallel for {num_gpus} GPUs.\")\n    # Ensure batch size is divisible by num_gpus for even splits, though DataParallel handles uneven splits too.\n    if config['batch_size'] % num_gpus != 0:\n        print(f\"Warning: Batch size {config['batch_size']} is not perfectly divisible by {num_gpus} GPUs.\")\n\n\nif not os.path.exists(config['cache_dir']):\n    os.makedirs(config['cache_dir'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:38:05.045205Z","iopub.execute_input":"2025-03-28T10:38:05.045501Z","iopub.status.idle":"2025-03-28T10:38:05.051452Z","shell.execute_reply.started":"2025-03-28T10:38:05.045479Z","shell.execute_reply":"2025-03-28T10:38:05.050732Z"}},"outputs":[{"name":"stdout","text":"Using primary device: cuda\nActivating DataParallel for 2 GPUs.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# --- 2. Data Loading ---\nprint(\"Loading data...\")\ntry:\n    final_df = pd.read_csv('./data/final_df.csv')\n    df_val = pd.read_csv('./data/df_val.csv')\n    df_test = pd.read_csv('./data/df_test.csv')\n\n    # Rename & Select Columns (asumsi kolom sudah benar setelah EDA)\n    final_df.rename(columns={'lemmatized_text': 'text'}, inplace=True)\n    df_val.rename(columns={'lemmatized_text': 'text'}, inplace=True)\n    df_test.rename(columns={'lemmatized_text': 'text'}, inplace=True)\n\n    # Mengubah value label menjadi 'negative' dan 'positive' sebelum encode label\n    final_df['label'] = np.where(final_df['label'] == 0, 'negative', 'positive')\n    df_val['label'] = np.where(df_val['label'] == 0, 'negative', 'positive')\n    df_test['label'] = np.where(df_test['label'] == 0, 'negative', 'positive')\n\n    # Encode Labels (Penting!)\n    label_encoder = LabelEncoder()\n    final_df['label_encoded'] = label_encoder.fit_transform(final_df['label'])\n    df_val['label_encoded'] = label_encoder.transform(df_val['label'])\n    df_test['label_encoded'] = label_encoder.transform(df_test['label'])\n    config['num_classes'] = len(label_encoder.classes_) # Update num_classes\n    print(f\"Classes: {label_encoder.classes_} -> {label_encoder.transform(label_encoder.classes_)}\")\n    print(f\"Number of classes: {config['num_classes']}\")\n\nexcept FileNotFoundError as e:\n    print(f\"Error loading data: {e}. Make sure CSV files are in the './data/' directory.\")\n    exit()\nexcept KeyError as e:\n    print(f\"Error: Column {e} not found. Check CSV column names.\")\n    exit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:38:16.340523Z","iopub.execute_input":"2025-03-28T10:38:16.340813Z","iopub.status.idle":"2025-03-28T10:38:17.033004Z","shell.execute_reply.started":"2025-03-28T10:38:16.340789Z","shell.execute_reply":"2025-03-28T10:38:17.032251Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nClasses: ['negative' 'positive'] -> [0 1]\nNumber of classes: 2\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"## Feature Extraction & Preprocessing","metadata":{}},{"cell_type":"code","source":"# --- 3. Feature Extraction & Preprocessing ---\n\n# Load SpaCy model (hanya jika diperlukan GCN atau linguistic features)\nif config['model_type'] in ['gcn', 'bert_lstm_gcn'] or config['use_linguistic_features']:\n    try:\n        nlp = spacy.load(\"en_core_web_sm\")\n    except OSError:\n        print(\"Downloading spacy en_core_web_sm model...\")\n        spacy.cli.download(\"en_core_web_sm\")\n        nlp = spacy.load(\"en_core_web_sm\")\nelse:\n    nlp = None\n\n# Inisialisasi Tokenizer BERT\ntokenizer = BertTokenizer.from_pretrained(config['bert_model_name'])\n\ndef simple_tokenizer(text):\n    \"\"\"Tokenizer sederhana: lowercase dan split berdasarkan non-alphanumeric.\"\"\"\n    text = text.lower()\n    # Simpan tanda baca dasar sebagai token terpisah, split yang lain\n    text = re.sub(r\"([.!?,'()])\", r\" \\1 \", text)\n    tokens = text.split()\n    return tokens\n\ndef build_vocab(texts, max_vocab_size, min_freq=2):\n    \"\"\"Membangun vocabulary dari list teks.\"\"\"\n    print(\"Building vocabulary...\")\n    word_counts = Counter()\n    for text in tqdm(texts, desc=\"Counting words\"):\n        tokens = simple_tokenizer(str(text)) # Pastikan string\n        word_counts.update(tokens)\n\n    # Filter berdasarkan frekuensi minimum\n    filtered_counts = {word: count for word, count in word_counts.items() if count >= min_freq}\n\n    # Urutkan berdasarkan frekuensi (descending)\n    sorted_words = sorted(filtered_counts.keys(), key=lambda w: word_counts[w], reverse=True)\n\n    # Buat vocab: <PAD>=0, <UNK>=1, lalu kata-kata lainnya\n    vocab = {'<PAD>': 0, '<UNK>': 1}\n    idx = 2\n    for word in sorted_words:\n        if idx >= max_vocab_size:\n            break\n        if word not in vocab: # Seharusnya tidak terjadi dengan sort unik, tapi aman\n            vocab[word] = idx\n            idx += 1\n\n    actual_vocab_size = len(vocab)\n    print(f\"Vocabulary built: Size = {actual_vocab_size} (Requested max: {max_vocab_size}, Min freq: {min_freq})\")\n    print(f\"Example vocab items: {list(vocab.items())[:10]} ... {list(vocab.items())[-5:]}\")\n    return vocab, actual_vocab_size\n\ndef pad_sequence(ids, max_len, pad_id):\n    \"\"\"Melakukan padding atau truncation pada sequence ID.\"\"\"\n    if len(ids) < max_len:\n        return ids + [pad_id] * (max_len - len(ids))\n    else:\n        return ids[:max_len]\n    \n# final_df, df_val, df_test sudah di-load dan di-preprocess labelnya\n# --- Tambahan: Bangun Vocabulary (HANYA jika model non-BERT mungkin digunakan) ---\nnon_bert_models = ['lstm', 'cnn', 'gcn']\nvocab = None\npad_id = 0 # Biasanya 0 untuk nn.Embedding padding_idx\nunk_id = 1 # Biasanya 1\nif any(m == config['model_type'] for m in non_bert_models):\n    # Gabungkan semua teks untuk membangun vocab (atau cukup training set)\n    all_train_texts = final_df['text'].tolist()\n    # Anda bisa juga menambahkan teks validasi/test jika ingin vocab lebih lengkap,\n    # tapi umumnya cukup dari training set.\n    # all_texts = final_df['text'].tolist() + df_val['text'].tolist() + df_test['text'].tolist()\n    # Bangun vocab dari data training\n    vocab, actual_vocab_size = build_vocab(\n        all_train_texts,\n        config['vocab_size'], # Gunakan config['vocab_size'] sebagai batas MAKSIMUM\n        min_freq=2 # Atur frekuensi minimum sesuai kebutuhan\n    )\n    # Update config dengan ukuran vocab aktual yang dihasilkan\n    config['vocab_size'] = actual_vocab_size\n    print(f\"Updated config['vocab_size'] to actual size: {config['vocab_size']}\")\nelse:\n    print(\"Skipping vocabulary building as a non-BERT model is not selected.\")\n    # Set vocab_size ke nilai default BERT jika hanya BERT yg dipakai (meski tidak relevan)\n    # config['vocab_size'] = tokenizer.vocab_size # Atau biarkan saja dari config awal\n\nclass FeatureExtractor:\n    def __init__(self, config, tokenizer, nlp=None):\n        self.config = config\n        self.tokenizer = tokenizer\n        self.nlp = nlp\n        self.cache_dir = config['cache_dir']\n\n    def _get_cache_path(self, text, prefix):\n        # Limit text length for hashing to avoid issues with very long texts\n        text_snippet = text[:2000]\n        text_hash = hashlib.md5(text_snippet.encode()).hexdigest()\n        return os.path.join(self.cache_dir, f\"{prefix}_{text_hash}.pkl\")\n\n    def get_bert_inputs(self, text):\n        return self.tokenizer(\n            text,\n            padding='max_length',\n            truncation=True,\n            max_length=self.config['max_length'],\n            return_tensors='pt'\n        )\n\n    def get_dependency_graph(self, text, use_cache=True):\n        if not self.nlp:\n            adj = torch.eye(self.config['max_length'])\n            return adj\n\n        # Ensure text is not excessively long for caching key generation\n        cache_key_text = str(text)[:2000] # Use a snippet for the hash key\n        cache_file = self._get_cache_path(cache_key_text, \"dep_graph\")\n\n        if use_cache and os.path.exists(cache_file):\n            try:\n                with open(cache_file, 'rb') as f:\n                    return pickle.load(f)\n            except Exception as e:\n                print(f\"Warning: Could not load cache file {cache_file}. Recomputing. Error: {e}\")\n\n        # Limit text length for SpaCy processing\n        doc = self.nlp(str(text)[:1500])\n        max_len = self.config['max_length']\n        adj_matrix = np.zeros((max_len, max_len))\n\n        token_map = {}\n        spacy_tokens = [token for token in doc if not token.is_space][:max_len]\n\n        valid_indices = set(range(len(spacy_tokens)))\n\n        for i, token in enumerate(spacy_tokens):\n            # Use indices relative to the spacy_tokens list\n            idx = i\n            head_idx_orig = token.head.i # Original index in doc\n\n            # Find the corresponding index in spacy_tokens for the head\n            head_idx = -1\n            for j, head_token_candidate in enumerate(spacy_tokens):\n                if head_token_candidate.i == head_idx_orig:\n                    head_idx = j\n                    break\n\n            # Add edge if both token and its head are within the first max_len spacy tokens\n            if head_idx != -1 and idx < max_len and head_idx < max_len:\n                 # Ensure we don't go out of bounds for the adj_matrix\n                 adj_matrix[idx, head_idx] = 1\n                 adj_matrix[head_idx, idx] = 1 # Undirected graph\n\n        # Add self-loops for tokens present\n        num_actual_tokens = len(spacy_tokens)\n        adj_matrix[:num_actual_tokens, :num_actual_tokens] += np.eye(num_actual_tokens)\n\n        # Add self-loops for padding positions\n        for i in range(num_actual_tokens, max_len):\n            adj_matrix[i, i] = 1\n\n        # Normalize (Symmetric)\n        rowsum = np.array(adj_matrix.sum(1)).flatten()\n        d_inv_sqrt = np.power(rowsum, -0.5)\n        d_inv_sqrt[np.isinf(d_inv_sqrt) | np.isnan(d_inv_sqrt)] = 0.\n        d_mat_inv_sqrt = np.diag(d_inv_sqrt)\n        normalized_adj = d_mat_inv_sqrt.dot(adj_matrix).dot(d_mat_inv_sqrt)\n\n        result = torch.FloatTensor(normalized_adj)\n\n        if use_cache:\n            try:\n                with open(cache_file, 'wb') as f:\n                    pickle.dump(result, f)\n            except Exception as e:\n                print(f\"Warning: Could not save cache file {cache_file}. Error: {e}\")\n\n        return result\n\n\n    def get_linguistic_features(self, text):\n        if not self.nlp or not self.config['use_linguistic_features']:\n            return torch.zeros(self.config['linguistic_feat_dim'])\n\n        features = {}\n        text_limited = str(text)[:1000]\n        words = text_limited.split()\n        word_count = len(words)\n\n        # Normalize features (mostly between 0 and 1)\n        features['text_length'] = min(len(text_limited), 1000) / 1000.0\n        features['word_count'] = min(word_count, 200) / 200.0\n        features['avg_word_length'] = min(np.mean([len(word) for word in words]) if words else 0, 20) / 20.0\n        features['exclamation_count'] = min(text_limited.count('!'), 10) / 10.0\n        features['question_count'] = min(text_limited.count('?'), 10) / 10.0\n        uppercase_word_count = sum(1 for word in words if word.isupper() and len(word) > 1)\n        features['uppercase_word_ratio'] = uppercase_word_count / (word_count + 1e-6) # Ratio is already scaled\n        features['uppercase_word_count_norm'] = min(uppercase_word_count, 20) / 20.0 # Absolute count normalized\n\n        positive_words = {'good', 'great', 'excellent', 'amazing', 'wonderful', 'best', 'love',\n                        'perfect', 'recommend', 'happy', 'awesome', 'like', 'highly'}\n        negative_words = {'bad', 'poor', 'terrible', 'horrible', 'worst', 'waste', 'disappointed',\n                        'disappointing', 'difficult', 'hate', 'problem', 'issue', 'fail', 'never', 'not'}\n\n        lower_words = text_limited.lower().split()\n        pos_count = sum(1 for word in lower_words if word in positive_words)\n        neg_count = sum(1 for word in lower_words if word in negative_words)\n\n        features['positive_word_count_norm'] = min(pos_count, 20) / 20.0\n        features['negative_word_count_norm'] = min(neg_count, 20) / 20.0\n        # Scaled sentiment score (-1 to 1 approx)\n        features['sentiment_score_scaled'] = (pos_count - neg_count) / (word_count + 1e-6)\n\n\n        # Ensure the correct number of features are returned matching linguistic_feat_dim\n        feature_values = [\n            features.get('text_length', 0),\n            features.get('word_count', 0),\n            features.get('avg_word_length', 0),\n            features.get('exclamation_count', 0),\n            features.get('question_count', 0),\n            features.get('uppercase_word_ratio', 0),\n            features.get('positive_word_count_norm', 0),\n            features.get('negative_word_count_norm', 0),\n            features.get('sentiment_score_scaled', 0),\n            # Add more features or padding zeros if needed up to linguistic_feat_dim\n        ]\n\n        # Pad or truncate to match linguistic_feat_dim\n        final_features = feature_values[:self.config['linguistic_feat_dim']]\n        if len(final_features) < self.config['linguistic_feat_dim']:\n            final_features.extend([0.0] * (self.config['linguistic_feat_dim'] - len(final_features)))\n\n        return torch.FloatTensor(final_features)\n\n\nfeature_extractor = FeatureExtractor(config, tokenizer, nlp)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:38:19.072470Z","iopub.execute_input":"2025-03-28T10:38:19.072766Z","iopub.status.idle":"2025-03-28T10:38:20.298261Z","shell.execute_reply.started":"2025-03-28T10:38:19.072743Z","shell.execute_reply":"2025-03-28T10:38:20.297555Z"}},"outputs":[{"name":"stdout","text":"Skipping vocabulary building as a non-BERT model is not selected.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Dataset & DataLoader","metadata":{}},{"cell_type":"code","source":"# --- 4. Dataset & DataLoader ---\nclass SentimentDataset(Dataset):\n    def __init__(self, texts, labels, feature_extractor, config, vocab=None, pad_id=0, unk_id=1): # Tambahkan vocab, pad_id, unk_id\n        self.texts = texts\n        self.labels = labels\n        self.feature_extractor = feature_extractor\n        self.config = config\n        self.model_type = config['model_type']\n        # Simpan vocab dan ID khusus jika model non-BERT\n        self.vocab = vocab\n        self.pad_id = pad_id\n        self.unk_id = unk_id\n\n        # Validasi: vocab diperlukan untuk model non-BERT\n        if self.model_type in ['lstm', 'cnn', 'gcn'] and self.vocab is None:\n            raise ValueError(f\"Model type '{self.model_type}' requires a vocabulary, but 'vocab' was not provided to SentimentDataset.\")\n\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx]) # Pastikan string\n        label = self.labels[idx]\n\n        item = {'text': text, 'label': torch.tensor(label, dtype=torch.long)}\n\n        # --- Fitur BERT (jika diperlukan) ---\n        if 'bert' in self.model_type:\n            bert_inputs = self.feature_extractor.get_bert_inputs(text)\n            item['input_ids'] = bert_inputs['input_ids'].squeeze(0)\n            item['attention_mask'] = bert_inputs['attention_mask'].squeeze(0)\n\n        # --- Fitur GCN (selalu diekstrak jika GCN ada di model type, menggunakan SpaCy) ---\n        # Catatan: Untuk GCN murni, adj_matrix ini mungkin tidak sempurna selaras\n        # dengan token_ids dari simple_tokenizer. Ini adalah kompromi.\n        if 'gcn' in self.model_type:\n            item['adj_matrix'] = self.feature_extractor.get_dependency_graph(text)\n\n        # --- Fitur Linguistik (jika diperlukan) ---\n        if self.config['use_linguistic_features'] and self.model_type in ['bert_lstm_gcn', 'bert_lstm_cnn']:\n            item['linguistic_features'] = self.feature_extractor.get_linguistic_features(text)\n\n        # --- Fitur untuk model non-BERT (LSTM, CNN, GCN murni) ---\n        if self.model_type in ['lstm', 'cnn', 'gcn']:\n            # 1. Tokenisasi Non-BERT\n            tokens = simple_tokenizer(text)\n\n            # 2. Konversi Token ke ID (menggunakan vocab yang diberikan)\n            token_ids = [self.vocab.get(token, self.unk_id) for token in tokens] # Gunakan unk_id jika OOV\n\n            # 3. Padding / Truncation\n            max_len = self.config['max_length']\n            padded_ids = pad_sequence(token_ids, max_len, self.pad_id)\n\n            # 4. Buat Tensor dan tambahkan ke item\n            item['token_ids'] = torch.tensor(padded_ids, dtype=torch.long)\n\n            # (Tidak perlu lagi placeholder atau warning)\n            # (adj_matrix sudah ditangani di blok 'gcn' di atas)\n        return item\n\n\n# Buat datasets\nprint(\"Creating datasets...\")\ntrain_dataset = SentimentDataset(\n    final_df['text'].tolist(),\n    final_df['label_encoded'].tolist(),\n    feature_extractor,\n    config,\n    vocab=vocab, # Kirim vocab\n    pad_id=pad_id, # Kirim pad_id\n    unk_id=unk_id  # Kirim unk_id\n)\nval_dataset = SentimentDataset(\n    df_val['text'].tolist(),\n    df_val['label_encoded'].tolist(),\n    feature_extractor,\n    config,\n    vocab=vocab, # Kirim vocab\n    pad_id=pad_id, # Kirim pad_id\n    unk_id=unk_id  # Kirim unk_id\n)\ntest_dataset = SentimentDataset(\n    df_test['text'].tolist(),\n    df_test['label_encoded'].tolist(),\n    feature_extractor,\n    config,\n    vocab=vocab, # Kirim vocab\n    pad_id=pad_id, # Kirim pad_id\n    unk_id=unk_id  # Kirim unk_id\n)\n\n# Buat dataloaders\n# num_workers > 0 can cause issues on Kaggle notebooks, keep it 0 unless you are sure.\ntrain_dataloader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=0)\nval_dataloader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=0)\ntest_dataloader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=0)\n\nprint(\"Dataloaders created.\")\nprint(f\"Train batches: {len(train_dataloader)}, Val batches: {len(val_dataloader)}, Test batches: {len(test_dataloader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:38:24.023024Z","iopub.execute_input":"2025-03-28T10:38:24.023364Z","iopub.status.idle":"2025-03-28T10:38:24.043785Z","shell.execute_reply.started":"2025-03-28T10:38:24.023334Z","shell.execute_reply":"2025-03-28T10:38:24.042896Z"}},"outputs":[{"name":"stdout","text":"Creating datasets...\nDataloaders created.\nTrain batches: 782, Val batches: 157, Test batches: 235\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Model Definitions","metadata":{}},{"cell_type":"code","source":"# --- 5. Model Definitions ---\n\n# 5.1 GCN Layer\nclass GraphConvolution(nn.Module):\n    def __init__(self, in_features, out_features, bias=True):\n        super(GraphConvolution, self).__init__()\n        self.in_features = in_features\n        self.out_features = out_features\n        self.weight = nn.Parameter(torch.FloatTensor(in_features, out_features))\n        if bias:\n            self.bias = nn.Parameter(torch.FloatTensor(out_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        torch.nn.init.xavier_uniform_(self.weight)\n        if self.bias is not None:\n            torch.nn.init.zeros_(self.bias)\n\n    def forward(self, input, adj):\n        # input: [batch_size, seq_len, in_features]\n        # adj: [batch_size, seq_len, seq_len]\n        support = torch.matmul(input, self.weight) # [batch_size, seq_len, out_features] - matmul handles batch dimension correctly\n        output = torch.bmm(adj, support) # [batch_size, seq_len, out_features]\n\n        if self.bias is not None:\n             output = output + self.bias # Bias broadcasts correctly over batch and seq_len\n\n        return output\n\n# 5.2 BERT Model\nclass BERTClassifier(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert_model_name = config['bert_model_name']\n        self.bert_config = AutoConfig.from_pretrained(self.bert_model_name)\n        self.bert = BertModel.from_pretrained(self.bert_model_name, config=self.bert_config)\n        self.dropout = nn.Dropout(config['dropout'])\n        self.classifier = nn.Linear(self.bert_config.hidden_size, config['num_classes'])\n\n        if config['freeze_bert']:\n            print(\"Freezing BERT parameters.\")\n            for param in self.bert.parameters():\n                param.requires_grad = False\n\n    def forward(self, input_ids, attention_mask, **kwargs):\n        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        pooled_output = outputs.pooler_output\n        pooled_output = self.dropout(pooled_output)\n        logits = self.classifier(pooled_output)\n        return logits\n\n# 5.3 LSTM Model (Standalone - Requires non-BERT input processing)\nclass LSTMClassifier(nn.Module):\n    # ... (Keep definition as is, ensure Dataset provides 'token_ids' if used)\n    def __init__(self, config):\n            super().__init__()\n            self.embedding = nn.Embedding(config['vocab_size'], config['embedding_dim'], padding_idx=0) # Asumsi padding_idx=0\n            self.lstm = nn.LSTM(config['embedding_dim'], config['hidden_dim'],\n                                num_layers=config['lstm_layers'], bidirectional=True,\n                                batch_first=True, dropout=config['dropout'] if config['lstm_layers'] > 1 else 0)\n            self.dropout = nn.Dropout(config['dropout'])\n            self.classifier = nn.Linear(config['hidden_dim'] * 2, config['num_classes']) # *2 for bidirectional\n\n    def forward(self, token_ids, **kwargs): # Terima token_ids dari Dataset\n            # This expects 'token_ids' key in the batch from dataloader\n            embedded = self.dropout(self.embedding(token_ids)) # [batch, seq_len, embed_dim]\n            lstm_out, (hidden, cell) = self.lstm(embedded) # lstm_out: [batch, seq_len, hidden*2]\n            # Concat final forward and backward hidden states\n            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1) # [batch, hidden*2]\n            hidden = self.dropout(hidden)\n            logits = self.classifier(hidden)\n            return logits\n\n# 5.4 CNN Model (Standalone - Requires non-BERT input processing)\nclass CNNClassifier(nn.Module):\n    # ... (Keep definition as is, ensure Dataset provides 'token_ids' if used)\n    def __init__(self, config):\n        super().__init__()\n        self.embedding = nn.Embedding(config['vocab_size'], config['embedding_dim'], padding_idx=0)\n        # Conv1d expects (batch, embed_dim, seq_len)\n        self.convs = nn.ModuleList([\n            nn.Conv1d(in_channels=config['embedding_dim'],\n                      out_channels=config['cnn_filters'],\n                      kernel_size=k)\n            for k in config['cnn_kernel_sizes']\n        ])\n        self.dropout = nn.Dropout(config['dropout'])\n        self.classifier = nn.Linear(len(config['cnn_kernel_sizes']) * config['cnn_filters'], config['num_classes'])\n\n    def forward(self, token_ids, **kwargs):\n        # This expects 'token_ids' key in the batch from dataloader\n        embedded = self.embedding(token_ids) # [batch, seq_len, embed_dim]\n        embedded = embedded.permute(0, 2, 1) # [batch, embed_dim, seq_len]\n        embedded = self.dropout(embedded)\n\n        conved = [F.leaky_relu(conv(embedded)) for conv in self.convs] # list of [batch, filters, seq_len-k+1]\n        # Max-over-time pooling\n        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved] # list of [batch, filters]\n        cat = torch.cat(pooled, dim=1) # [batch, filters * num_kernels]\n        cat = self.dropout(cat)\n        logits = self.classifier(cat)\n        return logits\n\n# 5.5 GCN Model (Standalone - Requires non-BERT input processing)\nclass GCNClassifier(nn.Module):\n    # ... (Keep definition as is, ensure Dataset provides 'token_ids' and 'adj_matrix')\n    def __init__(self, config):\n        super().__init__()\n        self.embedding = nn.Embedding(config['vocab_size'], config['embedding_dim'], padding_idx=0)\n        self.dropout = nn.Dropout(config['dropout'])\n        self.gcn_layers = nn.ModuleList()\n        self.gcn_layers.append(GraphConvolution(config['embedding_dim'], config['hidden_dim']))\n        for _ in range(1, config['gcn_layers']):\n            self.gcn_layers.append(GraphConvolution(config['hidden_dim'], config['hidden_dim']))\n        self.classifier = nn.Linear(config['hidden_dim'], config['num_classes'])\n\n    def forward(self, token_ids, adj_matrix, **kwargs):\n        # This expects 'token_ids' and 'adj_matrix' keys\n        embedded = self.dropout(self.embedding(token_ids)) # [batch, seq_len, embed_dim]\n        gcn_output = embedded\n        for gcn_layer in self.gcn_layers:\n            gcn_output = F.leaky_relu(gcn_layer(gcn_output, adj_matrix)) # [batch, seq_len, hidden_dim]\n            gcn_output = self.dropout(gcn_output)\n\n        # Pooling (Max or Mean)\n        pooled = torch.max(gcn_output, dim=1)[0] # [batch, hidden_dim]\n        # pooled = torch.mean(gcn_output, dim=1) # Alternative: Mean pooling\n\n        logits = self.classifier(pooled)\n        return logits\n\n# 5.6 BERT-LSTM-GCN Model\nclass BERTBiLSTMGCNModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert_model_name = config['bert_model_name']\n        self.bert_config = AutoConfig.from_pretrained(self.bert_model_name)\n        self.bert = BertModel.from_pretrained(self.bert_model_name, config=self.bert_config)\n        bert_dim = self.bert_config.hidden_size\n        hidden_dim = config['hidden_dim']\n\n        if config['freeze_bert']:\n            print(\"Freezing BERT parameters.\")\n            for param in self.bert.parameters():\n                param.requires_grad = False\n\n        self.lstm = nn.LSTM(bert_dim, hidden_dim, config['lstm_layers'],\n                            bidirectional=True, batch_first=True,\n                            dropout=config['dropout'] if config['lstm_layers'] > 1 else 0)\n\n        self.gcn_layers = nn.ModuleList()\n        # GCN operates on BERT output dimension first\n        self.gcn_layers.append(GraphConvolution(bert_dim, hidden_dim))\n        for _ in range(1, config['gcn_layers']):\n            self.gcn_layers.append(GraphConvolution(hidden_dim, hidden_dim))\n\n        self.lstm_attention = nn.Linear(hidden_dim * 2, 1)\n        self.lstm_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.gcn_proj = nn.Linear(hidden_dim, hidden_dim) # GCN output is already hidden_dim\n\n        self.use_linguistic = config['use_linguistic_features']\n        if self.use_linguistic:\n            self.linguistic_proj = nn.Linear(config['linguistic_feat_dim'], hidden_dim)\n            fusion_dim = hidden_dim * 3 # LSTM + GCN + Ling\n        else:\n            fusion_dim = hidden_dim * 2 # LSTM + GCN\n\n        self.fusion = nn.Linear(fusion_dim, hidden_dim)\n        self.classifier = nn.Linear(hidden_dim, config['num_classes'])\n        self.dropout = nn.Dropout(config['dropout'])\n\n    def forward(self, input_ids, attention_mask, adj_matrix, linguistic_features=None, **kwargs):\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = bert_outputs.last_hidden_state # [batch, seq_len, bert_dim]\n        sequence_output_dropout = self.dropout(sequence_output) # Apply dropout *once* after BERT\n\n        # --- BiLSTM Path ---\n        # Use the dropout applied version for consistency if needed, or original sequence_output\n        lstm_output, _ = self.lstm(sequence_output_dropout) # [batch, seq_len, hidden*2]\n        attn_weights = F.softmax(self.lstm_attention(lstm_output), dim=1)\n        lstm_pooled = torch.sum(attn_weights * lstm_output, dim=1)\n        lstm_features = F.leaky_relu(self.lstm_proj(lstm_pooled))\n        lstm_features = self.dropout(lstm_features)\n\n        # --- GCN Path ---\n        # Input to GCN is BERT's sequence output\n        gcn_input = sequence_output # Use original BERT output for GCN\n        gcn_output = gcn_input\n        for i, gcn_layer in enumerate(self.gcn_layers):\n             # Apply dropout before activation for subsequent layers\n             if i > 0:\n                 gcn_output = self.dropout(gcn_output)\n             gcn_output = F.leaky_relu(gcn_layer(gcn_output, adj_matrix))\n        # Apply dropout after the last GCN layer before pooling\n        gcn_output = self.dropout(gcn_output)\n\n        # Max pooling for GCN\n        gcn_pooled = torch.max(gcn_output, dim=1)[0]\n        # GCN projection might not be needed if last GCN layer outputs hidden_dim\n        # gcn_features = F.leaky_relu(self.gcn_proj(gcn_pooled)) # Optional projection\n        gcn_features = gcn_pooled # Assume last GCN layer outputs hidden_dim\n        gcn_features = self.dropout(gcn_features) # Dropout after pooling/projection\n\n        # --- Combine Features ---\n        combined_features = [lstm_features, gcn_features]\n\n        if self.use_linguistic and linguistic_features is not None:\n             # Ensure linguistic_features is float\n             linguistic_features = linguistic_features.float()\n             ling_features = F.leaky_relu(self.linguistic_proj(linguistic_features))\n             ling_features = self.dropout(ling_features)\n             combined_features.append(ling_features)\n\n        combined = torch.cat(combined_features, dim=1)\n\n        fused = F.leaky_relu(self.fusion(combined))\n        fused = self.dropout(fused)\n        logits = self.classifier(fused)\n\n        return logits\n\n\n# 5.7 BERT-LSTM-CNN Model\nclass BERTBiLSTMCnnModel(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.bert_model_name = config['bert_model_name']\n        self.bert_config = AutoConfig.from_pretrained(self.bert_model_name)\n        self.bert = BertModel.from_pretrained(self.bert_model_name, config=self.bert_config)\n        bert_dim = self.bert_config.hidden_size\n        hidden_dim = config['hidden_dim']\n\n        if config['freeze_bert']:\n            print(\"Freezing BERT parameters.\")\n            for param in self.bert.parameters():\n                param.requires_grad = False\n\n        self.lstm = nn.LSTM(bert_dim, hidden_dim, config['lstm_layers'],\n                            bidirectional=True, batch_first=True,\n                            dropout=config['dropout'] if config['lstm_layers'] > 1 else 0)\n\n        self.convs = nn.ModuleList([\n            nn.Conv1d(in_channels=bert_dim,\n                      out_channels=config['cnn_filters'],\n                      kernel_size=k)\n            for k in config['cnn_kernel_sizes']\n        ])\n        cnn_output_dim = len(config['cnn_kernel_sizes']) * config['cnn_filters']\n\n        self.lstm_attention = nn.Linear(hidden_dim * 2, 1)\n        self.lstm_proj = nn.Linear(hidden_dim * 2, hidden_dim)\n        self.cnn_proj = nn.Linear(cnn_output_dim, hidden_dim)\n\n        self.use_linguistic = config['use_linguistic_features']\n        if self.use_linguistic:\n            self.linguistic_proj = nn.Linear(config['linguistic_feat_dim'], hidden_dim)\n            fusion_dim = hidden_dim * 3 # LSTM + CNN + Ling\n        else:\n            fusion_dim = hidden_dim * 2 # LSTM + CNN\n\n        self.fusion = nn.Linear(fusion_dim, hidden_dim)\n        self.classifier = nn.Linear(hidden_dim, config['num_classes'])\n        self.dropout = nn.Dropout(config['dropout'])\n\n    def forward(self, input_ids, attention_mask, linguistic_features=None, **kwargs): # adj_matrix not needed\n        bert_outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        sequence_output = bert_outputs.last_hidden_state # [batch, seq_len, bert_dim]\n        sequence_output_dropout = self.dropout(sequence_output)\n\n        # --- BiLSTM Path ---\n        lstm_output, _ = self.lstm(sequence_output_dropout)\n        attn_weights = F.softmax(self.lstm_attention(lstm_output), dim=1)\n        lstm_pooled = torch.sum(attn_weights * lstm_output, dim=1)\n        lstm_features = F.leaky_relu(self.lstm_proj(lstm_pooled))\n        lstm_features = self.dropout(lstm_features)\n\n        # --- CNN Path ---\n        # Input needs to be [batch, bert_dim, seq_len]\n        cnn_input = sequence_output.permute(0, 2, 1)\n        # Apply dropout before CNN? Or rely on dropout after BERT? Let's apply after BERT only.\n        # cnn_input_dropout = sequence_output_dropout.permute(0, 2, 1) # Option\n        conved = [F.leaky_relu(conv(cnn_input)) for conv in self.convs]\n        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n        cnn_pooled = torch.cat(pooled, dim=1)\n        # Apply dropout before projection\n        cnn_pooled = self.dropout(cnn_pooled)\n        cnn_features = F.leaky_relu(self.cnn_proj(cnn_pooled))\n        # Apply dropout after projection? Let's stick to after pooling/before projection.\n\n        # --- Combine Features ---\n        combined_features = [lstm_features, cnn_features]\n\n        if self.use_linguistic and linguistic_features is not None:\n            linguistic_features = linguistic_features.float() # Ensure float\n            ling_features = F.leaky_relu(self.linguistic_proj(linguistic_features))\n            ling_features = self.dropout(ling_features)\n            combined_features.append(ling_features)\n\n        combined = torch.cat(combined_features, dim=1)\n\n        fused = F.leaky_relu(self.fusion(combined))\n        fused = self.dropout(fused)\n        logits = self.classifier(fused)\n\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:38:27.064348Z","iopub.execute_input":"2025-03-28T10:38:27.064671Z","iopub.status.idle":"2025-03-28T10:38:27.102768Z","shell.execute_reply.started":"2025-03-28T10:38:27.064641Z","shell.execute_reply":"2025-03-28T10:38:27.101954Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## Helper Functions","metadata":{}},{"cell_type":"code","source":"# --- 6. Helper Functions ---\n\ndef get_model(config):\n    \"\"\"Factory function to create model instance based on config.\"\"\"\n    model_type = config['model_type']\n    print(f\"Initializing model: {model_type}\")\n\n    if model_type == 'bert':\n        model = BERTClassifier(config)\n    elif model_type == 'lstm':\n        model = LSTMClassifier(config) # Requires Dataset providing 'token_ids'\n    elif model_type == 'cnn':\n        model = CNNClassifier(config)  # Requires Dataset providing 'token_ids'\n    elif model_type == 'gcn':\n        model = GCNClassifier(config) # Requires Dataset providing 'token_ids', 'adj_matrix'\n    elif model_type == 'bert_lstm_gcn':\n        model = BERTBiLSTMGCNModel(config)\n    elif model_type == 'bert_lstm_cnn':\n        model = BERTBiLSTMCnnModel(config)\n    else:\n        raise ValueError(f\"Unknown model_type: {model_type}\")\n\n    # Move model to the primary device specified in config\n    return model.to(config['device'])\n\ndef visualize_model(model, dataloader, config, filename=\"model_graph\"):\n    \"\"\"Visualizes the model using torchviz.\"\"\"\n    if not torchviz:\n        print(\"torchviz not installed. Skipping model visualization.\")\n        return\n    # ===> IMPORTANT: Ensure visualization happens BEFORE DataParallel wrapping <===\n    if isinstance(model, nn.DataParallel):\n        print(\"Warning: Visualizing DataParallel wrapped model. Visualizing the underlying module.\")\n        model_to_viz = model.module\n    else:\n        model_to_viz = model\n\n    try:\n        sample_batch = next(iter(dataloader))\n        # Move only necessary tensor inputs to device\n        inputs = {k: v.to(config['device']) for k, v in sample_batch.items()\n                  if isinstance(v, torch.Tensor) and k != 'label'} # Exclude label\n\n        # Ensure inputs match the model's forward signature (after potential DP wrapping)\n        # Filter inputs based on the actual forward signature (more robust)\n        # This part is complex; simpler to assume inputs are correct for now.\n\n        y = model_to_viz(**inputs)\n\n        dot = torchviz.make_dot(y, params=dict(model_to_viz.named_parameters()))\n        dot.format = 'png'\n        dot.render(filename)\n        print(f\"Model graph saved to {filename}.png\")\n    except Exception as e:\n        print(f\"Could not visualize model: {e}\")\n        print(\"Check if sample batch inputs match the model's forward signature.\")\n        import traceback\n        traceback.print_exc()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:38:32.256431Z","iopub.execute_input":"2025-03-28T10:38:32.256725Z","iopub.status.idle":"2025-03-28T10:38:32.264618Z","shell.execute_reply.started":"2025-03-28T10:38:32.256701Z","shell.execute_reply":"2025-03-28T10:38:32.263553Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Training & Evaluation Logic","metadata":{}},{"cell_type":"code","source":"# --- 7. Training & Evaluation Logic ---\n\ndef train_epoch(model, dataloader, optimizer, criterion, device, config, epoch):\n    model.train() # Set model to training mode\n    epoch_loss = 0\n    correct = 0\n    total = 0\n\n    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False, dynamic_ncols=True)\n    for batch in progress_bar:\n        # Move tensors to the primary device (DataParallel handles distribution)\n        inputs = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n        labels = inputs.pop('label')\n\n        optimizer.zero_grad()\n\n        # Forward pass - DataParallel handles splitting/gathering\n        outputs = model(**inputs)\n\n        # Loss calculation happens on the primary device (where outputs are gathered)\n        loss = criterion(outputs, labels)\n\n        # Check for NaN loss\n        if torch.isnan(loss):\n             print(\"\\nNaN loss detected! Skipping batch.\")\n             # Optionally add more debugging here: print inputs, outputs etc.\n             # Consider gradient clipping if gradients explode.\n             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Example clipping\n             continue # Skip backward pass for this batch\n\n\n        # Backward pass - Gradients are computed on each GPU and summed on primary\n        loss.backward()\n\n        # Optional: Gradient Clipping (useful if gradients explode)\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n\n        # Metrics calculated on primary device\n        epoch_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\n        batch_acc = (predicted == labels).sum().item() / labels.size(0) if labels.size(0) > 0 else 0\n        progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\", 'acc': f\"{batch_acc:.4f}\"})\n\n    if total == 0: return 0.0, 0.0 # Avoid division by zero\n    accuracy = correct / total\n    avg_loss = epoch_loss / len(dataloader)\n    return avg_loss, accuracy\n\ndef evaluate_epoch(model, dataloader, criterion, device, config):\n    model.eval() # Set model to evaluation mode\n    epoch_loss = 0\n    correct = 0\n    total = 0\n    all_preds = []\n    all_labels = []\n    all_probs = []\n\n    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False, dynamic_ncols=True)\n    with torch.no_grad():\n        for batch in progress_bar:\n            inputs = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n            labels = inputs.pop('label')\n\n            outputs = model(**inputs)\n            loss = criterion(outputs, labels) # Calculated on primary GPU\n\n            epoch_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n\n            # Collect predictions and labels (move to CPU for numpy/sklearn)\n            all_labels.extend(labels.cpu().numpy())\n            all_preds.extend(predicted.cpu().numpy())\n\n            if outputs.shape[1] >= 2:\n                probs = F.softmax(outputs, dim=1)[:, 1].cpu().numpy() # Prob for positive class\n                all_probs.extend(probs)\n            elif outputs.shape[1] == 1:\n                 probs = torch.sigmoid(outputs).squeeze().cpu().numpy()\n                 all_probs.extend(probs)\n\n\n    if total == 0: return 0.0, 0.0, None, [], [] # Avoid division by zero\n    accuracy = correct / total\n    avg_loss = epoch_loss / len(dataloader)\n\n    roc_auc = None\n    # Ensure binary classification and probabilities were collected\n    if len(np.unique(all_labels)) == 2 and len(all_probs) == len(all_labels):\n        try:\n            # Ensure labels are binary (0, 1) for roc_auc_score\n            # The LabelEncoder already produced 0 and 1.\n            roc_auc = roc_auc_score(all_labels, all_probs)\n        except ValueError as e:\n            print(f\"Warning: Could not calculate ROC AUC - {e}\")\n        except Exception as e: # Catch other potential errors\n            print(f\"Warning: Error calculating ROC AUC: {e}\")\n\n\n    return avg_loss, accuracy, roc_auc, all_labels, all_preds\n\ndef plot_training_history(history, filename=\"training_history.png\"):\n    \"\"\"Plots loss and accuracy curves.\"\"\"\n    epochs_ran = len(history['train_loss'])\n    if epochs_ran == 0:\n        print(\"No history to plot.\")\n        return\n    epochs = range(1, epochs_ran + 1)\n\n    plt.figure(figsize=(12, 5))\n\n    # Plot Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, history['train_loss'], 'bo-', label='Training Loss')\n    plt.plot(epochs, history['val_loss'], 'ro-', label='Validation Loss')\n    plt.title('Training and Validation Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.grid(True)\n\n    # Plot Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, history['train_acc'], 'bo-', label='Training Accuracy')\n    plt.plot(epochs, history['val_acc'], 'ro-', label='Validation Accuracy')\n    # Filter out None values before plotting ROC AUC\n    valid_roc_auc = [(e, auc) for e, auc in zip(epochs, history.get('val_roc_auc', [])) if auc is not None]\n    if valid_roc_auc:\n        roc_epochs, roc_values = zip(*valid_roc_auc)\n        plt.plot(roc_epochs, roc_values, 'go-', label='Validation ROC AUC')\n\n    plt.title('Training and Validation Metrics')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy / ROC AUC')\n    # Adjust ylim if needed, e.g., plt.ylim(0, 1.05) for accuracy/AUC\n    plt.legend()\n    plt.grid(True)\n\n    plt.tight_layout()\n    plt.savefig(filename)\n    print(f\"Training history plot saved to {filename}\")\n    plt.close() # Close the plot to free memory, especially in loops/notebooks\n\ndef save_training_log(history, filename):\n    \"\"\"Saves training history to a JSON file.\"\"\"\n    try:\n        # Convert numpy arrays or tensors in history to lists for JSON serialization\n        serializable_history = {}\n        for key, value in history.items():\n            if isinstance(value, list) and value and isinstance(value[0], (np.ndarray, torch.Tensor)):\n                 serializable_history[key] = [item.tolist() if hasattr(item, 'tolist') else item for item in value]\n            elif isinstance(value, (np.ndarray, torch.Tensor)):\n                 serializable_history[key] = value.tolist()\n            else:\n                 serializable_history[key] = value # Assume already serializable\n\n        with open(filename, 'w') as f:\n            json.dump(serializable_history, f, indent=4)\n        print(f\"Training log saved to {filename}\")\n    except Exception as e:\n        print(f\"Error saving training log: {e}\")\n\n\n\n\ndef run_training(config, model, train_dataloader, val_dataloader):\n    \"\"\"Main training loop.\"\"\"\n    device = config['device'] # Primary device\n    lr = config['learning_rate'] if 'bert' in config['model_type'] else config['non_bert_lr']\n    # Pass model.module.parameters() if DataParallel, else model.parameters()\n    # Optimizer usually handles DataParallel correctly, passing model.parameters() is fine.\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n\n    best_val_metric = -1\n    metric_to_monitor = 'acc' # 'acc', 'roc_auc', 'loss'\n    patience_counter = 0\n    patience_limit = 3 # Example early stopping patience\n\n    training_history = {\n        'train_loss': [], 'train_acc': [],\n        'val_loss': [], 'val_acc': [], 'val_roc_auc': []\n    }\n\n    print(\"\\n--- Starting Training ---\")\n    num_gpus = torch.cuda.device_count()\n    model_is_parallel = isinstance(model, nn.DataParallel)\n    if model_is_parallel:\n        print(f\"Training with DataParallel on {num_gpus} GPUs.\")\n    elif device != 'cpu':\n         print(f\"Training on single GPU: {torch.cuda.get_device_name(0)}\")\n    else:\n        print(\"Training on CPU.\")\n\n\n    for epoch in range(config['epochs']):\n        print(f\"\\nEpoch {epoch+1}/{config['epochs']}\")\n\n        train_loss, train_acc = train_epoch(model, train_dataloader, optimizer, criterion, device, config, epoch + 1) # Pass epoch number\n        training_history['train_loss'].append(train_loss)\n        training_history['train_acc'].append(train_acc)\n\n        val_loss, val_acc, val_roc_auc, _, _ = evaluate_epoch(model, val_dataloader, criterion, device, config)\n        training_history['val_loss'].append(val_loss)\n        training_history['val_acc'].append(val_acc)\n        training_history['val_roc_auc'].append(val_roc_auc) # Can be None\n\n        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\", end=\"\")\n        if val_roc_auc is not None:\n            print(f\", Val ROC AUC: {val_roc_auc:.4f}\")\n        else:\n            print(\", Val ROC AUC: N/A\") # Handle None case\n\n        # === WANDB INTEGRATION START ===\n        # --- Log metrics to WandB ---\n        log_dict = {\n            \"epoch\": epoch + 1,\n            \"train/loss\": train_loss,\n            \"train/accuracy\": train_acc,\n            \"val/loss\": val_loss,\n            \"val/accuracy\": val_acc,\n        }\n        # Only log ROC AUC if it's calculated\n        if val_roc_auc is not None:\n            log_dict[\"val/roc_auc\"] = val_roc_auc\n        wandb.log(log_dict)\n        # === WANDB INTEGRATION END ===\n\n        # Determine the current metric value to check for improvement\n        current_val_metric = None\n        if metric_to_monitor == 'acc':\n            current_val_metric = val_acc\n        elif metric_to_monitor == 'roc_auc':\n            current_val_metric = val_roc_auc # Could be None\n        elif metric_to_monitor == 'loss':\n             current_val_metric = val_loss # Lower is better\n\n        is_better = False\n        if current_val_metric is not None: # Only compare if the metric is valid\n            if metric_to_monitor == 'loss':\n                is_better = current_val_metric < best_val_metric if best_val_metric != float('inf') else True\n                if is_better: best_val_metric = current_val_metric # Update best_val_metric if lower loss\n            else: # Accuracy or ROC AUC - higher is better\n                is_better = current_val_metric > best_val_metric\n                if is_better: best_val_metric = current_val_metric # Update best_val_metric if higher metric\n\n\n        if is_better:\n            print(f\"Validation {metric_to_monitor} improved ({best_val_metric:.4f} -> {current_val_metric:.4f}). Saving model...\")\n            best_val_metric = current_val_metric\n            try:\n                # ====> SAVE CORRECT STATE DICT <====\n                if isinstance(model, nn.DataParallel):\n                    model_state = model.module.state_dict() # Save the underlying model's state\n                else:\n                    model_state = model.state_dict()\n                save_file(model_state, config['model_save_path'])\n                print(f\"Best model saved to {config['model_save_path']}\")\n\n                # === WANDB INTEGRATION START ===\n                # --- Log the best model as an artifact ---\n                try:\n                    model_artifact_name = f\"{wandb.run.name}-best-model\"\n                    config_dict = dict(config)\n                    artifact = wandb.Artifact(model_artifact_name, type='model', metadata=config_dict)\n                    artifact.add_file(config['model_save_path'])\n                    wandb.log_artifact(artifact, aliases=[\"best\", f\"epoch_{epoch+1}\"])\n                    print(f\"Wandb: Logged best model artifact: {model_artifact_name}\")\n                except Exception as e:\n                    print(f\"Wandb: Error logging model artifact: {e}\")\n                # === WANDB INTEGRATION END ===\n                \n                patience_counter = 0 # Reset patience\n            except Exception as e:\n                print(f\"Error saving model: {e}\")\n        else:\n             patience_counter += 1\n             print(f\"Validation {metric_to_monitor} did not improve. Patience: {patience_counter}/{patience_limit}\")\n             if patience_counter >= patience_limit:\n                 print(\"Early stopping triggered.\")\n                 break # Stop training\n\n\n    print(\"--- Training Finished ---\")\n    save_training_log(training_history, config['log_file'])\n    plot_training_history(training_history) # Plot at the end\n\n    # Return the model (it might be wrapped in DataParallel) and history\n\n    # === WANDB INTEGRATION START ===\n    # --- Log the final training history plot ---\n    try:\n        history_plot_filename = \"training_history.png\"\n        if os.path.exists(history_plot_filename):\n            wandb.log({\"training_curves\": wandb.Image(history_plot_filename)})\n    except Exception as e:\n        print(f\"Wandb: Could not log training history plot: {e}\")\n    # === WANDB INTEGRATION END ===\n\n    \n    return model, training_history","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:38:35.056342Z","iopub.execute_input":"2025-03-28T10:38:35.056670Z","iopub.status.idle":"2025-03-28T10:38:35.082825Z","shell.execute_reply.started":"2025-03-28T10:38:35.056641Z","shell.execute_reply":"2025-03-28T10:38:35.082002Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def evaluate_model_on_test(config, test_dataloader, label_encoder):\n    \"\"\"Evaluates the final *saved* model on the test set.\"\"\"\n    print(\"\\n--- Evaluating on Test Set ---\")\n    device = config['device']\n    criterion = nn.CrossEntropyLoss() # For loss calculation only\n\n    # 1. Create a fresh model instance on the primary device\n    model = get_model(config)\n\n    # 2. Load the *base model* state dict saved during training\n    try:\n        print(f\"Loading best model state from: {config['model_save_path']}\")\n        # Load state dict onto the primary device directly\n        state_dict = load_file(config['model_save_path'], device=device)\n        model.load_state_dict(state_dict)\n        print(\"Model state loaded successfully.\")\n    except FileNotFoundError:\n        print(f\"Error: Model file not found at {config['model_save_path']}. Cannot evaluate.\")\n        # === WANDB INTEGRATION START ===\n        if wandb.run: # Check if a wandb run is active before finishing\n             wandb.finish()\n        # === WANDB INTEGRATION END ===\n        return\n    except Exception as e:\n        print(f\"Error loading model state: {e}. Cannot evaluate.\")\n        # === WANDB INTEGRATION START ===\n        if wandb.run:\n             wandb.finish()\n        # === WANDB INTEGRATION END ===\n        return\n\n    # 3. Wrap the loaded model with DataParallel if multiple GPUs are available for evaluation\n    num_gpus = torch.cuda.device_count()\n    if num_gpus > 1 and device != 'cpu':\n        print(f\"Using {num_gpus} GPUs for evaluation! Wrapping model with DataParallel.\")\n        model = nn.DataParallel(model)\n        # Note: model is already on the primary device 'cuda:0' from get_model\n\n    model.to(device) # Ensure model is on the correct device (redundant if already done, but safe)\n\n    # 4. Perform evaluation\n    test_loss, test_acc, test_roc_auc, y_true, y_pred = evaluate_epoch(model, test_dataloader, criterion, device, config)\n\n    print(f\"\\nTest Loss: {test_loss:.4f}\")\n    print(f\"Test Accuracy: {test_acc:.4f}\")\n    if test_roc_auc is not None:\n        print(f\"Test ROC AUC: {test_roc_auc:.4f}\")\n    else:\n        print(\"Test ROC AUC: N/A\")\n\n\n    # === WANDB INTEGRATION START ===\n    # --- Log final test metrics ---\n    # Use wandb.summary for metrics that summarize the *entire run*\n    if wandb.run: # Check if run is active\n        wandb.summary[\"test/loss\"] = test_loss\n        wandb.summary[\"test/accuracy\"] = test_acc\n        if test_roc_auc is not None:\n            wandb.summary[\"test/roc_auc\"] = test_roc_auc\n    # === WANDB INTEGRATION END ===\n\n\n    # Ensure labels and preds are available before generating reports\n    if not y_true or not y_pred:\n         print(\"Evaluation failed, no predictions generated.\")\n         # === WANDB INTEGRATION START ===\n         # Finish the run even if evaluation fails partially\n         if wandb.run:\n             wandb.finish()\n         # === WANDB INTEGRATION END ===\n         return\n\n\n    # Classification Report\n    target_names = label_encoder.classes_.astype(str)\n    print(\"\\nClassification Report:\")\n    try: # Outer try for classification report generation/logging\n        # Ensure y_true and y_pred are numpy arrays\n        y_true_np = np.array(y_true)\n        y_pred_np = np.array(y_pred)\n        report_str = classification_report(y_true_np, y_pred_np, target_names=target_names, digits=4)\n        print(report_str)\n        # === WANDB INTEGRATION START ===\n        # Log classification report as text (optional)\n        try: # Inner try for logging\n             if wandb.run:\n                 wandb.log({\"test/classification_report\": wandb.Html(f\"<pre>{report_str}</pre>\")})\n        except Exception as e:\n             print(f\"Wandb: Error logging classification report: {e}\")\n        # === WANDB INTEGRATION END ===\n    except Exception as e: # <--- **** ADDED THIS EXCEPT BLOCK ****\n         print(f\"Could not generate or log classification report: {e}\")\n    # <--- **** END OF ADDED BLOCK **** ---\n\n    # Confusion Matrix\n    print(\"\\nConfusion Matrix:\")\n    try: # Outer try for overall CM generation/saving\n        y_true_np = np.array(y_true)\n        y_pred_np = np.array(y_pred)\n        cm = confusion_matrix(y_true_np, y_pred_np)\n        plt.figure(figsize=(6, 5))\n        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_names, yticklabels=target_names)\n        plt.xlabel('Predicted Label')\n        plt.ylabel('True Label')\n        plt.title('Confusion Matrix (Test Set)')\n        cm_filename = \"confusion_matrix_test.png\"\n        plt.savefig(cm_filename)\n        print(f\"Confusion matrix saved to {cm_filename}\")\n        plt.close() # Close plot\n\n        # === WANDB INTEGRATION START ===\n        # --- Log confusion matrix plot ---\n        try: # Inner try specifically for logging the plot\n            if wandb.run:\n                wandb.log({\"test/confusion_matrix\": wandb.Image(cm_filename)})\n        except Exception as e: # Exception for the INNER try (wandb logging)\n            print(f\"Wandb: Could not log confusion matrix image: {e}\")\n        # === WANDB INTEGRATION END ===\n\n    except Exception as e: # Exception for the OUTER try (generating/saving the plot)\n        print(f\"Could not generate or save confusion matrix plot: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:38:40.472043Z","iopub.execute_input":"2025-03-28T10:38:40.472360Z","iopub.status.idle":"2025-03-28T10:38:40.483391Z","shell.execute_reply.started":"2025-03-28T10:38:40.472335Z","shell.execute_reply":"2025-03-28T10:38:40.482393Z"}},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":"# Main Execution","metadata":{}},{"cell_type":"markdown","source":"## CNN","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # === WANDB INTEGRATION START ===\n    # --- Log in to WandB using Kaggle Secrets ---\n    try:\n        from kaggle_secrets import UserSecretsClient\n        import os # Import os to potentially set environment variable\n\n        user_secrets = UserSecretsClient()\n\n        # Retrieve the secret variable (ensure the label matches what you set in Kaggle Secrets)\n        # METHOD 1: Directly use the key with wandb.login (Recommended for clarity)\n        wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n        wandb.login(key=wandb_api_key)\n        print(\"Wandb login successful using Kaggle Secret.\")\n\n        # METHOD 2: Set environment variable (wandb.init often picks this up automatically)\n        # os.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"WANDB_API_KEY\")\n        # print(\"WANDB_API_KEY environment variable set from Kaggle Secret.\")\n        # If using Method 2, wandb.login() might not even be strictly necessary\n        # as wandb.init() usually checks this environment variable.\n\n    except ImportError:\n        print(\"kaggle_secrets not found, assuming API key is set elsewhere (e.g., environment variable).\")\n    except Exception as e:\n        print(f\"Error retrieving WANDB_API_KEY from Kaggle Secrets or logging in: {e}\")\n        # Decide how to handle this - maybe exit, maybe proceed if key might be elsewhere\n        # For now, we'll proceed, but wandb logging might fail.\n        pass # Or raise e if login is critical\n\n\n    # --- Initialize WandB Run ---\n    run_name = f\"{config['model_type']}-e{config['epochs']}-bs{config['batch_size']}-lr{config['learning_rate']}\"\n    # wandb.init should now work without prompting if login was successful or env var is set\n    wandb.init(\n        project=\"sentiment-analysis-hybrid\",  # Replace with your project name\n        config=config,\n        name=run_name,\n        job_type=\"train\",\n    )\n    config = wandb.config # Copy back config\n    # === WANDB INTEGRATION END ===\n    \n    # 1. Get the base model instance (moved to primary device by get_model)\n    model = get_model(config)\n    primary_device = config['device']\n    num_gpus = torch.cuda.device_count()\n\n    # === WANDB INTEGRATION START ===\n    # --- Watch the model (optional but useful for gradients/parameters) ---\n    # Apply wandb.watch *before* DataParallel wrapping if possible\n    # Note: Watching DataParallel models might require watching model.module later\n    try:\n        # Log gradients and parameters of the model\n        wandb.watch(model, log=\"all\", log_freq=100) # Log every 100 batches\n    except Exception as e:\n        print(f\"Could not watch model with wandb: {e}\")\n    # === WANDB INTEGRATION END ===\n\n    # 2. (Optional) Visualize Architecture *before* wrapping\n    if torchviz and len(train_dataloader) > 0:\n        print(\"Attempting model visualization...\")\n        # Pass the unwrapped model\n        visualize_model(model, train_dataloader, config, filename=f\"{config['model_type']}_graph\")\n       \n        # === WANDB INTEGRATION START ===\n        # Log the model graph image (if created)\n        try:\n            graph_filename = f\"{config['model_type']}_graph.png\"\n            if os.path.exists(graph_filename):\n                 wandb.log({\"model_architecture\": wandb.Image(graph_filename)})\n        except Exception as e:\n            print(f\"Wandb: Could not log model architecture image: {e}\")\n        # === WANDB INTEGRATION END ===\n\n    # 3. Wrap model with DataParallel *before* training if multiple GPUs available\n    if num_gpus > 1 and primary_device != 'cpu':\n        print(f\"Wrapping model with nn.DataParallel for training across {num_gpus} GPUs.\")\n        model = nn.DataParallel(model)\n        # DataParallel will manage distribution to devices cuda:0, cuda:1, ...\n        # Model should already be on cuda:0 from get_model\n\n    # Ensure model is on the correct device (important if CPU or single GPU)\n    model.to(primary_device)\n\n    # 4. Latih Model (pass the potentially wrapped model)\n    # run_training now returns the potentially wrapped model and history\n    trained_model, history = run_training(config, model, train_dataloader, val_dataloader)\n\n    # 5. Evaluasi Model Terbaik di Test Set\n    # The evaluation function now handles loading the saved *base* state dict\n    # and potentially wrapping the model again for evaluation.\n    evaluate_model_on_test(config, test_dataloader, label_encoder)\n\n    print(\"\\n--- Analysis Complete ---\")\n\n    # === WANDB INTEGRATION START ===\n    # --- Finish the WandB run ---\n    wandb.finish()\n    # === WANDB INTEGRATION END ===\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T07:58:58.360307Z","iopub.execute_input":"2025-03-27T07:58:58.360620Z","iopub.status.idle":"2025-03-27T08:00:32.290886Z","shell.execute_reply.started":"2025-03-27T07:58:58.360595Z","shell.execute_reply":"2025-03-27T08:00:32.290254Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaybeitsai\u001b[0m (\u001b[33mhkacode\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Wandb login successful using Kaggle Secret.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250327_075904-gdxoi4ng</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/gdxoi4ng' target=\"_blank\">cnn-e3-bs128-lr2e-05</a></strong> to <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/gdxoi4ng' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/gdxoi4ng</a>"},"metadata":{}},{"name":"stdout","text":"Initializing model: cnn\nAttempting model visualization...\nModel graph saved to cnn_graph.png\nWrapping model with nn.DataParallel for training across 2 GPUs.\n\n--- Starting Training ---\nTraining with DataParallel on 2 GPUs.\n\nEpoch 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.4426, Train Acc: 0.7949\nVal Loss: 0.3614, Val Acc: 0.8416, Val ROC AUC: 0.9207\nValidation acc improved (0.8416 -> 0.8416). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: cnn-e3-bs128-lr2e-05-best-model\n\nEpoch 2/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.3441, Train Acc: 0.8507\nVal Loss: 0.3502, Val Acc: 0.8482, Val ROC AUC: 0.9311\nValidation acc improved (0.8482 -> 0.8482). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: cnn-e3-bs128-lr2e-05-best-model\n\nEpoch 3/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.3095, Train Acc: 0.8702\nVal Loss: 0.3259, Val Acc: 0.8590, Val ROC AUC: 0.9359\nValidation acc improved (0.8590 -> 0.8590). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: cnn-e3-bs128-lr2e-05-best-model\n--- Training Finished ---\nTraining log saved to training_log.json\nTraining history plot saved to training_history.png\n\n--- Evaluating on Test Set ---\nInitializing model: cnn\nLoading best model state from: best_model.safetensors\nModel state loaded successfully.\nUsing 2 GPUs for evaluation! Wrapping model with DataParallel.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/235 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nTest Loss: 0.3173\nTest Accuracy: 0.8602\nTest ROC AUC: 0.9386\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    negative     0.8655    0.8530    0.8592     15000\n    positive     0.8551    0.8674    0.8612     15000\n\n    accuracy                         0.8602     30000\n   macro avg     0.8603    0.8602    0.8602     30000\nweighted avg     0.8603    0.8602    0.8602     30000\n\n\nConfusion Matrix:\nConfusion matrix saved to confusion_matrix_test.png\n\n--- Analysis Complete ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train/accuracy</td><td>▁▆█</td></tr><tr><td>train/loss</td><td>█▃▁</td></tr><tr><td>val/accuracy</td><td>▁▄█</td></tr><tr><td>val/loss</td><td>█▆▁</td></tr><tr><td>val/roc_auc</td><td>▁▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>test/accuracy</td><td>0.8602</td></tr><tr><td>test/loss</td><td>0.31728</td></tr><tr><td>test/roc_auc</td><td>0.93856</td></tr><tr><td>train/accuracy</td><td>0.87023</td></tr><tr><td>train/loss</td><td>0.30949</td></tr><tr><td>val/accuracy</td><td>0.85905</td></tr><tr><td>val/loss</td><td>0.3259</td></tr><tr><td>val/roc_auc</td><td>0.93588</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">cnn-e3-bs128-lr2e-05</strong> at: <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/gdxoi4ng' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/gdxoi4ng</a><br> View project at: <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 4 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250327_075904-gdxoi4ng/logs</code>"},"metadata":{}}],"execution_count":18},{"cell_type":"markdown","source":"## LSTM","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # === WANDB INTEGRATION START ===\n    # --- Log in to WandB using Kaggle Secrets ---\n    try:\n        from kaggle_secrets import UserSecretsClient\n        import os # Import os to potentially set environment variable\n\n        user_secrets = UserSecretsClient()\n\n        # Retrieve the secret variable (ensure the label matches what you set in Kaggle Secrets)\n        # METHOD 1: Directly use the key with wandb.login (Recommended for clarity)\n        wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n        wandb.login(key=wandb_api_key)\n        print(\"Wandb login successful using Kaggle Secret.\")\n\n        # METHOD 2: Set environment variable (wandb.init often picks this up automatically)\n        # os.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"WANDB_API_KEY\")\n        # print(\"WANDB_API_KEY environment variable set from Kaggle Secret.\")\n        # If using Method 2, wandb.login() might not even be strictly necessary\n        # as wandb.init() usually checks this environment variable.\n\n    except ImportError:\n        print(\"kaggle_secrets not found, assuming API key is set elsewhere (e.g., environment variable).\")\n    except Exception as e:\n        print(f\"Error retrieving WANDB_API_KEY from Kaggle Secrets or logging in: {e}\")\n        # Decide how to handle this - maybe exit, maybe proceed if key might be elsewhere\n        # For now, we'll proceed, but wandb logging might fail.\n        pass # Or raise e if login is critical\n\n\n    # --- Initialize WandB Run ---\n    run_name = f\"{config['model_type']}-e{config['epochs']}-bs{config['batch_size']}-lr{config['learning_rate']}\"\n    # wandb.init should now work without prompting if login was successful or env var is set\n    wandb.init(\n        project=\"sentiment-analysis-hybrid\",  # Replace with your project name\n        config=config,\n        name=run_name,\n        job_type=\"train\",\n    )\n    config = wandb.config # Copy back config\n    # === WANDB INTEGRATION END ===\n    \n    # 1. Get the base model instance (moved to primary device by get_model)\n    model = get_model(config)\n    primary_device = config['device']\n    num_gpus = torch.cuda.device_count()\n\n    # === WANDB INTEGRATION START ===\n    # --- Watch the model (optional but useful for gradients/parameters) ---\n    # Apply wandb.watch *before* DataParallel wrapping if possible\n    # Note: Watching DataParallel models might require watching model.module later\n    try:\n        # Log gradients and parameters of the model\n        wandb.watch(model, log=\"all\", log_freq=100) # Log every 100 batches\n    except Exception as e:\n        print(f\"Could not watch model with wandb: {e}\")\n    # === WANDB INTEGRATION END ===\n\n    # 2. (Optional) Visualize Architecture *before* wrapping\n    if torchviz and len(train_dataloader) > 0:\n        print(\"Attempting model visualization...\")\n        # Pass the unwrapped model\n        visualize_model(model, train_dataloader, config, filename=f\"{config['model_type']}_graph\")\n       \n        # === WANDB INTEGRATION START ===\n        # Log the model graph image (if created)\n        try:\n            graph_filename = f\"{config['model_type']}_graph.png\"\n            if os.path.exists(graph_filename):\n                 wandb.log({\"model_architecture\": wandb.Image(graph_filename)})\n        except Exception as e:\n            print(f\"Wandb: Could not log model architecture image: {e}\")\n        # === WANDB INTEGRATION END ===\n\n    # 3. Wrap model with DataParallel *before* training if multiple GPUs available\n    if num_gpus > 1 and primary_device != 'cpu':\n        print(f\"Wrapping model with nn.DataParallel for training across {num_gpus} GPUs.\")\n        model = nn.DataParallel(model)\n        # DataParallel will manage distribution to devices cuda:0, cuda:1, ...\n        # Model should already be on cuda:0 from get_model\n\n    # Ensure model is on the correct device (important if CPU or single GPU)\n    model.to(primary_device)\n\n    # 4. Latih Model (pass the potentially wrapped model)\n    # run_training now returns the potentially wrapped model and history\n    trained_model, history = run_training(config, model, train_dataloader, val_dataloader)\n\n    # 5. Evaluasi Model Terbaik di Test Set\n    # The evaluation function now handles loading the saved *base* state dict\n    # and potentially wrapping the model again for evaluation.\n    evaluate_model_on_test(config, test_dataloader, label_encoder)\n\n    print(\"\\n--- Analysis Complete ---\")\n\n    # === WANDB INTEGRATION START ===\n    # --- Finish the WandB run ---\n    wandb.finish()\n    # === WANDB INTEGRATION END ===\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T08:27:11.203268Z","iopub.execute_input":"2025-03-27T08:27:11.203589Z","iopub.status.idle":"2025-03-27T08:28:37.147352Z","shell.execute_reply.started":"2025-03-27T08:27:11.203563Z","shell.execute_reply":"2025-03-27T08:28:37.146691Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaybeitsai\u001b[0m (\u001b[33mhkacode\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Wandb login successful using Kaggle Secret.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250327_082719-9mhawvs7</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/9mhawvs7' target=\"_blank\">lstm-e3-bs128-lr2e-05</a></strong> to <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/9mhawvs7' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/9mhawvs7</a>"},"metadata":{}},{"name":"stdout","text":"Initializing model: lstm\nAttempting model visualization...\nModel graph saved to lstm_graph.png\nWrapping model with nn.DataParallel for training across 2 GPUs.\n\n--- Starting Training ---\nTraining with DataParallel on 2 GPUs.\n\nEpoch 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.4442, Train Acc: 0.7911\nVal Loss: 0.4825, Val Acc: 0.7770, Val ROC AUC: 0.8896\nValidation acc improved (0.7770 -> 0.7770). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: lstm-e3-bs128-lr2e-05-best-model\n\nEpoch 2/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.2902, Train Acc: 0.8828\nVal Loss: 0.3692, Val Acc: 0.8231, Val ROC AUC: 0.9363\nValidation acc improved (0.8231 -> 0.8231). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: lstm-e3-bs128-lr2e-05-best-model\n\nEpoch 3/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.2407, Train Acc: 0.9036\nVal Loss: 0.4938, Val Acc: 0.7783, Val ROC AUC: 0.9407\nValidation acc did not improve. Patience: 1/3\n--- Training Finished ---\nTraining log saved to training_log.json\nTraining history plot saved to training_history.png\n\n--- Evaluating on Test Set ---\nInitializing model: lstm\nLoading best model state from: best_model.safetensors\nModel state loaded successfully.\nUsing 2 GPUs for evaluation! Wrapping model with DataParallel.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/235 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nTest Loss: 0.3602\nTest Accuracy: 0.8287\nTest ROC AUC: 0.9393\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    negative     0.9417    0.7007    0.8035     15000\n    positive     0.7617    0.9566    0.8481     15000\n\n    accuracy                         0.8287     30000\n   macro avg     0.8517    0.8287    0.8258     30000\nweighted avg     0.8517    0.8287    0.8258     30000\n\n\nConfusion Matrix:\nConfusion matrix saved to confusion_matrix_test.png\n\n--- Analysis Complete ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train/accuracy</td><td>▁▇█</td></tr><tr><td>train/loss</td><td>█▃▁</td></tr><tr><td>val/accuracy</td><td>▁█▁</td></tr><tr><td>val/loss</td><td>▇▁█</td></tr><tr><td>val/roc_auc</td><td>▁▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>test/accuracy</td><td>0.82867</td></tr><tr><td>test/loss</td><td>0.36022</td></tr><tr><td>test/roc_auc</td><td>0.93929</td></tr><tr><td>train/accuracy</td><td>0.90358</td></tr><tr><td>train/loss</td><td>0.24066</td></tr><tr><td>val/accuracy</td><td>0.7783</td></tr><tr><td>val/loss</td><td>0.4938</td></tr><tr><td>val/roc_auc</td><td>0.94071</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lstm-e3-bs128-lr2e-05</strong> at: <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/9mhawvs7' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/9mhawvs7</a><br> View project at: <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 4 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250327_082719-9mhawvs7/logs</code>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## GCN","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # === WANDB INTEGRATION START ===\n    # --- Log in to WandB using Kaggle Secrets ---\n    try:\n        from kaggle_secrets import UserSecretsClient\n        import os # Import os to potentially set environment variable\n\n        user_secrets = UserSecretsClient()\n\n        # Retrieve the secret variable (ensure the label matches what you set in Kaggle Secrets)\n        # METHOD 1: Directly use the key with wandb.login (Recommended for clarity)\n        wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n        wandb.login(key=wandb_api_key)\n        print(\"Wandb login successful using Kaggle Secret.\")\n\n        # METHOD 2: Set environment variable (wandb.init often picks this up automatically)\n        # os.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"WANDB_API_KEY\")\n        # print(\"WANDB_API_KEY environment variable set from Kaggle Secret.\")\n        # If using Method 2, wandb.login() might not even be strictly necessary\n        # as wandb.init() usually checks this environment variable.\n\n    except ImportError:\n        print(\"kaggle_secrets not found, assuming API key is set elsewhere (e.g., environment variable).\")\n    except Exception as e:\n        print(f\"Error retrieving WANDB_API_KEY from Kaggle Secrets or logging in: {e}\")\n        # Decide how to handle this - maybe exit, maybe proceed if key might be elsewhere\n        # For now, we'll proceed, but wandb logging might fail.\n        pass # Or raise e if login is critical\n\n\n    # --- Initialize WandB Run ---\n    run_name = f\"{config['model_type']}-e{config['epochs']}-bs{config['batch_size']}-lr{config['learning_rate']}\"\n    # wandb.init should now work without prompting if login was successful or env var is set\n    wandb.init(\n        project=\"sentiment-analysis-hybrid\",  # Replace with your project name\n        config=config,\n        name=run_name,\n        job_type=\"train\",\n    )\n    config = wandb.config # Copy back config\n    # === WANDB INTEGRATION END ===\n    \n    # 1. Get the base model instance (moved to primary device by get_model)\n    model = get_model(config)\n    primary_device = config['device']\n    num_gpus = torch.cuda.device_count()\n\n    # === WANDB INTEGRATION START ===\n    # --- Watch the model (optional but useful for gradients/parameters) ---\n    # Apply wandb.watch *before* DataParallel wrapping if possible\n    # Note: Watching DataParallel models might require watching model.module later\n    try:\n        # Log gradients and parameters of the model\n        wandb.watch(model, log=\"all\", log_freq=100) # Log every 100 batches\n    except Exception as e:\n        print(f\"Could not watch model with wandb: {e}\")\n    # === WANDB INTEGRATION END ===\n\n    # 2. (Optional) Visualize Architecture *before* wrapping\n    if torchviz and len(train_dataloader) > 0:\n        print(\"Attempting model visualization...\")\n        # Pass the unwrapped model\n        visualize_model(model, train_dataloader, config, filename=f\"{config['model_type']}_graph\")\n       \n        # === WANDB INTEGRATION START ===\n        # Log the model graph image (if created)\n        try:\n            graph_filename = f\"{config['model_type']}_graph.png\"\n            if os.path.exists(graph_filename):\n                 wandb.log({\"model_architecture\": wandb.Image(graph_filename)})\n        except Exception as e:\n            print(f\"Wandb: Could not log model architecture image: {e}\")\n        # === WANDB INTEGRATION END ===\n\n    # 3. Wrap model with DataParallel *before* training if multiple GPUs available\n    if num_gpus > 1 and primary_device != 'cpu':\n        print(f\"Wrapping model with nn.DataParallel for training across {num_gpus} GPUs.\")\n        model = nn.DataParallel(model)\n        # DataParallel will manage distribution to devices cuda:0, cuda:1, ...\n        # Model should already be on cuda:0 from get_model\n\n    # Ensure model is on the correct device (important if CPU or single GPU)\n    model.to(primary_device)\n\n    # 4. Latih Model (pass the potentially wrapped model)\n    # run_training now returns the potentially wrapped model and history\n    trained_model, history = run_training(config, model, train_dataloader, val_dataloader)\n\n    # 5. Evaluasi Model Terbaik di Test Set\n    # The evaluation function now handles loading the saved *base* state dict\n    # and potentially wrapping the model again for evaluation.\n    evaluate_model_on_test(config, test_dataloader, label_encoder)\n\n    print(\"\\n--- Analysis Complete ---\")\n\n    # === WANDB INTEGRATION START ===\n    # --- Finish the WandB run ---\n    wandb.finish()\n    # === WANDB INTEGRATION END ===\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T00:55:41.774093Z","iopub.execute_input":"2025-03-28T00:55:41.774403Z","iopub.status.idle":"2025-03-28T01:27:14.722490Z","shell.execute_reply.started":"2025-03-28T00:55:41.774381Z","shell.execute_reply":"2025-03-28T01:27:14.721848Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaybeitsai\u001b[0m (\u001b[33mhkacode\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Wandb login successful using Kaggle Secret.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250328_005548-wbxti19m</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/wbxti19m' target=\"_blank\">gcn-e3-bs128-lr2e-05</a></strong> to <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/wbxti19m' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/wbxti19m</a>"},"metadata":{}},{"name":"stdout","text":"Initializing model: gcn\nAttempting model visualization...\nModel graph saved to gcn_graph.png\nWrapping model with nn.DataParallel for training across 2 GPUs.\n\n--- Starting Training ---\nTraining with DataParallel on 2 GPUs.\n\nEpoch 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.4312, Train Acc: 0.7931\nVal Loss: 0.4642, Val Acc: 0.7683, Val ROC AUC: 0.9064\nValidation acc improved (0.7683 -> 0.7683). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: gcn-e3-bs128-lr2e-05-best-model\n\nEpoch 2/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.3215, Train Acc: 0.8610\nVal Loss: 0.3955, Val Acc: 0.8286, Val ROC AUC: 0.9185\nValidation acc improved (0.8286 -> 0.8286). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: gcn-e3-bs128-lr2e-05-best-model\n\nEpoch 3/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.2916, Train Acc: 0.8765\nVal Loss: 0.3611, Val Acc: 0.8470, Val ROC AUC: 0.9288\nValidation acc improved (0.8470 -> 0.8470). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: gcn-e3-bs128-lr2e-05-best-model\n--- Training Finished ---\nTraining log saved to training_log.json\nTraining history plot saved to training_history.png\n\n--- Evaluating on Test Set ---\nInitializing model: gcn\nLoading best model state from: best_model.safetensors\nModel state loaded successfully.\nUsing 2 GPUs for evaluation! Wrapping model with DataParallel.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/235 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nTest Loss: 0.3587\nTest Accuracy: 0.8499\nTest ROC AUC: 0.9302\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    negative     0.9003    0.7869    0.8398     15000\n    positive     0.8107    0.9129    0.8588     15000\n\n    accuracy                         0.8499     30000\n   macro avg     0.8555    0.8499    0.8493     30000\nweighted avg     0.8555    0.8499    0.8493     30000\n\n\nConfusion Matrix:\nConfusion matrix saved to confusion_matrix_test.png\n\n--- Analysis Complete ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train/accuracy</td><td>▁▇█</td></tr><tr><td>train/loss</td><td>█▃▁</td></tr><tr><td>val/accuracy</td><td>▁▆█</td></tr><tr><td>val/loss</td><td>█▃▁</td></tr><tr><td>val/roc_auc</td><td>▁▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>test/accuracy</td><td>0.84987</td></tr><tr><td>test/loss</td><td>0.35874</td></tr><tr><td>test/roc_auc</td><td>0.93017</td></tr><tr><td>train/accuracy</td><td>0.87649</td></tr><tr><td>train/loss</td><td>0.29158</td></tr><tr><td>val/accuracy</td><td>0.847</td></tr><tr><td>val/loss</td><td>0.36108</td></tr><tr><td>val/roc_auc</td><td>0.9288</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">gcn-e3-bs128-lr2e-05</strong> at: <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/wbxti19m' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/wbxti19m</a><br> View project at: <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid</a><br>Synced 5 W&B file(s), 0 media file(s), 6 artifact file(s) and 4 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250328_005548-wbxti19m/logs</code>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## BERT","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # === WANDB INTEGRATION START ===\n    # --- Log in to WandB using Kaggle Secrets ---\n    try:\n        from kaggle_secrets import UserSecretsClient\n        import os # Import os to potentially set environment variable\n\n        user_secrets = UserSecretsClient()\n\n        # Retrieve the secret variable (ensure the label matches what you set in Kaggle Secrets)\n        # METHOD 1: Directly use the key with wandb.login (Recommended for clarity)\n        wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n        wandb.login(key=wandb_api_key)\n        print(\"Wandb login successful using Kaggle Secret.\")\n\n        # METHOD 2: Set environment variable (wandb.init often picks this up automatically)\n        # os.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"WANDB_API_KEY\")\n        # print(\"WANDB_API_KEY environment variable set from Kaggle Secret.\")\n        # If using Method 2, wandb.login() might not even be strictly necessary\n        # as wandb.init() usually checks this environment variable.\n\n    except ImportError:\n        print(\"kaggle_secrets not found, assuming API key is set elsewhere (e.g., environment variable).\")\n    except Exception as e:\n        print(f\"Error retrieving WANDB_API_KEY from Kaggle Secrets or logging in: {e}\")\n        # Decide how to handle this - maybe exit, maybe proceed if key might be elsewhere\n        # For now, we'll proceed, but wandb logging might fail.\n        pass # Or raise e if login is critical\n\n\n    # --- Initialize WandB Run ---\n    run_name = f\"{config['model_type']}-e{config['epochs']}-bs{config['batch_size']}-lr{config['learning_rate']}\"\n    # wandb.init should now work without prompting if login was successful or env var is set\n    wandb.init(\n        project=\"sentiment-analysis-hybrid\",  # Replace with your project name\n        config=config,\n        name=run_name,\n        job_type=\"train\",\n    )\n    config = wandb.config # Copy back config\n    # === WANDB INTEGRATION END ===\n    \n    # 1. Get the base model instance (moved to primary device by get_model)\n    model = get_model(config)\n    primary_device = config['device']\n    num_gpus = torch.cuda.device_count()\n\n    # === WANDB INTEGRATION START ===\n    # --- Watch the model (optional but useful for gradients/parameters) ---\n    # Apply wandb.watch *before* DataParallel wrapping if possible\n    # Note: Watching DataParallel models might require watching model.module later\n    try:\n        # Log gradients and parameters of the model\n        wandb.watch(model, log=\"all\", log_freq=100) # Log every 100 batches\n    except Exception as e:\n        print(f\"Could not watch model with wandb: {e}\")\n    # === WANDB INTEGRATION END ===\n\n    # 2. (Optional) Visualize Architecture *before* wrapping\n    if torchviz and len(train_dataloader) > 0:\n        print(\"Attempting model visualization...\")\n        # Pass the unwrapped model\n        visualize_model(model, train_dataloader, config, filename=f\"{config['model_type']}_graph\")\n       \n        # === WANDB INTEGRATION START ===\n        # Log the model graph image (if created)\n        try:\n            graph_filename = f\"{config['model_type']}_graph.png\"\n            if os.path.exists(graph_filename):\n                 wandb.log({\"model_architecture\": wandb.Image(graph_filename)})\n        except Exception as e:\n            print(f\"Wandb: Could not log model architecture image: {e}\")\n        # === WANDB INTEGRATION END ===\n\n    # 3. Wrap model with DataParallel *before* training if multiple GPUs available\n    if num_gpus > 1 and primary_device != 'cpu':\n        print(f\"Wrapping model with nn.DataParallel for training across {num_gpus} GPUs.\")\n        model = nn.DataParallel(model)\n        # DataParallel will manage distribution to devices cuda:0, cuda:1, ...\n        # Model should already be on cuda:0 from get_model\n\n    # Ensure model is on the correct device (important if CPU or single GPU)\n    model.to(primary_device)\n\n    # 4. Latih Model (pass the potentially wrapped model)\n    # run_training now returns the potentially wrapped model and history\n    trained_model, history = run_training(config, model, train_dataloader, val_dataloader)\n\n    # 5. Evaluasi Model Terbaik di Test Set\n    # The evaluation function now handles loading the saved *base* state dict\n    # and potentially wrapping the model again for evaluation.\n    evaluate_model_on_test(config, test_dataloader, label_encoder)\n\n    print(\"\\n--- Analysis Complete ---\")\n\n    # === WANDB INTEGRATION START ===\n    # --- Finish the WandB run ---\n    wandb.finish()\n    # === WANDB INTEGRATION END ===\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-27T09:31:13.678140Z","iopub.execute_input":"2025-03-27T09:31:13.678476Z","iopub.status.idle":"2025-03-27T10:43:30.738234Z","shell.execute_reply.started":"2025-03-27T09:31:13.678447Z","shell.execute_reply":"2025-03-27T10:43:30.737270Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaybeitsai\u001b[0m (\u001b[33mhkacode\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Wandb login successful using Kaggle Secret.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250327_093119-mrewdz5o</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/mrewdz5o' target=\"_blank\">bert-e3-bs128-lr2e-05</a></strong> to <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/mrewdz5o' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/mrewdz5o</a>"},"metadata":{}},{"name":"stdout","text":"Initializing model: bert\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"16b0cbdc38904b5fb88d18356bab3d17"}},"metadata":{}},{"name":"stdout","text":"Attempting model visualization...\nModel graph saved to bert_graph.png\nWrapping model with nn.DataParallel for training across 2 GPUs.\n\n--- Starting Training ---\nTraining with DataParallel on 2 GPUs.\n\nEpoch 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.2556, Train Acc: 0.8951\nVal Loss: 0.1656, Val Acc: 0.9406, Val ROC AUC: 0.9836\nValidation acc improved (0.9406 -> 0.9406). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: bert-e3-bs128-lr2e-05-best-model\n\nEpoch 2/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1608, Train Acc: 0.9391\nVal Loss: 0.1495, Val Acc: 0.9459, Val ROC AUC: 0.9853\nValidation acc improved (0.9459 -> 0.9459). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: bert-e3-bs128-lr2e-05-best-model\n\nEpoch 3/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1063, Train Acc: 0.9623\nVal Loss: 0.1647, Val Acc: 0.9421, Val ROC AUC: 0.9856\nValidation acc did not improve. Patience: 1/3\n--- Training Finished ---\nTraining log saved to training_log.json\nTraining history plot saved to training_history.png\n\n--- Evaluating on Test Set ---\nInitializing model: bert\nLoading best model state from: best_model.safetensors\nModel state loaded successfully.\nUsing 2 GPUs for evaluation! Wrapping model with DataParallel.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/235 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nTest Loss: 0.1433\nTest Accuracy: 0.9465\nTest ROC AUC: 0.9867\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    negative     0.9462    0.9468    0.9465     15000\n    positive     0.9468    0.9461    0.9464     15000\n\n    accuracy                         0.9465     30000\n   macro avg     0.9465    0.9465    0.9465     30000\nweighted avg     0.9465    0.9465    0.9465     30000\n\n\nConfusion Matrix:\nConfusion matrix saved to confusion_matrix_test.png\n\n--- Analysis Complete ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train/accuracy</td><td>▁▆█</td></tr><tr><td>train/loss</td><td>█▄▁</td></tr><tr><td>val/accuracy</td><td>▁█▃</td></tr><tr><td>val/loss</td><td>█▁█</td></tr><tr><td>val/roc_auc</td><td>▁▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>test/accuracy</td><td>0.94647</td></tr><tr><td>test/loss</td><td>0.14335</td></tr><tr><td>test/roc_auc</td><td>0.98669</td></tr><tr><td>train/accuracy</td><td>0.96226</td></tr><tr><td>train/loss</td><td>0.10633</td></tr><tr><td>val/accuracy</td><td>0.94205</td></tr><tr><td>val/loss</td><td>0.16472</td></tr><tr><td>val/roc_auc</td><td>0.98556</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bert-e3-bs128-lr2e-05</strong> at: <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/mrewdz5o' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid/runs/mrewdz5o</a><br> View project at: <a href='https://wandb.ai/hkacode/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/hkacode/sentiment-analysis-hybrid</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 4 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250327_093119-mrewdz5o/logs</code>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"## BERT-LSTM-CNN","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # === WANDB INTEGRATION START ===\n    # --- Log in to WandB using Kaggle Secrets ---\n    try:\n        from kaggle_secrets import UserSecretsClient\n        import os # Import os to potentially set environment variable\n\n        user_secrets = UserSecretsClient()\n\n        # Retrieve the secret variable (ensure the label matches what you set in Kaggle Secrets)\n        # METHOD 1: Directly use the key with wandb.login (Recommended for clarity)\n        wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n        wandb.login(key=wandb_api_key)\n        print(\"Wandb login successful using Kaggle Secret.\")\n\n        # METHOD 2: Set environment variable (wandb.init often picks this up automatically)\n        # os.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"WANDB_API_KEY\")\n        # print(\"WANDB_API_KEY environment variable set from Kaggle Secret.\")\n        # If using Method 2, wandb.login() might not even be strictly necessary\n        # as wandb.init() usually checks this environment variable.\n\n    except ImportError:\n        print(\"kaggle_secrets not found, assuming API key is set elsewhere (e.g., environment variable).\")\n    except Exception as e:\n        print(f\"Error retrieving WANDB_API_KEY from Kaggle Secrets or logging in: {e}\")\n        # Decide how to handle this - maybe exit, maybe proceed if key might be elsewhere\n        # For now, we'll proceed, but wandb logging might fail.\n        pass # Or raise e if login is critical\n\n\n    # --- Initialize WandB Run ---\n    run_name = f\"{config['model_type']}-e{config['epochs']}-bs{config['batch_size']}-lr{config['learning_rate']}\"\n    # wandb.init should now work without prompting if login was successful or env var is set\n    wandb.init(\n        project=\"sentiment-analysis-hybrid\",  # Replace with your project name\n        config=config,\n        name=run_name,\n        job_type=\"train\",\n    )\n    config = wandb.config # Copy back config\n    # === WANDB INTEGRATION END ===\n    \n    # 1. Get the base model instance (moved to primary device by get_model)\n    model = get_model(config)\n    primary_device = config['device']\n    num_gpus = torch.cuda.device_count()\n\n    # === WANDB INTEGRATION START ===\n    # --- Watch the model (optional but useful for gradients/parameters) ---\n    # Apply wandb.watch *before* DataParallel wrapping if possible\n    # Note: Watching DataParallel models might require watching model.module later\n    try:\n        # Log gradients and parameters of the model\n        wandb.watch(model, log=\"all\", log_freq=100) # Log every 100 batches\n    except Exception as e:\n        print(f\"Could not watch model with wandb: {e}\")\n    # === WANDB INTEGRATION END ===\n\n    # 2. (Optional) Visualize Architecture *before* wrapping\n    if torchviz and len(train_dataloader) > 0:\n        print(\"Attempting model visualization...\")\n        # Pass the unwrapped model\n        visualize_model(model, train_dataloader, config, filename=f\"{config['model_type']}_graph\")\n       \n        # === WANDB INTEGRATION START ===\n        # Log the model graph image (if created)\n        try:\n            graph_filename = f\"{config['model_type']}_graph.png\"\n            if os.path.exists(graph_filename):\n                 wandb.log({\"model_architecture\": wandb.Image(graph_filename)})\n        except Exception as e:\n            print(f\"Wandb: Could not log model architecture image: {e}\")\n        # === WANDB INTEGRATION END ===\n\n    # 3. Wrap model with DataParallel *before* training if multiple GPUs available\n    if num_gpus > 1 and primary_device != 'cpu':\n        print(f\"Wrapping model with nn.DataParallel for training across {num_gpus} GPUs.\")\n        model = nn.DataParallel(model)\n        # DataParallel will manage distribution to devices cuda:0, cuda:1, ...\n        # Model should already be on cuda:0 from get_model\n\n    # Ensure model is on the correct device (important if CPU or single GPU)\n    model.to(primary_device)\n\n    # 4. Latih Model (pass the potentially wrapped model)\n    # run_training now returns the potentially wrapped model and history\n    trained_model, history = run_training(config, model, train_dataloader, val_dataloader)\n\n    # 5. Evaluasi Model Terbaik di Test Set\n    # The evaluation function now handles loading the saved *base* state dict\n    # and potentially wrapping the model again for evaluation.\n    evaluate_model_on_test(config, test_dataloader, label_encoder)\n\n    print(\"\\n--- Analysis Complete ---\")\n\n    # === WANDB INTEGRATION START ===\n    # --- Finish the WandB run ---\n    wandb.finish()\n    # === WANDB INTEGRATION END ===\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:38:56.523777Z","iopub.execute_input":"2025-03-28T10:38:56.524130Z","iopub.status.idle":"2025-03-28T11:56:36.085831Z","shell.execute_reply.started":"2025-03-28T10:38:56.524101Z","shell.execute_reply":"2025-03-28T11:56:36.085184Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mriqalter\u001b[0m (\u001b[33mmasriq\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"name":"stdout","text":"Wandb login successful using Kaggle Secret.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.19.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250328_103902-tojcu4s0</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/masriq/sentiment-analysis-hybrid/runs/tojcu4s0' target=\"_blank\">bert_lstm_cnn-e3-bs128-lr2e-05</a></strong> to <a href='https://wandb.ai/masriq/sentiment-analysis-hybrid' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/masriq/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/masriq/sentiment-analysis-hybrid</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/masriq/sentiment-analysis-hybrid/runs/tojcu4s0' target=\"_blank\">https://wandb.ai/masriq/sentiment-analysis-hybrid/runs/tojcu4s0</a>"},"metadata":{}},{"name":"stdout","text":"Initializing model: bert_lstm_cnn\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"893a83448b92482ebe97a3c28e8d4315"}},"metadata":{}},{"name":"stdout","text":"Attempting model visualization...\nModel graph saved to bert_lstm_cnn_graph.png\nWrapping model with nn.DataParallel for training across 2 GPUs.\n\n--- Starting Training ---\nTraining with DataParallel on 2 GPUs.\n\nEpoch 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.2696, Train Acc: 0.8939\nVal Loss: 0.1639, Val Acc: 0.9431, Val ROC AUC: 0.9841\nValidation acc improved (0.9431 -> 0.9431). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: bert_lstm_cnn-e3-bs128-lr2e-05-best-model\n\nEpoch 2/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1661, Train Acc: 0.9382\nVal Loss: 0.1515, Val Acc: 0.9454, Val ROC AUC: 0.9854\nValidation acc improved (0.9454 -> 0.9454). Saving model...\nBest model saved to best_model.safetensors\nWandb: Logged best model artifact: bert_lstm_cnn-e3-bs128-lr2e-05-best-model\n\nEpoch 3/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1119, Train Acc: 0.9614\nVal Loss: 0.1551, Val Acc: 0.9429, Val ROC AUC: 0.9859\nValidation acc did not improve. Patience: 1/3\n--- Training Finished ---\nTraining log saved to training_log.json\nTraining history plot saved to training_history.png\n\n--- Evaluating on Test Set ---\nInitializing model: bert_lstm_cnn\nLoading best model state from: best_model.safetensors\nModel state loaded successfully.\nUsing 2 GPUs for evaluation! Wrapping model with DataParallel.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/235 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nTest Loss: 0.1466\nTest Accuracy: 0.9449\nTest ROC AUC: 0.9866\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    negative     0.9310    0.9611    0.9458     15000\n    positive     0.9598    0.9287    0.9440     15000\n\n    accuracy                         0.9449     30000\n   macro avg     0.9454    0.9449    0.9449     30000\nweighted avg     0.9454    0.9449    0.9449     30000\n\n\nConfusion Matrix:\nConfusion matrix saved to confusion_matrix_test.png\n\n--- Analysis Complete ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train/accuracy</td><td>▁▆█</td></tr><tr><td>train/loss</td><td>█▃▁</td></tr><tr><td>val/accuracy</td><td>▁█▁</td></tr><tr><td>val/loss</td><td>█▁▃</td></tr><tr><td>val/roc_auc</td><td>▁▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>test/accuracy</td><td>0.94493</td></tr><tr><td>test/loss</td><td>0.14657</td></tr><tr><td>test/roc_auc</td><td>0.98662</td></tr><tr><td>train/accuracy</td><td>0.96136</td></tr><tr><td>train/loss</td><td>0.11194</td></tr><tr><td>val/accuracy</td><td>0.94295</td></tr><tr><td>val/loss</td><td>0.15506</td></tr><tr><td>val/roc_auc</td><td>0.98589</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bert_lstm_cnn-e3-bs128-lr2e-05</strong> at: <a href='https://wandb.ai/masriq/sentiment-analysis-hybrid/runs/tojcu4s0' target=\"_blank\">https://wandb.ai/masriq/sentiment-analysis-hybrid/runs/tojcu4s0</a><br> View project at: <a href='https://wandb.ai/masriq/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/masriq/sentiment-analysis-hybrid</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 4 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250328_103902-tojcu4s0/logs</code>"},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"## BERT-LSTM-GCN","metadata":{}},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    # === WANDB INTEGRATION START ===\n    # --- Log in to WandB using Kaggle Secrets ---\n    try:\n        from kaggle_secrets import UserSecretsClient\n        import os # Import os to potentially set environment variable\n\n        user_secrets = UserSecretsClient()\n\n        # Retrieve the secret variable (ensure the label matches what you set in Kaggle Secrets)\n        # METHOD 1: Directly use the key with wandb.login (Recommended for clarity)\n        wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n        wandb.login(key=wandb_api_key)\n        print(\"Wandb login successful using Kaggle Secret.\")\n\n        # METHOD 2: Set environment variable (wandb.init often picks this up automatically)\n        # os.environ[\"WANDB_API_KEY\"] = user_secrets.get_secret(\"WANDB_API_KEY\")\n        # print(\"WANDB_API_KEY environment variable set from Kaggle Secret.\")\n        # If using Method 2, wandb.login() might not even be strictly necessary\n        # as wandb.init() usually checks this environment variable.\n\n    except ImportError:\n        print(\"kaggle_secrets not found, assuming API key is set elsewhere (e.g., environment variable).\")\n    except Exception as e:\n        print(f\"Error retrieving WANDB_API_KEY from Kaggle Secrets or logging in: {e}\")\n        # Decide how to handle this - maybe exit, maybe proceed if key might be elsewhere\n        # For now, we'll proceed, but wandb logging might fail.\n        pass # Or raise e if login is critical\n\n\n    # --- Initialize WandB Run ---\n    run_name = f\"{config['model_type']}-e{config['epochs']}-bs{config['batch_size']}-lr{config['learning_rate']}\"\n    # wandb.init should now work without prompting if login was successful or env var is set\n    wandb.init(\n        project=\"sentiment-analysis-hybrid\",  # Replace with your project name\n        config=config,\n        name=run_name,\n        job_type=\"train\",\n    )\n    config = wandb.config # Copy back config\n    # === WANDB INTEGRATION END ===\n    \n    # 1. Get the base model instance (moved to primary device by get_model)\n    model = get_model(config)\n    primary_device = config['device']\n    num_gpus = torch.cuda.device_count()\n\n    # === WANDB INTEGRATION START ===\n    # --- Watch the model (optional but useful for gradients/parameters) ---\n    # Apply wandb.watch *before* DataParallel wrapping if possible\n    # Note: Watching DataParallel models might require watching model.module later\n    try:\n        # Log gradients and parameters of the model\n        wandb.watch(model, log=\"all\", log_freq=100) # Log every 100 batches\n    except Exception as e:\n        print(f\"Could not watch model with wandb: {e}\")\n    # === WANDB INTEGRATION END ===\n\n    # 2. (Optional) Visualize Architecture *before* wrapping\n    if torchviz and len(train_dataloader) > 0:\n        print(\"Attempting model visualization...\")\n        # Pass the unwrapped model\n        visualize_model(model, train_dataloader, config, filename=f\"{config['model_type']}_graph\")\n       \n        # === WANDB INTEGRATION START ===\n        # Log the model graph image (if created)\n        try:\n            graph_filename = f\"{config['model_type']}_graph.png\"\n            if os.path.exists(graph_filename):\n                 wandb.log({\"model_architecture\": wandb.Image(graph_filename)})\n        except Exception as e:\n            print(f\"Wandb: Could not log model architecture image: {e}\")\n        # === WANDB INTEGRATION END ===\n\n    # 3. Wrap model with DataParallel *before* training if multiple GPUs available\n    if num_gpus > 1 and primary_device != 'cpu':\n        print(f\"Wrapping model with nn.DataParallel for training across {num_gpus} GPUs.\")\n        model = nn.DataParallel(model)\n        # DataParallel will manage distribution to devices cuda:0, cuda:1, ...\n        # Model should already be on cuda:0 from get_model\n\n    # Ensure model is on the correct device (important if CPU or single GPU)\n    model.to(primary_device)\n\n    # 4. Latih Model (pass the potentially wrapped model)\n    # run_training now returns the potentially wrapped model and history\n    trained_model, history = run_training(config, model, train_dataloader, val_dataloader)\n\n    # 5. Evaluasi Model Terbaik di Test Set\n    # The evaluation function now handles loading the saved *base* state dict\n    # and potentially wrapping the model again for evaluation.\n    evaluate_model_on_test(config, test_dataloader, label_encoder)\n\n    print(\"\\n--- Analysis Complete ---\")\n\n    # === WANDB INTEGRATION START ===\n    # --- Finish the WandB run ---\n    wandb.finish()\n    # === WANDB INTEGRATION END ===\n","metadata":{"execution":{"iopub.status.busy":"2025-03-27T04:49:07.858895Z","iopub.execute_input":"2025-03-27T04:49:07.859269Z","iopub.status.idle":"2025-03-27T06:37:25.553586Z","shell.execute_reply.started":"2025-03-27T04:49:07.859239Z","shell.execute_reply":"2025-03-27T06:37:25.552917Z"},"trusted":true},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n","output_type":"stream"},{"name":"stdout","text":"Wandb login successful using Kaggle Secret.\nInitializing model: bert_lstm_gcn\nAttempting model visualization...\nModel graph saved to bert_lstm_gcn_graph.png\nWrapping model with nn.DataParallel for training across 2 GPUs.\n\n--- Starting Training ---\nTraining with DataParallel on 2 GPUs.\n\nEpoch 1/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.2776, Train Acc: 0.8842\nVal Loss: 0.2526, Val Acc: 0.9338, Val ROC AUC: 0.9816\nValidation acc improved (0.9338 -> 0.9338). Saving model...\nBest model saved to best_model.safetensors\nWandb: Error logging model artifact: metadata must be dict, not <class 'wandb.sdk.wandb_config.Config'>\n\nEpoch 2/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1693, Train Acc: 0.9371\nVal Loss: 0.2112, Val Acc: 0.9453, Val ROC AUC: 0.9849\nValidation acc improved (0.9453 -> 0.9453). Saving model...\nBest model saved to best_model.safetensors\nWandb: Error logging model artifact: metadata must be dict, not <class 'wandb.sdk.wandb_config.Config'>\n\nEpoch 3/3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training:   0%|          | 0/782 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/157 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1132, Train Acc: 0.9603\nVal Loss: 0.1875, Val Acc: 0.9454, Val ROC AUC: 0.9842\nValidation acc improved (0.9454 -> 0.9454). Saving model...\nBest model saved to best_model.safetensors\nWandb: Error logging model artifact: metadata must be dict, not <class 'wandb.sdk.wandb_config.Config'>\n--- Training Finished ---\nTraining log saved to training_log.json\nTraining history plot saved to training_history.png\n\n--- Evaluating on Test Set ---\nInitializing model: bert_lstm_gcn\nLoading best model state from: best_model.safetensors\nModel state loaded successfully.\nUsing 2 GPUs for evaluation! Wrapping model with DataParallel.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/235 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\nTest Loss: 0.1851\nTest Accuracy: 0.9430\nTest ROC AUC: 0.9852\n\nClassification Report:\n              precision    recall  f1-score   support\n\n    negative     0.9433    0.9426    0.9429     15000\n    positive     0.9426    0.9433    0.9430     15000\n\n    accuracy                         0.9430     30000\n   macro avg     0.9430    0.9430    0.9430     30000\nweighted avg     0.9430    0.9430    0.9430     30000\n\n\nConfusion Matrix:\nConfusion matrix saved to confusion_matrix_test.png\n\n--- Analysis Complete ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>train/accuracy</td><td>▁▆█</td></tr><tr><td>train/loss</td><td>█▃▁</td></tr><tr><td>val/accuracy</td><td>▁██</td></tr><tr><td>val/loss</td><td>█▄▁</td></tr><tr><td>val/roc_auc</td><td>▁█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>test/accuracy</td><td>0.94297</td></tr><tr><td>test/loss</td><td>0.18509</td></tr><tr><td>test/roc_auc</td><td>0.98519</td></tr><tr><td>train/accuracy</td><td>0.96031</td></tr><tr><td>train/loss</td><td>0.11319</td></tr><tr><td>val/accuracy</td><td>0.9454</td></tr><tr><td>val/loss</td><td>0.18754</td></tr><tr><td>val/roc_auc</td><td>0.98416</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bert_lstm_gcn-e3-bs128-lr2e-05</strong> at: <a href='https://wandb.ai/masriq/sentiment-analysis-hybrid/runs/tvm6l6g0' target=\"_blank\">https://wandb.ai/masriq/sentiment-analysis-hybrid/runs/tvm6l6g0</a><br> View project at: <a href='https://wandb.ai/masriq/sentiment-analysis-hybrid' target=\"_blank\">https://wandb.ai/masriq/sentiment-analysis-hybrid</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 6 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250327_044122-tvm6l6g0/logs</code>"},"metadata":{}}],"execution_count":32}]}